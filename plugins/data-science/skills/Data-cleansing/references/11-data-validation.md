# 11. Data Validation (ë°ì´í„° ê²€ì¦)

**ìƒì„±ì¼**: 2025-01-26  
**ë²„ì „**: 1.0  
**ì¹´í…Œê³ ë¦¬**: Data Quality & Validation

---

## 1. ê°œìš” (Overview)

### 1.1 ëª©ì  (Purpose)

ë°ì´í„° ê²€ì¦(Data Validation)ì€ ë°ì´í„° í´ë Œì§• í”„ë¡œì„¸ìŠ¤ì˜ ìµœì¢… ë‹¨ê³„ë¡œ, ë°ì´í„°ì˜ ì¼ê´€ì„±(consistency), ì •í™•ì„±(accuracy), ë¬´ê²°ì„±(integrity)ì„ ë³´ì¥í•˜ëŠ” í•„ìˆ˜ í”„ë¡œì„¸ìŠ¤ì…ë‹ˆë‹¤. ì´ ë ˆí¼ëŸ°ìŠ¤ëŠ” êµì°¨ í•„ë“œ ê²€ì¦, ì°¸ì¡° ë¬´ê²°ì„± ê²€ì¦, ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ê²€ì¦ì„ ìë™í™”í•˜ëŠ” ë°©ë²•ì„ ì œê³µí•©ë‹ˆë‹¤.

### 1.2 ì ìš© ì‹œê¸° (When to Apply)

**í•„ìˆ˜ ì ìš© ì‹œì **:
- âœ… ë°ì´í„° í´ë Œì§• ì‘ì—… ì™„ë£Œ í›„ ìµœì¢… ê²€ì¦ ë‹¨ê³„
- âœ… í”„ë¡œë•ì…˜ í™˜ê²½ìœ¼ë¡œ ë°ì´í„° ë°°í¬ ì „
- âœ… ë°ì´í„° í†µí•©(integration) í›„ ì¼ê´€ì„± í™•ì¸
- âœ… ML ëª¨ë¸ í•™ìŠµ ì „ ë°ì´í„° í’ˆì§ˆ í™•ë³´

**ìƒí™©ë³„ ì ìš©**:
- ğŸ”¹ ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ ë°ì´í„° ë³‘í•© ì‹œ
- ğŸ”¹ ì™¸ë¶€ ë°ì´í„°ë¥¼ ë°›ì•˜ì„ ë•Œ
- ğŸ”¹ ì‹œê³„ì—´ ë°ì´í„°ì˜ ë…¼ë¦¬ì  ìˆœì„œ í™•ì¸
- ğŸ”¹ ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ì´ ë³µì¡í•œ ë„ë©”ì¸ (ê¸ˆìœµ, ì˜ë£Œ ë“±)

### 1.3 ê²€ì¦ ë ˆë²¨ (Validation Levels)

```
Level 1: Field-Level Validation (ê°œë³„ í•„ë“œ)
â””â”€â”€ Data type, format, range, null check

Level 2: Cross-Field Validation (êµì°¨ í•„ë“œ)
â””â”€â”€ start_date < end_date, age vs birth_date

Level 3: Record-Level Validation (ë ˆì½”ë“œ)
â””â”€â”€ Business rules, calculation verification

Level 4: Referential Integrity (ì°¸ì¡° ë¬´ê²°ì„±)
â””â”€â”€ Foreign key relationships, orphan records

Level 5: Dataset-Level Validation (ë°ì´í„°ì…‹)
â””â”€â”€ Aggregation checks, distribution validation
```

---

## 2. ì´ë¡ ì  ë°°ê²½ (Theoretical Background)

### 2.1 ë°ì´í„° ê²€ì¦ì˜ ì¤‘ìš”ì„±

**ë°ì´í„° í’ˆì§ˆ ì°¨ì› (Data Quality Dimensions)**:
1. **Completeness** (ì™„ì „ì„±): ê²°ì¸¡ê°’ì´ ì—†ëŠ”ê°€?
2. **Consistency** (ì¼ê´€ì„±): ë°ì´í„° ê°„ ëª¨ìˆœì´ ì—†ëŠ”ê°€?
3. **Accuracy** (ì •í™•ì„±): ë°ì´í„°ê°€ ì‹¤ì œì™€ ì¼ì¹˜í•˜ëŠ”ê°€?
4. **Validity** (ìœ íš¨ì„±): ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ì„ ì¤€ìˆ˜í•˜ëŠ”ê°€?
5. **Timeliness** (ì ì‹œì„±): ë°ì´í„°ê°€ ìµœì‹ ì¸ê°€?
6. **Uniqueness** (ìœ ì¼ì„±): ì¤‘ë³µì´ ì—†ëŠ”ê°€?

### 2.2 ê²€ì¦ ë©”ì»¤ë‹ˆì¦˜

**Type 1: Constraint-Based Validation**
- ì •ì˜ëœ ì œì•½ ì¡°ê±´ ê¸°ë°˜ (ë²”ìœ„, í˜•ì‹, NOT NULL ë“±)
- ëª…ì‹œì  ê·œì¹™ (Explicit rules)
- ì˜ˆ: `age BETWEEN 0 AND 120`

**Type 2: Relationship-Based Validation**
- í•„ë“œ ê°„ ê´€ê³„ ê²€ì¦
- ê³„ì‚°ì‹ ê²€ì¦
- ì˜ˆ: `total = quantity Ã— unit_price`

**Type 3: Reference-Based Validation**
- ì™¸ë˜ í‚¤(Foreign Key) ê²€ì¦
- ì°¸ì¡° í…Œì´ë¸”ê³¼ì˜ ì¼ì¹˜ì„±
- ì˜ˆ: `customer_id IN customers.id`

**Type 4: Statistical Validation**
- í†µê³„ì  íŒ¨í„´ ê²€ì¦
- ì´ìƒ ë¶„í¬ íƒì§€
- ì˜ˆ: `mean(sales) > 0 AND std(sales) < 1000`

### 2.3 ê²€ì¦ ì‹œë‚˜ë¦¬ì˜¤

**ì‹œë‚˜ë¦¬ì˜¤ 1: E-commerce ì£¼ë¬¸ ë°ì´í„°**
```python
# í•„ìˆ˜ ê²€ì¦ í•­ëª©
- order_date <= shipped_date <= delivered_date
- quantity > 0 AND unit_price > 0
- total_amount = quantity Ã— unit_price Ã— (1 - discount_rate)
- customer_id exists in customers table
- product_id exists in products table
- payment_method in ['credit_card', 'debit_card', 'paypal', 'cash']
```

**ì‹œë‚˜ë¦¬ì˜¤ 2: ì˜ë£Œ í™˜ì ë°ì´í„°**
```python
# í•„ìˆ˜ ê²€ì¦ í•­ëª©
- admission_date <= discharge_date
- age = (today - birth_date) / 365.25
- blood_pressure_systolic > blood_pressure_diastolic
- patient_id is unique
- diagnosis_code follows ICD-10 format
- medication_dose within safe range
```

**ì‹œë‚˜ë¦¬ì˜¤ 3: ê¸ˆìœµ ê±°ë˜ ë°ì´í„°**
```python
# í•„ìˆ˜ ê²€ì¦ í•­ëª©
- transaction_amount != 0
- account_balance_before + transaction_amount = account_balance_after
- transaction_date >= account_open_date
- currency_code in ISO_4217_codes
- transaction_type in ['deposit', 'withdrawal', 'transfer']
```

---

## 3. êµ¬í˜„ (Implementation)

### 3.1 êµì°¨ í•„ë“œ ê²€ì¦ (Cross-Field Validation)

```python
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Any

class CrossFieldValidator:
    """
    êµì°¨ í•„ë“œ ì¼ê´€ì„± ê²€ì¦ í´ë˜ìŠ¤
    Cross-field consistency validation
    """
    
    def __init__(self, df: pd.DataFrame):
        """
        Parameters:
        -----------
        df : pd.DataFrame
            ê²€ì¦í•  ë°ì´í„°í”„ë ˆì„
        """
        self.df = df.copy()
        self.violations = []
        
    def validate_date_sequence(
        self, 
        date_columns: List[str],
        allow_same_date: bool = False
    ) -> pd.DataFrame:
        """
        ë‚ ì§œ ì»¬ëŸ¼ë“¤ì´ ì˜¬ë°”ë¥¸ ìˆœì„œì¸ì§€ ê²€ì¦
        Validate chronological order of date columns
        
        Parameters:
        -----------
        date_columns : List[str]
            ìˆœì„œëŒ€ë¡œ ì •ë ¬ë˜ì–´ì•¼ í•˜ëŠ” ë‚ ì§œ ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
            ì˜ˆ: ['start_date', 'end_date', 'completed_date']
        allow_same_date : bool
            ë™ì¼í•œ ë‚ ì§œë¥¼ í—ˆìš©í• ì§€ ì—¬ë¶€
            
        Returns:
        --------
        violations_df : pd.DataFrame
            ê·œì¹™ ìœ„ë°˜ ë ˆì½”ë“œ
            
        Example:
        --------
        >>> validator = CrossFieldValidator(df)
        >>> violations = validator.validate_date_sequence(
        ...     ['order_date', 'shipped_date', 'delivered_date']
        ... )
        """
        # ë‚ ì§œ ì»¬ëŸ¼ì„ datetimeìœ¼ë¡œ ë³€í™˜
        for col in date_columns:
            if col in self.df.columns:
                self.df[col] = pd.to_datetime(self.df[col], errors='coerce')
        
        # ìˆœì„œ ê²€ì¦
        mask = pd.Series([False] * len(self.df), index=self.df.index)
        
        for i in range(len(date_columns) - 1):
            col1, col2 = date_columns[i], date_columns[i + 1]
            
            if col1 not in self.df.columns or col2 not in self.df.columns:
                continue
                
            if allow_same_date:
                # col1 <= col2
                invalid = self.df[col1] > self.df[col2]
            else:
                # col1 < col2
                invalid = self.df[col1] >= self.df[col2]
            
            mask = mask | invalid.fillna(False)
        
        violations = self.df[mask].copy()
        
        if len(violations) > 0:
            self.violations.append({
                'rule': f'Date sequence: {" < ".join(date_columns)}',
                'violations_count': len(violations),
                'severity': 'HIGH',
                'columns': date_columns
            })
            
        return violations
    
    def validate_calculation(
        self,
        result_column: str,
        formula: str,
        tolerance: float = 0.01
    ) -> pd.DataFrame:
        """
        ê³„ì‚°ì‹ ê²€ì¦ (ì˜ˆ: total = quantity Ã— price)
        Validate calculation formulas
        
        Parameters:
        -----------
        result_column : str
            ê²°ê³¼ ì»¬ëŸ¼ëª…
        formula : str
            ê³„ì‚° ê³µì‹ (pandas eval í˜•ì‹)
            ì˜ˆ: 'quantity * unit_price'
        tolerance : float
            í—ˆìš© ì˜¤ì°¨ (ë¶€ë™ì†Œìˆ˜ì  ë¹„êµìš©)
            
        Returns:
        --------
        violations_df : pd.DataFrame
            ê³„ì‚° ë¶ˆì¼ì¹˜ ë ˆì½”ë“œ
            
        Example:
        --------
        >>> violations = validator.validate_calculation(
        ...     result_column='total_amount',
        ...     formula='quantity * unit_price * (1 - discount_rate)',
        ...     tolerance=0.01
        ... )
        """
        try:
            # ê³µì‹ ê³„ì‚°
            calculated = self.df.eval(formula)
            actual = self.df[result_column]
            
            # ì°¨ì´ ê³„ì‚° (ì ˆëŒ€ê°’)
            difference = np.abs(calculated - actual)
            
            # í—ˆìš© ì˜¤ì°¨ë¥¼ ì´ˆê³¼í•˜ëŠ” ë ˆì½”ë“œ
            mask = difference > tolerance
            violations = self.df[mask].copy()
            
            # ê³„ì‚° ê²°ê³¼ ì¶”ê°€
            violations['calculated_value'] = calculated[mask]
            violations['actual_value'] = actual[mask]
            violations['difference'] = difference[mask]
            
            if len(violations) > 0:
                self.violations.append({
                    'rule': f'{result_column} = {formula}',
                    'violations_count': len(violations),
                    'severity': 'HIGH',
                    'columns': [result_column]
                })
                
            return violations
            
        except Exception as e:
            print(f"Error in calculation validation: {e}")
            return pd.DataFrame()
    
    def validate_age_birthdate(
        self,
        age_column: str = 'age',
        birthdate_column: str = 'birth_date',
        tolerance_years: float = 1.0
    ) -> pd.DataFrame:
        """
        ë‚˜ì´ì™€ ìƒë…„ì›”ì¼ ì¼ì¹˜ì„± ê²€ì¦
        Validate consistency between age and birth date
        
        Parameters:
        -----------
        age_column : str
            ë‚˜ì´ ì»¬ëŸ¼ëª…
        birthdate_column : str
            ìƒë…„ì›”ì¼ ì»¬ëŸ¼ëª…
        tolerance_years : float
            í—ˆìš© ì˜¤ì°¨ (ë…„)
            
        Returns:
        --------
        violations_df : pd.DataFrame
            ë‚˜ì´ ë¶ˆì¼ì¹˜ ë ˆì½”ë“œ
        """
        if age_column not in self.df.columns or birthdate_column not in self.df.columns:
            return pd.DataFrame()
        
        # ìƒë…„ì›”ì¼ì„ datetimeìœ¼ë¡œ ë³€í™˜
        birth_dates = pd.to_datetime(self.df[birthdate_column], errors='coerce')
        
        # í˜„ì¬ ë‚ ì§œ ê¸°ì¤€ ë‚˜ì´ ê³„ì‚°
        today = pd.Timestamp.now()
        calculated_age = (today - birth_dates).dt.days / 365.25
        
        # ë‚˜ì´ ì°¨ì´ ê³„ì‚°
        age_difference = np.abs(self.df[age_column] - calculated_age)
        
        # í—ˆìš© ì˜¤ì°¨ë¥¼ ì´ˆê³¼í•˜ëŠ” ë ˆì½”ë“œ
        mask = age_difference > tolerance_years
        violations = self.df[mask].copy()
        
        violations['calculated_age'] = calculated_age[mask]
        violations['actual_age'] = self.df[age_column][mask]
        violations['age_difference'] = age_difference[mask]
        
        if len(violations) > 0:
            self.violations.append({
                'rule': f'{age_column} must match {birthdate_column}',
                'violations_count': len(violations),
                'severity': 'MEDIUM',
                'columns': [age_column, birthdate_column]
            })
            
        return violations
    
    def validate_conditional_logic(
        self,
        condition: str,
        required_fields: List[str],
        rule_description: str
    ) -> pd.DataFrame:
        """
        ì¡°ê±´ë¶€ ë¡œì§ ê²€ì¦
        Validate conditional business logic
        
        Parameters:
        -----------
        condition : str
            ì¡°ê±´ì‹ (pandas query í˜•ì‹)
            ì˜ˆ: "status == 'active' and end_date.isnull()"
        required_fields : List[str]
            ì¡°ê±´ì— ì‚¬ìš©ëœ í•„ë“œ ëª©ë¡
        rule_description : str
            ê·œì¹™ ì„¤ëª…
            
        Returns:
        --------
        violations_df : pd.DataFrame
            ì¡°ê±´ ìœ„ë°˜ ë ˆì½”ë“œ
            
        Example:
        --------
        >>> # í™œì„± ìƒíƒœì¸ ê²½ìš° ì¢…ë£Œì¼ì´ ì—†ì–´ì•¼ í•¨
        >>> violations = validator.validate_conditional_logic(
        ...     condition="status == 'active' and end_date.notna()",
        ...     required_fields=['status', 'end_date'],
        ...     rule_description='Active records should not have end_date'
        ... )
        """
        try:
            # ì¡°ê±´ì— ë§ëŠ” ë ˆì½”ë“œ ì°¾ê¸° (ì´ê²ƒì´ ìœ„ë°˜)
            violations = self.df.query(condition).copy()
            
            if len(violations) > 0:
                self.violations.append({
                    'rule': rule_description,
                    'violations_count': len(violations),
                    'severity': 'MEDIUM',
                    'columns': required_fields
                })
                
            return violations
            
        except Exception as e:
            print(f"Error in conditional logic validation: {e}")
            return pd.DataFrame()
    
    def get_validation_summary(self) -> pd.DataFrame:
        """
        ê²€ì¦ ê²°ê³¼ ìš”ì•½
        Get validation summary
        
        Returns:
        --------
        summary_df : pd.DataFrame
            ê²€ì¦ ê²°ê³¼ ìš”ì•½ í…Œì´ë¸”
        """
        if not self.violations:
            print("âœ… No validation violations found!")
            return pd.DataFrame()
        
        summary = pd.DataFrame(self.violations)
        summary = summary.sort_values('violations_count', ascending=False)
        
        print(f"\nâš ï¸ Found {len(summary)} validation rule violations")
        print(f"Total violation records: {summary['violations_count'].sum()}")
        
        return summary


# ì‚¬ìš© ì˜ˆì‹œ 1: E-commerce ì£¼ë¬¸ ë°ì´í„° ê²€ì¦
def validate_ecommerce_orders(df: pd.DataFrame) -> Dict[str, pd.DataFrame]:
    """
    E-commerce ì£¼ë¬¸ ë°ì´í„° ì¢…í•© ê²€ì¦
    Comprehensive validation for e-commerce orders
    """
    validator = CrossFieldValidator(df)
    results = {}
    
    # 1. ë‚ ì§œ ìˆœì„œ ê²€ì¦
    print("1. Validating date sequence...")
    results['date_violations'] = validator.validate_date_sequence(
        ['order_date', 'shipped_date', 'delivered_date']
    )
    
    # 2. ê¸ˆì•¡ ê³„ì‚° ê²€ì¦
    print("2. Validating amount calculation...")
    results['amount_violations'] = validator.validate_calculation(
        result_column='total_amount',
        formula='quantity * unit_price',
        tolerance=0.01
    )
    
    # 3. ìˆ˜ëŸ‰ ì–‘ìˆ˜ ê²€ì¦
    print("3. Validating positive quantities...")
    results['quantity_violations'] = validator.validate_conditional_logic(
        condition='quantity <= 0',
        required_fields=['quantity'],
        rule_description='Quantity must be positive'
    )
    
    # 4. í• ì¸ìœ¨ ë²”ìœ„ ê²€ì¦
    print("4. Validating discount rate...")
    if 'discount_rate' in df.columns:
        results['discount_violations'] = validator.validate_conditional_logic(
            condition='discount_rate < 0 or discount_rate > 1',
            required_fields=['discount_rate'],
            rule_description='Discount rate must be between 0 and 1'
        )
    
    # ê²€ì¦ ìš”ì•½
    print("\n" + "="*60)
    summary = validator.get_validation_summary()
    print("\nValidation Summary:")
    print(summary)
    
    return results


# ì‚¬ìš© ì˜ˆì‹œ 2: ì˜ë£Œ í™˜ì ë°ì´í„° ê²€ì¦
def validate_patient_records(df: pd.DataFrame) -> Dict[str, pd.DataFrame]:
    """
    ì˜ë£Œ í™˜ì ë°ì´í„° ì¢…í•© ê²€ì¦
    Comprehensive validation for patient records
    """
    validator = CrossFieldValidator(df)
    results = {}
    
    # 1. ì…ì›-í‡´ì› ë‚ ì§œ ê²€ì¦
    print("1. Validating admission/discharge dates...")
    results['date_violations'] = validator.validate_date_sequence(
        ['admission_date', 'discharge_date']
    )
    
    # 2. ë‚˜ì´-ìƒë…„ì›”ì¼ ê²€ì¦
    print("2. Validating age vs birth date...")
    results['age_violations'] = validator.validate_age_birthdate(
        age_column='age',
        birthdate_column='birth_date',
        tolerance_years=1.0
    )
    
    # 3. í˜ˆì•• ê²€ì¦ (ìˆ˜ì¶•ê¸° > ì´ì™„ê¸°)
    print("3. Validating blood pressure...")
    if 'bp_systolic' in df.columns and 'bp_diastolic' in df.columns:
        results['bp_violations'] = validator.validate_conditional_logic(
            condition='bp_systolic <= bp_diastolic',
            required_fields=['bp_systolic', 'bp_diastolic'],
            rule_description='Systolic BP must be greater than Diastolic BP'
        )
    
    # 4. ì²´ì§ˆëŸ‰ì§€ìˆ˜(BMI) ê³„ì‚° ê²€ì¦
    print("4. Validating BMI calculation...")
    if all(col in df.columns for col in ['bmi', 'weight_kg', 'height_m']):
        results['bmi_violations'] = validator.validate_calculation(
            result_column='bmi',
            formula='weight_kg / (height_m ** 2)',
            tolerance=0.1
        )
    
    # ê²€ì¦ ìš”ì•½
    print("\n" + "="*60)
    summary = validator.get_validation_summary()
    print("\nValidation Summary:")
    print(summary)
    
    return results
```

### 3.2 ì°¸ì¡° ë¬´ê²°ì„± ê²€ì¦ (Referential Integrity Validation)

```python
class ReferentialIntegrityValidator:
    """
    ì°¸ì¡° ë¬´ê²°ì„± ê²€ì¦ í´ë˜ìŠ¤
    Foreign key and referential integrity validation
    """
    
    def __init__(self):
        self.violations = []
    
    def validate_foreign_key(
        self,
        df: pd.DataFrame,
        fk_column: str,
        reference_df: pd.DataFrame,
        pk_column: str,
        relationship_name: str = None
    ) -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """
        ì™¸ë˜ í‚¤(Foreign Key) ê²€ì¦
        Validate foreign key relationships
        
        Parameters:
        -----------
        df : pd.DataFrame
            ê²€ì¦í•  ì£¼ ë°ì´í„°í”„ë ˆì„ (ì™¸ë˜ í‚¤ í¬í•¨)
        fk_column : str
            ì™¸ë˜ í‚¤ ì»¬ëŸ¼ëª…
        reference_df : pd.DataFrame
            ì°¸ì¡° ë°ì´í„°í”„ë ˆì„ (ê¸°ë³¸ í‚¤ í¬í•¨)
        pk_column : str
            ê¸°ë³¸ í‚¤ ì»¬ëŸ¼ëª…
        relationship_name : str, optional
            ê´€ê³„ ì´ë¦„ (ì˜ˆ: 'orders -> customers')
            
        Returns:
        --------
        orphan_records : pd.DataFrame
            ê³ ì•„ ë ˆì½”ë“œ (orphan records)
        stats : dict
            ê²€ì¦ í†µê³„
            
        Example:
        --------
        >>> validator = ReferentialIntegrityValidator()
        >>> orphans, stats = validator.validate_foreign_key(
        ...     df=orders_df,
        ...     fk_column='customer_id',
        ...     reference_df=customers_df,
        ...     pk_column='customer_id',
        ...     relationship_name='orders -> customers'
        ... )
        """
        # NULL ê°’ ì œì™¸ (NULLì€ ì™¸ë˜ í‚¤ì—ì„œ í—ˆìš©ë  ìˆ˜ ìˆìŒ)
        non_null_fk = df[df[fk_column].notna()].copy()
        
        # ì°¸ì¡° í…Œì´ë¸”ì˜ ìœ ë‹ˆí¬ í‚¤
        reference_keys = set(reference_df[pk_column].dropna().unique())
        
        # ê³ ì•„ ë ˆì½”ë“œ ì°¾ê¸°
        orphan_mask = ~non_null_fk[fk_column].isin(reference_keys)
        orphan_records = non_null_fk[orphan_mask].copy()
        
        # í†µê³„ ê³„ì‚°
        stats = {
            'relationship': relationship_name or f'{fk_column} -> {pk_column}',
            'total_records': len(df),
            'non_null_fk': len(non_null_fk),
            'null_fk': len(df) - len(non_null_fk),
            'orphan_records': len(orphan_records),
            'orphan_percentage': 100 * len(orphan_records) / len(non_null_fk) if len(non_null_fk) > 0 else 0,
            'unique_orphan_keys': orphan_records[fk_column].nunique(),
            'reference_table_size': len(reference_df),
            'reference_unique_keys': len(reference_keys)
        }
        
        # ìœ„ë°˜ ê¸°ë¡
        if len(orphan_records) > 0:
            self.violations.append({
                'relationship': stats['relationship'],
                'orphan_count': stats['orphan_records'],
                'orphan_pct': stats['orphan_percentage'],
                'severity': 'HIGH' if stats['orphan_percentage'] > 5 else 'MEDIUM'
            })
        
        return orphan_records, stats
    
    def validate_multiple_relationships(
        self,
        relationships: List[Dict[str, Any]]
    ) -> pd.DataFrame:
        """
        ì—¬ëŸ¬ ì™¸ë˜ í‚¤ ê´€ê³„ë¥¼ í•œë²ˆì— ê²€ì¦
        Validate multiple foreign key relationships
        
        Parameters:
        -----------
        relationships : List[Dict]
            ê²€ì¦í•  ê´€ê³„ ëª©ë¡
            [{
                'df': main_dataframe,
                'fk_column': 'foreign_key_column',
                'reference_df': reference_dataframe,
                'pk_column': 'primary_key_column',
                'name': 'relationship_name'
            }, ...]
            
        Returns:
        --------
        summary_df : pd.DataFrame
            ê²€ì¦ ê²°ê³¼ ìš”ì•½
            
        Example:
        --------
        >>> relationships = [
        ...     {
        ...         'df': orders_df,
        ...         'fk_column': 'customer_id',
        ...         'reference_df': customers_df,
        ...         'pk_column': 'customer_id',
        ...         'name': 'orders -> customers'
        ...     },
        ...     {
        ...         'df': orders_df,
        ...         'fk_column': 'product_id',
        ...         'reference_df': products_df,
        ...         'pk_column': 'product_id',
        ...         'name': 'orders -> products'
        ...     }
        ... ]
        >>> summary = validator.validate_multiple_relationships(relationships)
        """
        results = []
        
        for rel in relationships:
            print(f"\nValidating: {rel.get('name', 'Unknown relationship')}")
            orphans, stats = self.validate_foreign_key(
                df=rel['df'],
                fk_column=rel['fk_column'],
                reference_df=rel['reference_df'],
                pk_column=rel['pk_column'],
                relationship_name=rel.get('name')
            )
            
            results.append(stats)
            
            # ê³ ì•„ ë ˆì½”ë“œ í†µê³„ ì¶œë ¥
            if stats['orphan_records'] > 0:
                print(f"  âš ï¸ Found {stats['orphan_records']} orphan records "
                      f"({stats['orphan_percentage']:.2f}%)")
                print(f"  Unique orphan keys: {stats['unique_orphan_keys']}")
            else:
                print(f"  âœ… No orphan records found")
        
        summary_df = pd.DataFrame(results)
        return summary_df
    
    def validate_bidirectional_relationship(
        self,
        df1: pd.DataFrame,
        col1: str,
        df2: pd.DataFrame,
        col2: str,
        relationship_name: str = None
    ) -> Dict[str, Any]:
        """
        ì–‘ë°©í–¥ ê´€ê³„ ê²€ì¦ (Many-to-Many)
        Validate bidirectional relationships
        
        Parameters:
        -----------
        df1 : pd.DataFrame
            ì²« ë²ˆì§¸ ë°ì´í„°í”„ë ˆì„
        col1 : str
            ì²« ë²ˆì§¸ ë°ì´í„°í”„ë ˆì„ì˜ í‚¤ ì»¬ëŸ¼
        df2 : pd.DataFrame
            ë‘ ë²ˆì§¸ ë°ì´í„°í”„ë ˆì„
        col2 : str
            ë‘ ë²ˆì§¸ ë°ì´í„°í”„ë ˆì„ì˜ í‚¤ ì»¬ëŸ¼
        relationship_name : str, optional
            ê´€ê³„ ì´ë¦„
            
        Returns:
        --------
        results : dict
            ì–‘ë°©í–¥ ê²€ì¦ ê²°ê³¼
        """
        # df1 -> df2 ê²€ì¦
        keys1 = set(df1[col1].dropna().unique())
        keys2 = set(df2[col2].dropna().unique())
        
        # ê° ë°©í–¥ì˜ ê³ ì•„ í‚¤
        orphans_in_df1 = keys1 - keys2  # df1ì—ë§Œ ìˆê³  df2ì— ì—†ëŠ” í‚¤
        orphans_in_df2 = keys2 - keys1  # df2ì—ë§Œ ìˆê³  df1ì— ì—†ëŠ” í‚¤
        
        results = {
            'relationship': relationship_name or f'{col1} <-> {col2}',
            'keys_in_df1': len(keys1),
            'keys_in_df2': len(keys2),
            'common_keys': len(keys1 & keys2),
            'orphans_in_df1': len(orphans_in_df1),
            'orphans_in_df2': len(orphans_in_df2),
            'orphan_keys_df1': list(orphans_in_df1)[:10],  # ìƒ˜í”Œ
            'orphan_keys_df2': list(orphans_in_df2)[:10]   # ìƒ˜í”Œ
        }
        
        return results
    
    def get_integrity_report(self) -> pd.DataFrame:
        """
        ì°¸ì¡° ë¬´ê²°ì„± ê²€ì¦ ë¦¬í¬íŠ¸
        Get referential integrity report
        """
        if not self.violations:
            print("âœ… No referential integrity violations found!")
            return pd.DataFrame()
        
        report = pd.DataFrame(self.violations)
        report = report.sort_values('orphan_count', ascending=False)
        
        print(f"\nâš ï¸ Found {len(report)} referential integrity violations")
        print(f"Total orphan records: {report['orphan_count'].sum()}")
        
        return report


# ì‚¬ìš© ì˜ˆì‹œ: E-commerce ë°ì´í„°ë² ì´ìŠ¤ ë¬´ê²°ì„± ê²€ì¦
def validate_ecommerce_integrity(
    orders_df: pd.DataFrame,
    customers_df: pd.DataFrame,
    products_df: pd.DataFrame,
    payments_df: pd.DataFrame
) -> Dict[str, Any]:
    """
    E-commerce ë°ì´í„°ë² ì´ìŠ¤ì˜ ì°¸ì¡° ë¬´ê²°ì„± ì¢…í•© ê²€ì¦
    """
    validator = ReferentialIntegrityValidator()
    
    # ì—¬ëŸ¬ ê´€ê³„ ì •ì˜
    relationships = [
        {
            'df': orders_df,
            'fk_column': 'customer_id',
            'reference_df': customers_df,
            'pk_column': 'customer_id',
            'name': 'orders -> customers'
        },
        {
            'df': orders_df,
            'fk_column': 'product_id',
            'reference_df': products_df,
            'pk_column': 'product_id',
            'name': 'orders -> products'
        },
        {
            'df': payments_df,
            'fk_column': 'order_id',
            'reference_df': orders_df,
            'pk_column': 'order_id',
            'name': 'payments -> orders'
        }
    ]
    
    # ì¼ê´„ ê²€ì¦
    summary = validator.validate_multiple_relationships(relationships)
    
    # ë¦¬í¬íŠ¸ ìƒì„±
    integrity_report = validator.get_integrity_report()
    
    return {
        'summary': summary,
        'report': integrity_report
    }
```

### 3.3 ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ê²€ì¦ (Business Logic Validation)

```python
from typing import Callable

class BusinessRuleValidator:
    """
    ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ê²€ì¦ í´ë˜ìŠ¤
    Business rule validation engine
    """
    
    def __init__(self, df: pd.DataFrame):
        self.df = df.copy()
        self.rules = []
        self.results = []
    
    def add_rule(
        self,
        rule_name: str,
        rule_func: Callable,
        severity: str = 'MEDIUM',
        description: str = None
    ):
        """
        ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ì¶”ê°€
        Add a business rule
        
        Parameters:
        -----------
        rule_name : str
            ê·œì¹™ ì´ë¦„
        rule_func : Callable
            ê²€ì¦ í•¨ìˆ˜ (DataFrameì„ ë°›ì•„ ìœ„ë°˜ ë ˆì½”ë“œ ë°˜í™˜)
        severity : str
            ì‹¬ê°ë„ ('HIGH', 'MEDIUM', 'LOW')
        description : str
            ê·œì¹™ ì„¤ëª…
        """
        self.rules.append({
            'name': rule_name,
            'func': rule_func,
            'severity': severity,
            'description': description or rule_name
        })
    
    def validate_all(self) -> pd.DataFrame:
        """
        ëª¨ë“  ê·œì¹™ ê²€ì¦
        Validate all business rules
        
        Returns:
        --------
        summary_df : pd.DataFrame
            ê²€ì¦ ê²°ê³¼ ìš”ì•½
        """
        print("Starting business rule validation...")
        print(f"Total rules to validate: {len(self.rules)}\n")
        
        for i, rule in enumerate(self.rules, 1):
            print(f"[{i}/{len(self.rules)}] Validating: {rule['name']}")
            
            try:
                violations = rule['func'](self.df)
                
                result = {
                    'rule_name': rule['name'],
                    'description': rule['description'],
                    'severity': rule['severity'],
                    'violations_count': len(violations),
                    'violations_pct': 100 * len(violations) / len(self.df),
                    'status': 'PASS' if len(violations) == 0 else 'FAIL'
                }
                
                self.results.append(result)
                
                if len(violations) > 0:
                    print(f"  âš ï¸ Found {len(violations)} violations ({result['violations_pct']:.2f}%)")
                else:
                    print(f"  âœ… Passed")
                    
            except Exception as e:
                print(f"  âŒ Error: {e}")
                self.results.append({
                    'rule_name': rule['name'],
                    'description': rule['description'],
                    'severity': rule['severity'],
                    'violations_count': -1,
                    'violations_pct': -1,
                    'status': 'ERROR',
                    'error': str(e)
                })
        
        summary_df = pd.DataFrame(self.results)
        return summary_df
    
    def get_violation_details(
        self,
        rule_name: str,
        max_records: int = 100
    ) -> pd.DataFrame:
        """
        íŠ¹ì • ê·œì¹™ì˜ ìœ„ë°˜ ìƒì„¸ ì¡°íšŒ
        Get detailed violations for a specific rule
        """
        rule = next((r for r in self.rules if r['name'] == rule_name), None)
        
        if rule is None:
            print(f"Rule '{rule_name}' not found")
            return pd.DataFrame()
        
        violations = rule['func'](self.df)
        
        if len(violations) > max_records:
            print(f"Showing first {max_records} of {len(violations)} violations")
            return violations.head(max_records)
        
        return violations


# ë„ë©”ì¸ë³„ ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ì˜ˆì‹œ

def create_financial_rules(df: pd.DataFrame) -> BusinessRuleValidator:
    """
    ê¸ˆìœµ ë„ë©”ì¸ ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™
    Financial domain business rules
    """
    validator = BusinessRuleValidator(df)
    
    # ê·œì¹™ 1: ê±°ë˜ ê¸ˆì•¡ì€ 0ì´ ì•„ë‹ˆì–´ì•¼ í•¨
    validator.add_rule(
        rule_name='NonZeroTransaction',
        rule_func=lambda df: df[df['transaction_amount'] == 0],
        severity='HIGH',
        description='Transaction amount must be non-zero'
    )
    
    # ê·œì¹™ 2: ì”ì•¡ ì •í•©ì„± (ì´ì „ ì”ì•¡ + ê±°ë˜ ê¸ˆì•¡ = í˜„ì¬ ì”ì•¡)
    validator.add_rule(
        rule_name='BalanceConsistency',
        rule_func=lambda df: df[
            np.abs(
                (df['balance_before'] + df['transaction_amount']) - df['balance_after']
            ) > 0.01
        ],
        severity='HIGH',
        description='Balance must be consistent: balance_before + transaction = balance_after'
    )
    
    # ê·œì¹™ 3: ë§ˆì´ë„ˆìŠ¤ ì”ì•¡ í™•ì¸ (ë‹¹ì¢Œ ê³„ì¢Œ ì œì™¸)
    validator.add_rule(
        rule_name='NegativeBalance',
        rule_func=lambda df: df[
            (df['balance_after'] < 0) & (df['account_type'] != 'overdraft')
        ],
        severity='HIGH',
        description='Non-overdraft accounts cannot have negative balance'
    )
    
    # ê·œì¹™ 4: ì¼ì¼ ê±°ë˜ í•œë„ ì´ˆê³¼
    validator.add_rule(
        rule_name='DailyTransactionLimit',
        rule_func=lambda df: df[
            df.groupby('account_id')['transaction_amount'].transform('sum') > 10000
        ],
        severity='MEDIUM',
        description='Daily transaction limit exceeded (>10,000)'
    )
    
    return validator


def create_healthcare_rules(df: pd.DataFrame) -> BusinessRuleValidator:
    """
    ì˜ë£Œ ë„ë©”ì¸ ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™
    Healthcare domain business rules
    """
    validator = BusinessRuleValidator(df)
    
    # ê·œì¹™ 1: í™˜ì ë‚˜ì´ ë²”ìœ„ (0-120)
    validator.add_rule(
        rule_name='ValidAge',
        rule_func=lambda df: df[(df['age'] < 0) | (df['age'] > 120)],
        severity='HIGH',
        description='Patient age must be between 0 and 120'
    )
    
    # ê·œì¹™ 2: í˜ˆì•• ì •ìƒ ë²”ìœ„ (ìˆ˜ì¶•ê¸°: 70-200, ì´ì™„ê¸°: 40-130)
    validator.add_rule(
        rule_name='ValidBloodPressure',
        rule_func=lambda df: df[
            (df['bp_systolic'] < 70) | (df['bp_systolic'] > 200) |
            (df['bp_diastolic'] < 40) | (df['bp_diastolic'] > 130)
        ],
        severity='MEDIUM',
        description='Blood pressure must be within normal range'
    )
    
    # ê·œì¹™ 3: ì²´ì˜¨ ì •ìƒ ë²”ìœ„ (35-42Â°C)
    validator.add_rule(
        rule_name='ValidTemperature',
        rule_func=lambda df: df[
            (df['temperature'] < 35) | (df['temperature'] > 42)
        ],
        severity='HIGH',
        description='Body temperature must be between 35 and 42Â°C'
    )
    
    # ê·œì¹™ 4: ì•½ë¬¼ íˆ¬ì—¬ëŸ‰ ì•ˆì „ ë²”ìœ„
    validator.add_rule(
        rule_name='SafeDosage',
        rule_func=lambda df: df[
            df['dosage_mg'] > df['max_safe_dosage_mg']
        ],
        severity='HIGH',
        description='Dosage must not exceed safe maximum'
    )
    
    # ê·œì¹™ 5: ì¬ì…ì› ê°„ê²© (í‡´ì› í›„ ìµœì†Œ 1ì¼ ê²½ê³¼)
    if 'previous_discharge_date' in df.columns:
        validator.add_rule(
            rule_name='ReadmissionInterval',
            rule_func=lambda df: df[
                (pd.to_datetime(df['admission_date']) - 
                 pd.to_datetime(df['previous_discharge_date'])).dt.days < 1
            ],
            severity='MEDIUM',
            description='Readmission must be at least 1 day after discharge'
        )
    
    return validator


def create_ecommerce_rules(df: pd.DataFrame) -> BusinessRuleValidator:
    """
    E-commerce ë„ë©”ì¸ ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™
    E-commerce domain business rules
    """
    validator = BusinessRuleValidator(df)
    
    # ê·œì¹™ 1: ì£¼ë¬¸ ìˆ˜ëŸ‰ì€ ì–‘ìˆ˜
    validator.add_rule(
        rule_name='PositiveQuantity',
        rule_func=lambda df: df[df['quantity'] <= 0],
        severity='HIGH',
        description='Order quantity must be positive'
    )
    
    # ê·œì¹™ 2: í• ì¸ìœ¨ ë²”ìœ„ (0-100%)
    validator.add_rule(
        rule_name='ValidDiscountRate',
        rule_func=lambda df: df[
            (df['discount_rate'] < 0) | (df['discount_rate'] > 1)
        ],
        severity='MEDIUM',
        description='Discount rate must be between 0 and 1'
    )
    
    # ê·œì¹™ 3: ë°°ì†¡ë¹„ëŠ” ì£¼ë¬¸ ê¸ˆì•¡ì— ë”°ë¼ ì ì •í•´ì•¼ í•¨
    validator.add_rule(
        rule_name='ShippingCost',
        rule_func=lambda df: df[
            (df['order_amount'] > 50) & (df['shipping_cost'] > 0)
        ],
        severity='LOW',
        description='Free shipping for orders over $50'
    )
    
    # ê·œì¹™ 4: í™˜ë¶ˆ ê¸ˆì•¡ì€ ì› ì£¼ë¬¸ ê¸ˆì•¡ ì´í•˜
    if 'refund_amount' in df.columns:
        validator.add_rule(
            rule_name='ValidRefundAmount',
            rule_func=lambda df: df[
                df['refund_amount'] > df['order_amount']
            ],
            severity='HIGH',
            description='Refund amount cannot exceed original order amount'
        )
    
    # ê·œì¹™ 5: ì·¨ì†Œëœ ì£¼ë¬¸ì€ ë°°ì†¡ë˜ì§€ ì•Šì•„ì•¼ í•¨
    validator.add_rule(
        rule_name='CancelledOrderNotShipped',
        rule_func=lambda df: df[
            (df['order_status'] == 'cancelled') & (df['shipped_date'].notna())
        ],
        severity='HIGH',
        description='Cancelled orders should not be shipped'
    )
    
    return validator
```

### 3.4 Great Expectations í†µí•©

```python
try:
    import great_expectations as gx
    from great_expectations.core.batch import RuntimeBatchRequest
    HAS_GX = True
except ImportError:
    HAS_GX = False
    print("Great Expectations not installed. Install with: pip install great-expectations")


class GreatExpectationsValidator:
    """
    Great Expectations ê¸°ë°˜ ìë™ ê²€ì¦
    Automated validation using Great Expectations
    """
    
    def __init__(self, context_root_dir: str = None):
        """
        Parameters:
        -----------
        context_root_dir : str, optional
            Great Expectations í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬
        """
        if not HAS_GX:
            raise ImportError("Great Expectations is required for this validator")
        
        self.context = gx.get_context(context_root_dir=context_root_dir)
    
    def create_expectation_suite(
        self,
        suite_name: str,
        df: pd.DataFrame
    ) -> None:
        """
        Expectation Suite ìƒì„±
        Create an expectation suite
        """
        # Data Source ìƒì„±
        datasource_name = f"datasource_{suite_name}"
        
        # Pandas DataFrameì„ ë°ì´í„° ì†ŒìŠ¤ë¡œ ì¶”ê°€
        datasource = self.context.sources.add_or_update_pandas(datasource_name)
        
        # Data Asset ì¶”ê°€
        data_asset = datasource.add_dataframe_asset(name=f"asset_{suite_name}")
        
        # Batch Request ìƒì„±
        batch_request = data_asset.build_batch_request(dataframe=df)
        
        # Expectation Suite ìƒì„±
        self.context.add_or_update_expectation_suite(suite_name)
        
        # Validator ìƒì„±
        validator = self.context.get_validator(
            batch_request=batch_request,
            expectation_suite_name=suite_name
        )
        
        return validator
    
    def add_common_expectations(
        self,
        validator,
        column_config: Dict[str, Dict[str, Any]]
    ):
        """
        ì¼ë°˜ì ì¸ Expectations ì¶”ê°€
        Add common expectations based on column configuration
        
        Parameters:
        -----------
        validator : Validator
            Great Expectations Validator
        column_config : Dict
            ì»¬ëŸ¼ë³„ ì„¤ì •
            {
                'column_name': {
                    'type': 'numeric|categorical|datetime',
                    'nullable': True|False,
                    'unique': True|False,
                    'min': value,
                    'max': value,
                    'values': [allowed_values]
                }
            }
        """
        df = validator.active_batch_definition
        
        for column, config in column_config.items():
            if column not in validator.active_batch.data.columns:
                continue
            
            # 1. ì»¬ëŸ¼ ì¡´ì¬ í™•ì¸
            validator.expect_column_to_exist(column)
            
            # 2. Null ê°’ ì²´í¬
            if not config.get('nullable', True):
                validator.expect_column_values_to_not_be_null(column)
            
            # 3. ìœ ë‹ˆí¬ ì²´í¬
            if config.get('unique', False):
                validator.expect_column_values_to_be_unique(column)
            
            # 4. ë°ì´í„° íƒ€ì…ë³„ ê²€ì¦
            if config.get('type') == 'numeric':
                # ìˆ˜ì¹˜í˜• ë²”ìœ„ ê²€ì¦
                if 'min' in config:
                    validator.expect_column_values_to_be_between(
                        column,
                        min_value=config['min'],
                        max_value=config.get('max')
                    )
            
            elif config.get('type') == 'categorical':
                # ë²”ì£¼í˜• ê°’ ê²€ì¦
                if 'values' in config:
                    validator.expect_column_values_to_be_in_set(
                        column,
                        value_set=config['values']
                    )
            
            elif config.get('type') == 'datetime':
                # ë‚ ì§œ í˜•ì‹ ê²€ì¦
                validator.expect_column_values_to_be_of_type(
                    column,
                    type_='datetime64[ns]'
                )
        
        # Suite ì €ì¥
        validator.save_expectation_suite(discard_failed_expectations=False)
        
        return validator
    
    def run_validation(
        self,
        df: pd.DataFrame,
        suite_name: str
    ) -> Dict[str, Any]:
        """
        ê²€ì¦ ì‹¤í–‰
        Run validation
        """
        # Checkpoint ìƒì„±
        checkpoint_name = f"checkpoint_{suite_name}"
        
        checkpoint = self.context.add_or_update_checkpoint(
            name=checkpoint_name,
            validations=[
                {
                    "batch_request": {
                        "datasource_name": f"datasource_{suite_name}",
                        "data_asset_name": f"asset_{suite_name}",
                        "options": {}
                    },
                    "expectation_suite_name": suite_name
                }
            ]
        )
        
        # ê²€ì¦ ì‹¤í–‰
        results = checkpoint.run()
        
        return results


# ì‚¬ìš© ì˜ˆì‹œ
def validate_with_great_expectations(df: pd.DataFrame) -> Dict[str, Any]:
    """
    Great Expectationsë¥¼ ì‚¬ìš©í•œ í¬ê´„ì  ê²€ì¦
    """
    if not HAS_GX:
        print("Great Expectations not available. Skipping...")
        return {}
    
    validator_gx = GreatExpectationsValidator()
    
    # Expectation Suite ìƒì„±
    validator = validator_gx.create_expectation_suite(
        suite_name="data_quality_suite",
        df=df
    )
    
    # ì»¬ëŸ¼ ì„¤ì •
    column_config = {
        'age': {
            'type': 'numeric',
            'nullable': False,
            'min': 0,
            'max': 120
        },
        'gender': {
            'type': 'categorical',
            'nullable': False,
            'values': ['Male', 'Female', 'Other']
        },
        'email': {
            'type': 'string',
            'nullable': False,
            'unique': True
        },
        'signup_date': {
            'type': 'datetime',
            'nullable': False
        }
    }
    
    # Expectations ì¶”ê°€
    validator_gx.add_common_expectations(validator, column_config)
    
    # ê²€ì¦ ì‹¤í–‰
    results = validator_gx.run_validation(df, "data_quality_suite")
    
    return results
```

### 3.5 Pandera ìŠ¤í‚¤ë§ˆ ê¸°ë°˜ ê²€ì¦

```python
try:
    import pandera as pa
    from pandera import Column, DataFrameSchema, Check
    HAS_PANDERA = True
except ImportError:
    HAS_PANDERA = False
    print("Pandera not installed. Install with: pip install pandera")


class PanderaValidator:
    """
    Pandera ìŠ¤í‚¤ë§ˆ ê¸°ë°˜ ê²€ì¦
    Schema-based validation using Pandera
    """
    
    @staticmethod
    def create_ecommerce_schema() -> DataFrameSchema:
        """
        E-commerce ë°ì´í„° ìŠ¤í‚¤ë§ˆ ì •ì˜
        """
        if not HAS_PANDERA:
            return None
        
        schema = DataFrameSchema({
            "order_id": Column(
                dtype="int64",
                checks=[
                    Check.greater_than(0),
                    Check(lambda s: s.is_unique, error="order_id must be unique")
                ],
                nullable=False
            ),
            "customer_id": Column(
                dtype="int64",
                checks=Check.greater_than(0),
                nullable=False
            ),
            "order_date": Column(
                dtype="datetime64[ns]",
                checks=Check.less_than_or_equal_to(pd.Timestamp.now()),
                nullable=False
            ),
            "quantity": Column(
                dtype="int64",
                checks=Check.in_range(1, 1000),
                nullable=False
            ),
            "unit_price": Column(
                dtype="float64",
                checks=Check.greater_than(0),
                nullable=False
            ),
            "discount_rate": Column(
                dtype="float64",
                checks=Check.in_range(0, 1),
                nullable=True
            ),
            "status": Column(
                dtype="object",
                checks=Check.isin(['pending', 'processing', 'shipped', 'delivered', 'cancelled']),
                nullable=False
            )
        })
        
        return schema
    
    @staticmethod
    def create_patient_schema() -> DataFrameSchema:
        """
        í™˜ì ë°ì´í„° ìŠ¤í‚¤ë§ˆ ì •ì˜
        """
        if not HAS_PANDERA:
            return None
        
        schema = DataFrameSchema({
            "patient_id": Column(
                dtype="object",
                checks=Check(lambda s: s.is_unique),
                nullable=False
            ),
            "age": Column(
                dtype="int64",
                checks=Check.in_range(0, 120),
                nullable=False
            ),
            "birth_date": Column(
                dtype="datetime64[ns]",
                checks=Check.less_than(pd.Timestamp.now()),
                nullable=False
            ),
            "bp_systolic": Column(
                dtype="int64",
                checks=Check.in_range(70, 200),
                nullable=True
            ),
            "bp_diastolic": Column(
                dtype="int64",
                checks=Check.in_range(40, 130),
                nullable=True
            ),
            "temperature": Column(
                dtype="float64",
                checks=Check.in_range(35.0, 42.0),
                nullable=True
            ),
            "admission_date": Column(
                dtype="datetime64[ns]",
                nullable=True
            ),
            "discharge_date": Column(
                dtype="datetime64[ns]",
                nullable=True
            )
        },
        checks=[
            # êµì°¨ í•„ë“œ ê²€ì¦: ìˆ˜ì¶•ê¸° í˜ˆì•• > ì´ì™„ê¸° í˜ˆì••
            Check(
                lambda df: (df['bp_systolic'] > df['bp_diastolic']).all(),
                error="bp_systolic must be greater than bp_diastolic"
            ),
            # êµì°¨ í•„ë“œ ê²€ì¦: ì…ì›ì¼ <= í‡´ì›ì¼
            Check(
                lambda df: (df['admission_date'] <= df['discharge_date']).all(),
                error="admission_date must be before or equal to discharge_date"
            )
        ])
        
        return schema
    
    @staticmethod
    def validate_dataframe(
        df: pd.DataFrame,
        schema: DataFrameSchema
    ) -> Tuple[bool, pd.DataFrame]:
        """
        ë°ì´í„°í”„ë ˆì„ ê²€ì¦
        
        Returns:
        --------
        is_valid : bool
            ê²€ì¦ í†µê³¼ ì—¬ë¶€
        validated_df : pd.DataFrame
            ê²€ì¦ëœ ë°ì´í„°í”„ë ˆì„
        """
        if not HAS_PANDERA or schema is None:
            return True, df
        
        try:
            validated_df = schema.validate(df, lazy=True)
            print("âœ… Schema validation passed!")
            return True, validated_df
            
        except pa.errors.SchemaErrors as err:
            print("âš ï¸ Schema validation failed!")
            print(f"\nFailure cases: {len(err.failure_cases)}")
            print("\nError details:")
            print(err.failure_cases)
            
            return False, df


# ì‚¬ìš© ì˜ˆì‹œ
def validate_with_pandera(df: pd.DataFrame, domain: str = 'ecommerce'):
    """
    Panderaë¥¼ ì‚¬ìš©í•œ ìŠ¤í‚¤ë§ˆ ê¸°ë°˜ ê²€ì¦
    """
    if not HAS_PANDERA:
        print("Pandera not available. Skipping...")
        return df
    
    validator = PanderaValidator()
    
    # ë„ë©”ì¸ë³„ ìŠ¤í‚¤ë§ˆ ì„ íƒ
    if domain == 'ecommerce':
        schema = validator.create_ecommerce_schema()
    elif domain == 'healthcare':
        schema = validator.create_patient_schema()
    else:
        print(f"Unknown domain: {domain}")
        return df
    
    # ê²€ì¦ ì‹¤í–‰
    is_valid, validated_df = validator.validate_dataframe(df, schema)
    
    return validated_df
```

---

## 4. ì˜ˆì‹œ (Examples)

### 4.1 ì „ì²´ ê²€ì¦ íŒŒì´í”„ë¼ì¸ ì˜ˆì‹œ

```python
import pandas as pd
import numpy as np
from datetime import datetime, timedelta

# ìƒ˜í”Œ ë°ì´í„° ìƒì„±
def create_sample_ecommerce_data():
    """
    E-commerce ìƒ˜í”Œ ë°ì´í„° ìƒì„± (ì¼ë¶€ ì˜¤ë¥˜ í¬í•¨)
    """
    np.random.seed(42)
    n_orders = 1000
    
    df = pd.DataFrame({
        'order_id': range(1, n_orders + 1),
        'customer_id': np.random.randint(1, 201, n_orders),
        'product_id': np.random.randint(1, 51, n_orders),
        'order_date': pd.date_range('2024-01-01', periods=n_orders, freq='H'),
        'quantity': np.random.randint(1, 10, n_orders),
        'unit_price': np.random.uniform(10, 500, n_orders).round(2),
        'discount_rate': np.random.uniform(0, 0.3, n_orders).round(2),
        'total_amount': 0.0,  # ë‚˜ì¤‘ì— ê³„ì‚°
        'status': np.random.choice(
            ['pending', 'processing', 'shipped', 'delivered', 'cancelled'],
            n_orders
        )
    })
    
    # total_amount ê³„ì‚°
    df['total_amount'] = (df['quantity'] * df['unit_price'] * (1 - df['discount_rate'])).round(2)
    
    # shipped_date, delivered_date ì¶”ê°€
    df['shipped_date'] = df['order_date'] + pd.to_timedelta(
        np.random.randint(1, 5, n_orders), unit='D'
    )
    df['delivered_date'] = df['shipped_date'] + pd.to_timedelta(
        np.random.randint(2, 10, n_orders), unit='D'
    )
    
    # ì˜ë„ì  ì˜¤ë¥˜ ì‚½ì…
    # 1. ë‚ ì§œ ìˆœì„œ ì˜¤ë¥˜ (5%)
    error_idx = np.random.choice(df.index, size=int(0.05 * n_orders), replace=False)
    df.loc[error_idx, 'delivered_date'] = df.loc[error_idx, 'order_date'] - timedelta(days=1)
    
    # 2. ê³„ì‚° ì˜¤ë¥˜ (3%)
    error_idx = np.random.choice(df.index, size=int(0.03 * n_orders), replace=False)
    df.loc[error_idx, 'total_amount'] = df.loc[error_idx, 'total_amount'] * 1.5
    
    # 3. ìŒìˆ˜ ìˆ˜ëŸ‰ (2%)
    error_idx = np.random.choice(df.index, size=int(0.02 * n_orders), replace=False)
    df.loc[error_idx, 'quantity'] = -1
    
    # 4. ì˜ëª»ëœ í• ì¸ìœ¨ (1%)
    error_idx = np.random.choice(df.index, size=int(0.01 * n_orders), replace=False)
    df.loc[error_idx, 'discount_rate'] = 1.5
    
    # 5. ì·¨ì†Œëœ ì£¼ë¬¸ì´ì§€ë§Œ ë°°ì†¡ë¨ (1%)
    cancelled_orders = df[df['status'] == 'cancelled'].sample(frac=0.1).index
    # ì´ë¯¸ shipped_dateì™€ delivered_dateê°€ ìˆìœ¼ë¯€ë¡œ ì˜¤ë¥˜
    
    return df


# ì°¸ì¡° í…Œì´ë¸” ìƒì„±
def create_reference_tables():
    """
    ê³ ê° ë° ì œí’ˆ ì°¸ì¡° í…Œì´ë¸” ìƒì„±
    """
    customers = pd.DataFrame({
        'customer_id': range(1, 201),
        'customer_name': [f'Customer_{i}' for i in range(1, 201)],
        'email': [f'customer{i}@example.com' for i in range(1, 201)]
    })
    
    products = pd.DataFrame({
        'product_id': range(1, 51),
        'product_name': [f'Product_{i}' for i in range(1, 51)],
        'category': np.random.choice(['Electronics', 'Clothing', 'Books', 'Home'], 50)
    })
    
    return customers, products


# ì „ì²´ ê²€ì¦ ì‹¤í–‰
def run_comprehensive_validation():
    """
    ì¢…í•© ë°ì´í„° ê²€ì¦ ì‹¤í–‰
    """
    print("="*80)
    print("COMPREHENSIVE DATA VALIDATION PIPELINE")
    print("="*80)
    
    # ë°ì´í„° ë¡œë“œ
    print("\n1. Loading data...")
    df = create_sample_ecommerce_data()
    customers_df, products_df = create_reference_tables()
    print(f"   Orders: {len(df)} rows")
    print(f"   Customers: {len(customers_df)} rows")
    print(f"   Products: {len(products_df)} rows")
    
    # Phase 1: êµì°¨ í•„ë“œ ê²€ì¦
    print("\n" + "="*80)
    print("PHASE 1: CROSS-FIELD VALIDATION")
    print("="*80)
    
    cross_validator = CrossFieldValidator(df)
    
    # ë‚ ì§œ ìˆœì„œ ê²€ì¦
    date_violations = cross_validator.validate_date_sequence(
        ['order_date', 'shipped_date', 'delivered_date']
    )
    print(f"\nDate sequence violations: {len(date_violations)}")
    if len(date_violations) > 0:
        print(date_violations[['order_id', 'order_date', 'shipped_date', 'delivered_date']].head())
    
    # ê¸ˆì•¡ ê³„ì‚° ê²€ì¦
    amount_violations = cross_validator.validate_calculation(
        result_column='total_amount',
        formula='quantity * unit_price * (1 - discount_rate)',
        tolerance=0.01
    )
    print(f"\nAmount calculation violations: {len(amount_violations)}")
    if len(amount_violations) > 0:
        print(amount_violations[['order_id', 'total_amount', 'calculated_value', 'difference']].head())
    
    # ê²€ì¦ ìš”ì•½
    cross_summary = cross_validator.get_validation_summary()
    
    # Phase 2: ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ê²€ì¦
    print("\n" + "="*80)
    print("PHASE 2: BUSINESS RULE VALIDATION")
    print("="*80)
    
    rule_validator = create_ecommerce_rules(df)
    rule_summary = rule_validator.validate_all()
    
    print("\n" + "-"*80)
    print("Business Rule Summary:")
    print(rule_summary[['rule_name', 'severity', 'violations_count', 'status']])
    
    # Phase 3: ì°¸ì¡° ë¬´ê²°ì„± ê²€ì¦
    print("\n" + "="*80)
    print("PHASE 3: REFERENTIAL INTEGRITY VALIDATION")
    print("="*80)
    
    integrity_validator = ReferentialIntegrityValidator()
    
    relationships = [
        {
            'df': df,
            'fk_column': 'customer_id',
            'reference_df': customers_df,
            'pk_column': 'customer_id',
            'name': 'orders -> customers'
        },
        {
            'df': df,
            'fk_column': 'product_id',
            'reference_df': products_df,
            'pk_column': 'product_id',
            'name': 'orders -> products'
        }
    ]
    
    integrity_summary = integrity_validator.validate_multiple_relationships(relationships)
    print("\n" + "-"*80)
    print("Referential Integrity Summary:")
    print(integrity_summary[['relationship', 'orphan_records', 'orphan_percentage']])
    
    # ìµœì¢… ë¦¬í¬íŠ¸
    print("\n" + "="*80)
    print("VALIDATION SUMMARY REPORT")
    print("="*80)
    
    total_violations = (
        len(date_violations) +
        len(amount_violations) +
        rule_summary[rule_summary['status'] == 'FAIL']['violations_count'].sum()
    )
    
    print(f"\nğŸ“Š Total records validated: {len(df)}")
    print(f"âš ï¸  Total violations found: {total_violations}")
    print(f"ğŸ“‰ Violation rate: {100 * total_violations / len(df):.2f}%")
    
    print("\nValidation breakdown:")
    print(f"  - Cross-field violations: {len(date_violations) + len(amount_violations)}")
    print(f"  - Business rule violations: {rule_summary[rule_summary['status'] == 'FAIL']['violations_count'].sum()}")
    print(f"  - Referential integrity violations: {integrity_summary['orphan_records'].sum()}")
    
    # ì‹¬ê°ë„ë³„ ìš”ì•½
    if len(cross_summary) > 0:
        print("\nBy severity:")
        severity_counts = cross_summary.groupby('severity')['violations_count'].sum()
        for severity, count in severity_counts.items():
            print(f"  - {severity}: {count} violations")
    
    return {
        'cross_field': {'violations': date_violations, 'summary': cross_summary},
        'business_rules': {'summary': rule_summary},
        'referential_integrity': {'summary': integrity_summary}
    }


# ì‹¤í–‰
if __name__ == "__main__":
    results = run_comprehensive_validation()
```

### 4.2 ì¶œë ¥ ì˜ˆì‹œ

```
================================================================================
COMPREHENSIVE DATA VALIDATION PIPELINE
================================================================================

1. Loading data...
   Orders: 1000 rows
   Customers: 200 rows
   Products: 50 rows

================================================================================
PHASE 1: CROSS-FIELD VALIDATION
================================================================================
1. Validating date sequence...

Date sequence violations: 50
   order_id order_date         shipped_date       delivered_date
0        15 2024-01-01 14:00:00 2024-01-05 14:00:00 2023-12-31 14:00:00
1        42 2024-01-02 17:00:00 2024-01-06 17:00:00 2024-01-01 17:00:00
...

2. Validating amount calculation...

Amount calculation violations: 30
   order_id  total_amount  calculated_value  difference
0        73        425.50            283.67      141.83
1       156        892.75            595.17      297.58
...

âš ï¸ Found 2 validation rule violations
Total violation records: 80

================================================================================
PHASE 2: BUSINESS RULE VALIDATION
================================================================================
Starting business rule validation...
Total rules to validate: 5

[1/5] Validating: PositiveQuantity
  âš ï¸ Found 20 violations (2.00%)
[2/5] Validating: ValidDiscountRate
  âš ï¸ Found 10 violations (1.00%)
[3/5] Validating: ShippingCost
  âœ… Passed
[4/5] Validating: CancelledOrderNotShipped
  âš ï¸ Found 8 violations (0.80%)
[5/5] Validating: ValidRefundAmount
  âœ… Passed

--------------------------------------------------------------------------------
Business Rule Summary:
                   rule_name severity  violations_count status
0          PositiveQuantity     HIGH                20   FAIL
1        ValidDiscountRate   MEDIUM                10   FAIL
2             ShippingCost      LOW                 0   PASS
3  CancelledOrderNotShipped     HIGH                 8   FAIL
4      ValidRefundAmount      HIGH                 0   PASS

================================================================================
PHASE 3: REFERENTIAL INTEGRITY VALIDATION
================================================================================

Validating: orders -> customers
  âœ… No orphan records found

Validating: orders -> products
  âœ… No orphan records found

--------------------------------------------------------------------------------
Referential Integrity Summary:
              relationship  orphan_records  orphan_percentage
0     orders -> customers               0                0.0
1      orders -> products               0                0.0

================================================================================
VALIDATION SUMMARY REPORT
================================================================================

ğŸ“Š Total records validated: 1000
âš ï¸  Total violations found: 118
ğŸ“‰ Violation rate: 11.80%

Validation breakdown:
  - Cross-field violations: 80
  - Business rule violations: 38
  - Referential integrity violations: 0

By severity:
  - HIGH: 50 violations
  - MEDIUM: 30 violations
```

---

## 5. ì—ì´ì „íŠ¸ ë§¤í•‘ (Agent Mapping)

### 5.1 Primary Agent

**`data-cleaning-specialist`**
- ì—­í• : ë°ì´í„° ê²€ì¦ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì´ê´„
- ì±…ì„:
  - êµì°¨ í•„ë“œ ê²€ì¦ ì‹¤í–‰
  - ì°¸ì¡° ë¬´ê²°ì„± ê²€ì¦
  - ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ì •ì˜ ë° ì‹¤í–‰
  - ê²€ì¦ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„±

### 5.2 Supporting Agents

**`data-scientist`**
- ì—­í• : í†µê³„ì  ê²€ì¦ ë° íŒ¨í„´ ë¶„ì„
- ì±…ì„:
  - ë°ì´í„° ë¶„í¬ ê²€ì¦
  - ì´ìƒ íŒ¨í„´ íƒì§€
  - ê²€ì¦ ì„ê³„ê°’ ì„¤ì •

**`technical-documentation-writer`**
- ì—­í• : ê²€ì¦ ë¦¬í¬íŠ¸ ë¬¸ì„œí™”
- ì±…ì„:
  - ê²€ì¦ ê²°ê³¼ ë¦¬í¬íŠ¸ ì‘ì„±
  - ìœ„ë°˜ ì‚¬í•­ ìƒì„¸ ë¬¸ì„œí™”
  - ë°ì´í„° í’ˆì§ˆ ëŒ€ì‹œë³´ë“œ ìƒì„±

### 5.3 ê´€ë ¨ ìŠ¤í‚¬

**í•„ìˆ˜ ìŠ¤í‚¬**:
- pandas (ë°ì´í„° ì¡°ì‘ ë° ê²€ì¦)
- numpy (ìˆ˜ì¹˜ ì—°ì‚°)
- great-expectations (ìë™ ê²€ì¦)
- pandera (ìŠ¤í‚¤ë§ˆ ê¸°ë°˜ ê²€ì¦)

**ì„ íƒ ìŠ¤í‚¬**:
- regex (ë¬¸ìì—´ íŒ¨í„´ ê²€ì¦)
- scipy (í†µê³„ ê²€ì •)
- cerberus (ë°ì´í„° ê²€ì¦ í”„ë ˆì„ì›Œí¬)

---

## 6. í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ (Required Libraries)

### 6.1 í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬

```bash
# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬
pip install pandas>=2.0.0
pip install numpy>=1.24.0

# ê²€ì¦ í”„ë ˆì„ì›Œí¬
pip install great-expectations>=0.18.0
pip install pandera>=0.17.0

# í†µê³„ ë° ë¶„ì„
pip install scipy>=1.11.0
```

### 6.2 ì„ íƒ ë¼ì´ë¸ŒëŸ¬ë¦¬

```bash
# ì¶”ê°€ ê²€ì¦ ë„êµ¬
pip install cerberus>=1.3.5
pip install pydantic>=2.0.0
pip install jsonschema>=4.19.0

# ë°ì´í„°ë² ì´ìŠ¤ ì§€ì›
pip install sqlalchemy>=2.0.0
pip install psycopg2-binary>=2.9.0  # PostgreSQL
pip install pymysql>=1.1.0  # MySQL
```

### 6.3 ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „ ê´€ë¦¬

```python
# requirements-validation.txt
pandas==2.1.4
numpy==1.26.2
great-expectations==0.18.8
pandera==0.17.2
scipy==1.11.4
cerberus==1.3.5
pydantic==2.5.3
jsonschema==4.20.0
```

---

## 7. ì²´í¬í¬ì¸íŠ¸ (Checkpoints)

### 7.1 ê²€ì¦ ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] ë°ì´í„° ë¡œë“œ ì™„ë£Œ
- [ ] ì»¬ëŸ¼ ì´ë¦„ ë° íƒ€ì… í™•ì¸
- [ ] ì˜ˆìƒ ìŠ¤í‚¤ë§ˆ ì •ì˜ ì™„ë£Œ
- [ ] ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ë¬¸ì„œí™” ì™„ë£Œ
- [ ] ì°¸ì¡° í…Œì´ë¸” ì¤€ë¹„ ì™„ë£Œ

### 7.2 ê²€ì¦ ì¤‘ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] êµì°¨ í•„ë“œ ê²€ì¦ ì‹¤í–‰
  - [ ] ë‚ ì§œ ìˆœì„œ ê²€ì¦
  - [ ] ê³„ì‚°ì‹ ê²€ì¦
  - [ ] ì¡°ê±´ë¶€ ë¡œì§ ê²€ì¦

- [ ] ì°¸ì¡° ë¬´ê²°ì„± ê²€ì¦ ì‹¤í–‰
  - [ ] ì™¸ë˜ í‚¤ ê²€ì¦
  - [ ] ê³ ì•„ ë ˆì½”ë“œ ì‹ë³„
  - [ ] ì–‘ë°©í–¥ ê´€ê³„ ê²€ì¦

- [ ] ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ê²€ì¦ ì‹¤í–‰
  - [ ] ë„ë©”ì¸ë³„ ê·œì¹™ ì ìš©
  - [ ] ìœ„ë°˜ ì‚¬í•­ ê¸°ë¡
  - [ ] ì‹¬ê°ë„ ë¶„ë¥˜

### 7.3 ê²€ì¦ í›„ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] ê²€ì¦ ê²°ê³¼ ìš”ì•½ ìƒì„±
- [ ] ìœ„ë°˜ ì‚¬í•­ ìƒì„¸ ë¦¬í¬íŠ¸ ì‘ì„±
- [ ] ì‹¬ê°ë„ë³„ ìš°ì„ ìˆœìœ„ ê²°ì •
- [ ] í›„ì† ì¡°ì¹˜ ê³„íš ìˆ˜ë¦½
- [ ] ê²€ì¦ ë¦¬í¬íŠ¸ ê³µìœ 

### 7.4 í’ˆì§ˆ ê¸°ì¤€

**Level 1: Excellent (ìš°ìˆ˜)**
- âœ… ëª¨ë“  í•„ë“œ ê²€ì¦ í†µê³¼
- âœ… ì°¸ì¡° ë¬´ê²°ì„± 100% ìœ ì§€
- âœ… ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ìœ„ë°˜ < 1%

**Level 2: Good (ì–‘í˜¸)**
- âœ… í•„ë“œ ê²€ì¦ í†µê³¼ìœ¨ > 95%
- âœ… ì°¸ì¡° ë¬´ê²°ì„± > 98%
- âœ… ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ìœ„ë°˜ < 5%

**Level 3: Acceptable (í—ˆìš©)**
- âš ï¸ í•„ë“œ ê²€ì¦ í†µê³¼ìœ¨ > 90%
- âš ï¸ ì°¸ì¡° ë¬´ê²°ì„± > 95%
- âš ï¸ ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ìœ„ë°˜ < 10%

**Level 4: Poor (ë¶€ì¡±)**
- âŒ í•„ë“œ ê²€ì¦ í†µê³¼ìœ¨ < 90%
- âŒ ì°¸ì¡° ë¬´ê²°ì„± < 95%
- âŒ ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ìœ„ë°˜ > 10%

---

## 8. íŠ¸ëŸ¬ë¸”ìŠˆíŒ… (Troubleshooting)

### 8.1 ì¼ë°˜ì  ì˜¤ë¥˜

**ì˜¤ë¥˜ 1: ë‚ ì§œ í˜•ì‹ ë¶ˆì¼ì¹˜**
```python
# ë¬¸ì œ
df['date'] = pd.to_datetime(df['date'])  # ValueError

# í•´ê²°
df['date'] = pd.to_datetime(df['date'], errors='coerce', format='%Y-%m-%d')
# ë˜ëŠ” ìë™ í¬ë§· ê°ì§€
df['date'] = pd.to_datetime(df['date'], infer_datetime_format=True)
```

**ì˜¤ë¥˜ 2: ë¶€ë™ì†Œìˆ˜ì  ë¹„êµ ì˜¤ë¥˜**
```python
# ë¬¸ì œ
df['calculated'] == df['actual']  # False negatives due to floating point precision

# í•´ê²°
np.abs(df['calculated'] - df['actual']) < 0.01  # í—ˆìš© ì˜¤ì°¨ ì‚¬ìš©
# ë˜ëŠ”
np.isclose(df['calculated'], df['actual'], atol=0.01)
```

**ì˜¤ë¥˜ 3: NULL ê°’ ì²˜ë¦¬**
```python
# ë¬¸ì œ
df['amount'] > 0  # NaNì€ False ë°˜í™˜

# í•´ê²°
df['amount'].notna() & (df['amount'] > 0)
```

### 8.2 ì„±ëŠ¥ ìµœì í™”

**ë¬¸ì œ: ëŒ€ìš©ëŸ‰ ë°ì´í„° ê²€ì¦ ì†ë„ ì €í•˜**

```python
# í•´ê²°ì±… 1: ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬
def validate_in_chunks(df, chunk_size=10000):
    """
    ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¥¼ ì²­í¬ë¡œ ë‚˜ëˆ  ê²€ì¦
    """
    chunks = [df[i:i+chunk_size] for i in range(0, len(df), chunk_size)]
    results = []
    
    for i, chunk in enumerate(chunks):
        print(f"Processing chunk {i+1}/{len(chunks)}...")
        result = validate_chunk(chunk)
        results.append(result)
    
    return pd.concat(results)


# í•´ê²°ì±… 2: ë²¡í„°í™” ì—°ì‚° ì‚¬ìš©
# ëŠë¦¼
def slow_validation(df):
    violations = []
    for idx, row in df.iterrows():
        if row['quantity'] * row['price'] != row['total']:
            violations.append(idx)
    return violations

# ë¹ ë¦„
def fast_validation(df):
    mask = np.abs(df['quantity'] * df['price'] - df['total']) > 0.01
    return df[mask].index.tolist()


# í•´ê²°ì±… 3: ë³‘ë ¬ ì²˜ë¦¬
from multiprocessing import Pool

def parallel_validation(df, n_cores=4):
    """
    ë³‘ë ¬ ì²˜ë¦¬ë¡œ ê²€ì¦ ì†ë„ í–¥ìƒ
    """
    chunks = np.array_split(df, n_cores)
    
    with Pool(n_cores) as pool:
        results = pool.map(validate_chunk, chunks)
    
    return pd.concat(results)
```

### 8.3 ë©”ëª¨ë¦¬ ê´€ë¦¬

**ë¬¸ì œ: ëŒ€ìš©ëŸ‰ ë°ì´í„° ë©”ëª¨ë¦¬ ë¶€ì¡±**

```python
# í•´ê²°ì±… 1: ë°ì´í„° íƒ€ì… ìµœì í™”
def optimize_dtypes(df):
    """
    ë°ì´í„° íƒ€ì… ìµœì í™”ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
    """
    for col in df.select_dtypes(include=['int64']).columns:
        df[col] = pd.to_numeric(df[col], downcast='integer')
    
    for col in df.select_dtypes(include=['float64']).columns:
        df[col] = pd.to_numeric(df[col], downcast='float')
    
    for col in df.select_dtypes(include=['object']).columns:
        if df[col].nunique() / len(df) < 0.5:
            df[col] = df[col].astype('category')
    
    return df


# í•´ê²°ì±… 2: ë¶ˆí•„ìš”í•œ ì»¬ëŸ¼ ì œê±°
def validate_minimal_columns(df, required_columns):
    """
    ê²€ì¦ì— í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ë¡œë“œ
    """
    df_minimal = df[required_columns].copy()
    return validate(df_minimal)


# í•´ê²°ì±… 3: ì²­í¬ ì½ê¸°
def validate_from_file(filepath, chunk_size=10000):
    """
    íŒŒì¼ì„ ì²­í¬ë¡œ ì½ìœ¼ë©´ì„œ ê²€ì¦
    """
    violations = []
    
    for chunk in pd.read_csv(filepath, chunksize=chunk_size):
        chunk_violations = validate_chunk(chunk)
        violations.append(chunk_violations)
    
    return pd.concat(violations)
```

### 8.4 Great Expectations ê´€ë ¨ ì´ìŠˆ

**ë¬¸ì œ: Context ì´ˆê¸°í™” ì‹¤íŒ¨**
```python
# í•´ê²°
import great_expectations as gx

# ìƒˆ í”„ë¡œì íŠ¸ ì´ˆê¸°í™”
context = gx.get_context(mode="file")

# ë˜ëŠ” ê¸°ì¡´ í”„ë¡œì íŠ¸ ì‚¬ìš©
context = gx.get_context(context_root_dir="/path/to/gx/directory")
```

**ë¬¸ì œ: Expectation Suite ì €ì¥ ì‹¤íŒ¨**
```python
# í•´ê²°
validator.save_expectation_suite(
    discard_failed_expectations=False,
    overwrite_existing=True
)
```

---

## 9. ì°¸ê³  ìë£Œ (References)

### 9.1 ê³µì‹ ë¬¸ì„œ

**Great Expectations**
- ê³µì‹ ë¬¸ì„œ: https://docs.greatexpectations.io/
- GitHub: https://github.com/great-expectations/great_expectations
- íŠœí† ë¦¬ì–¼: https://docs.greatexpectations.io/docs/tutorials/

**Pandera**
- ê³µì‹ ë¬¸ì„œ: https://pandera.readthedocs.io/
- GitHub: https://github.com/unionai-oss/pandera
- ì˜ˆì‹œ: https://pandera.readthedocs.io/en/stable/examples.html

**Pandas**
- Data Validation Guide: https://pandas.pydata.org/docs/user_guide/indexing.html
- Boolean Indexing: https://pandas.pydata.org/docs/user_guide/indexing.html#boolean-indexing

### 9.2 ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤

**Data Quality Frameworks**
- Data Quality Dimensions: https://www.dataversity.net/six-key-data-quality-dimensions/
- Data Validation Best Practices: https://www.kdnuggets.com/2021/01/data-validation-best-practices.html

**Database Integrity**
- Referential Integrity: https://en.wikipedia.org/wiki/Referential_integrity
- Foreign Key Constraints: https://www.postgresql.org/docs/current/ddl-constraints.html

### 9.3 ê´€ë ¨ ë ˆí¼ëŸ°ìŠ¤

**Data-cleansing Skill ë ˆí¼ëŸ°ìŠ¤**:
- `01-data-quality-assessment.md`: ë°ì´í„° í’ˆì§ˆ í‰ê°€
- `02-missing-data-patterns.md`: ê²°ì¸¡ê°’ íŒ¨í„´ ë¶„ì„
- `12-quality-reporting.md`: í’ˆì§ˆ ë¦¬í¬íŠ¸ ìƒì„±
- `13-data-lineage.md`: ë°ì´í„° ë¦¬ë‹ˆì§€ ì¶”ì 

**Workflow ë§¤í•‘**:
- `data-cleansing-workflow.md` Phase 6 (lines 1183-1308)
  - Section 6.1: êµì°¨ í•„ë“œ ê²€ì¦
  - Section 6.2: ì°¸ì¡° ë¬´ê²°ì„± ê²€ì¦

---

## ë§ˆë¬´ë¦¬ (Conclusion)

ë°ì´í„° ê²€ì¦ì€ ë°ì´í„° í´ë Œì§•ì˜ ìµœì¢… ë‹¨ê³„ë¡œ, ë°ì´í„° í’ˆì§ˆì„ ë³´ì¥í•˜ëŠ” ê°€ì¥ ì¤‘ìš”í•œ í”„ë¡œì„¸ìŠ¤ì…ë‹ˆë‹¤. ì´ ë ˆí¼ëŸ°ìŠ¤ì—ì„œ ë‹¤ë£¬ êµì°¨ í•„ë“œ ê²€ì¦, ì°¸ì¡° ë¬´ê²°ì„± ê²€ì¦, ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ê²€ì¦ì„ ì²´ê³„ì ìœ¼ë¡œ ì ìš©í•˜ë©´ ê³ í’ˆì§ˆ ë°ì´í„°ë¥¼ í™•ë³´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**í•µì‹¬ ì›ì¹™**:
1. **ê³„ì¸µì  ê²€ì¦**: Field â†’ Cross-Field â†’ Record â†’ Dataset ìˆœì„œ
2. **ìë™í™”**: Great Expectations, Pandera í™œìš©
3. **ëª…í™•í•œ ê·œì¹™**: ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì¹™ ë¬¸ì„œí™”
4. **ì‹¬ê°ë„ ë¶„ë¥˜**: HIGH/MEDIUM/LOW ìš°ì„ ìˆœìœ„
5. **ì¬í˜„ì„±**: ëª¨ë“  ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸ ë²„ì „ ê´€ë¦¬

**ë‹¤ìŒ ë‹¨ê³„**:
- ê²€ì¦ í†µê³¼ ì‹œ: `12-quality-reporting.md`ë¡œ í’ˆì§ˆ ë¦¬í¬íŠ¸ ìƒì„±
- ê²€ì¦ ì‹¤íŒ¨ ì‹œ: ê° Phase ë ˆí¼ëŸ°ìŠ¤ë¡œ ëŒì•„ê°€ ë°ì´í„° í´ë Œì§• ì¬ìˆ˜í–‰
- ìë™í™”: `15-automation-pipeline.md`ë¡œ ê²€ì¦ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•

---

**ì‘ì„±ì**: Claude Code  
**ìµœì¢… ìˆ˜ì •ì¼**: 2025-01-26  
**ë²„ì „**: 1.0
