# 13. Data Lineage (ë°ì´í„° ë¦¬ë‹ˆì§€)

**ìƒì„±ì¼**: 2025-01-26  
**ë²„ì „**: 1.0  
**ì¹´í…Œê³ ë¦¬**: Data Governance & Traceability

---

## 1. ê°œìš” (Overview)

### 1.1 ëª©ì  (Purpose)

ë°ì´í„° ë¦¬ë‹ˆì§€(Data Lineage)ëŠ” ë°ì´í„°ì˜ ì¶œì²˜, ì´ë™ ê²½ë¡œ, ë³€í™˜ ê³¼ì •ì„ ì¶”ì í•˜ê³  ë¬¸ì„œí™”í•˜ëŠ” í”„ë¡œì„¸ìŠ¤ì…ë‹ˆë‹¤. ë°ì´í„° í´ë Œì§• ì‘ì—…ì˜ íˆ¬ëª…ì„±, ì¬í˜„ì„±, ê°ì‚¬ ê°€ëŠ¥ì„±ì„ ë³´ì¥í•˜ë©°, ë°ì´í„° ê±°ë²„ë„ŒìŠ¤ì˜ í•µì‹¬ ìš”ì†Œì…ë‹ˆë‹¤.

### 1.2 ì ìš© ì‹œê¸° (When to Apply)

**í•„ìˆ˜ ì ìš© ì‹œì **:
- âœ… ëª¨ë“  ë°ì´í„° ë³€í™˜ ì‘ì—… ì‹œ
- âœ… ê·œì œ ì¤€ìˆ˜ê°€ í•„ìš”í•œ í”„ë¡œì íŠ¸ (ê¸ˆìœµ, ì˜ë£Œ ë“±)
- âœ… í”„ë¡œë•ì…˜ í™˜ê²½ ë°ì´í„° ì²˜ë¦¬
- âœ… ê°ì‚¬(audit) ìš”êµ¬ì‚¬í•­ì´ ìˆëŠ” ê²½ìš°

**ê¶Œì¥ ì ìš©**:
- ğŸ”¹ ë³µì¡í•œ ë°ì´í„° íŒŒì´í”„ë¼ì¸
- ğŸ”¹ ì—¬ëŸ¬ íŒ€ì´ í˜‘ì—…í•˜ëŠ” í”„ë¡œì íŠ¸
- ğŸ”¹ ì¥ê¸° ìš´ì˜ ë°ì´í„° ì‹œìŠ¤í…œ
- ğŸ”¹ ë°ì´í„° í’ˆì§ˆ ì´ìŠˆ ë””ë²„ê¹…

### 1.3 ë¦¬ë‹ˆì§€ ë ˆë²¨ (Lineage Levels)

```
Level 1: Technical Lineage (ê¸°ìˆ ì  ë¦¬ë‹ˆì§€)
â””â”€â”€ ì½”ë“œ ìˆ˜ì¤€ì˜ ë³€í™˜ ê¸°ë¡ (í•¨ìˆ˜, íŒŒë¼ë¯¸í„°)

Level 2: Operational Lineage (ìš´ì˜ ë¦¬ë‹ˆì§€)
â””â”€â”€ ì‹¤í–‰ ì´ë ¥ (ì‹œê°„, ì‚¬ìš©ì, ê²°ê³¼)

Level 3: Business Lineage (ë¹„ì¦ˆë‹ˆìŠ¤ ë¦¬ë‹ˆì§€)
â””â”€â”€ ë¹„ì¦ˆë‹ˆìŠ¤ ì»¨í…ìŠ¤íŠ¸ (ëª©ì , ì˜í–¥, ìŠ¹ì¸)

Level 4: Data Lineage (ë°ì´í„° ë¦¬ë‹ˆì§€)
â””â”€â”€ ë°ì´í„° íë¦„ ë° ì˜ì¡´ì„± (ì†ŒìŠ¤, íƒ€ê²Ÿ, ê´€ê³„)
```

---

## 2. ì´ë¡ ì  ë°°ê²½ (Theoretical Background)

### 2.1 ë°ì´í„° ë¦¬ë‹ˆì§€ì˜ ì¤‘ìš”ì„±

**1. íˆ¬ëª…ì„± (Transparency)**
- ë°ì´í„°ê°€ ì–´ë–»ê²Œ ë³€í™˜ë˜ì—ˆëŠ”ì§€ ëª…í™•íˆ íŒŒì•…
- ì´í•´ê´€ê³„ìì—ê²Œ ì‹ ë¢° ì œê³µ

**2. ì¬í˜„ì„± (Reproducibility)**
- ë™ì¼í•œ ì…ë ¥ì— ëŒ€í•´ ë™ì¼í•œ ì¶œë ¥ ë³´ì¥
- ê³¼í•™ì  ë°©ë²•ë¡  ì¤€ìˆ˜

**3. ë””ë²„ê¹… (Debugging)**
- ë°ì´í„° í’ˆì§ˆ ì´ìŠˆ ë°œìƒ ì‹œ ì›ì¸ ì¶”ì 
- ë¬¸ì œ ë°œìƒ ì§€ì  ì •í™•íˆ ì‹ë³„

**4. ê·œì œ ì¤€ìˆ˜ (Compliance)**
- GDPR, HIPAA ë“± ê·œì œ ìš”êµ¬ì‚¬í•­ ì¶©ì¡±
- ê°ì‚¬ ì¶”ì (audit trail) ì œê³µ

**5. ì˜í–¥ ë¶„ì„ (Impact Analysis)**
- ë³€ê²½ ì‚¬í•­ì˜ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì˜í–¥ í‰ê°€
- ì˜ì¡´ì„± ê´€ë¦¬

### 2.2 ë¦¬ë‹ˆì§€ êµ¬ì„± ìš”ì†Œ

**ë©”íƒ€ë°ì´í„° (Metadata)**:
```python
{
    "operation_id": "unique_identifier",
    "timestamp": "2024-01-26 10:30:00",
    "operation_type": "impute_missing_values",
    "user": "data_engineer_1",
    "parameters": {
        "method": "knn",
        "n_neighbors": 5
    },
    "input_data": {
        "shape": (10000, 15),
        "hash": "abc123...",
        "source": "raw_data.csv"
    },
    "output_data": {
        "shape": (10000, 15),
        "hash": "def456...",
        "target": "imputed_data.csv"
    },
    "metrics": {
        "rows_affected": 1500,
        "execution_time": 2.5
    },
    "status": "success"
}
```

### 2.3 ë¦¬ë‹ˆì§€ ì¶”ì  íŒ¨í„´

**Pattern 1: Linear Lineage (ì„ í˜• ë¦¬ë‹ˆì§€)**
```
Raw Data â†’ Cleaning â†’ Transformation â†’ Output
```

**Pattern 2: Branching Lineage (ë¶„ê¸° ë¦¬ë‹ˆì§€)**
```
                  â†’ Branch A â†’ Output A
Raw Data â†’ Split â†’ Branch B â†’ Output B
                  â†’ Branch C â†’ Output C
```

**Pattern 3: Merging Lineage (ë³‘í•© ë¦¬ë‹ˆì§€)**
```
Source A â†’ Cleaning A â†˜
                      â†’ Join â†’ Output
Source B â†’ Cleaning B â†—
```

---

## 3. êµ¬í˜„ (Implementation)

### 3.1 DataLineage í´ë˜ìŠ¤

```python
import pandas as pd
import numpy as np
from datetime import datetime
from typing import Dict, List, Any, Optional
import hashlib
import json
import pickle

class DataLineage:
    """
    ë°ì´í„° ë³€í™˜ ì´ë ¥ ì¶”ì  í´ë˜ìŠ¤
    Track data transformation history
    """
    
    def __init__(
        self,
        dataset_name: str,
        initial_df: pd.DataFrame,
        project_name: str = "Data Cleansing Project"
    ):
        """
        Parameters:
        -----------
        dataset_name : str
            ë°ì´í„°ì…‹ ì´ë¦„
        initial_df : pd.DataFrame
            ì´ˆê¸° ë°ì´í„°í”„ë ˆì„
        project_name : str
            í”„ë¡œì íŠ¸ ì´ë¦„
        """
        self.dataset_name = dataset_name
        self.project_name = project_name
        self.history = []
        self.current_df = initial_df.copy()
        
        # ì´ˆê¸° ìƒíƒœ ê¸°ë¡
        self._log_initial_state(initial_df)
    
    def _log_initial_state(self, df: pd.DataFrame):
        """
        ì´ˆê¸° ë°ì´í„° ìƒíƒœ ê¸°ë¡
        Log initial data state
        """
        initial_entry = {
            'operation_id': self._generate_id(),
            'timestamp': datetime.now().isoformat(),
            'operation_type': 'initial_load',
            'operation_name': 'Load Raw Data',
            'parameters': {},
            'data_before': self._capture_data_snapshot(df),
            'data_after': self._capture_data_snapshot(df),
            'changes': {
                'rows_added': 0,
                'rows_removed': 0,
                'columns_added': 0,
                'columns_removed': 0
            },
            'execution_time_seconds': 0.0,
            'status': 'success',
            'error': None,
            'user': 'system',
            'description': f'Initial load of {self.dataset_name}'
        }
        
        self.history.append(initial_entry)
    
    def _generate_id(self) -> str:
        """
        ê³ ìœ  ID ìƒì„±
        Generate unique ID
        """
        import uuid
        return str(uuid.uuid4())[:8]
    
    def _capture_data_snapshot(self, df: pd.DataFrame) -> Dict[str, Any]:
        """
        ë°ì´í„° ìŠ¤ëƒ…ìƒ· ìº¡ì²˜
        Capture data snapshot
        """
        snapshot = {
            'shape': df.shape,
            'rows': len(df),
            'columns': len(df.columns),
            'column_names': list(df.columns),
            'dtypes': {col: str(dtype) for col, dtype in df.dtypes.items()},
            'memory_mb': round(df.memory_usage(deep=True).sum() / 1024**2, 2),
            'hash': self._calculate_dataframe_hash(df),
            'missing_values': int(df.isnull().sum().sum()),
            'missing_rate': round(100 * df.isnull().sum().sum() / df.size, 2)
        }
        
        return snapshot
    
    def _calculate_dataframe_hash(self, df: pd.DataFrame) -> str:
        """
        ë°ì´í„°í”„ë ˆì„ í•´ì‹œ ê³„ì‚° (ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦ìš©)
        Calculate dataframe hash for integrity verification
        """
        try:
            # ë°ì´í„°í”„ë ˆì„ì„ ë°”ì´ë„ˆë¦¬ë¡œ ë³€í™˜í•˜ì—¬ í•´ì‹œ ê³„ì‚°
            df_bytes = pickle.dumps(df)
            hash_obj = hashlib.md5(df_bytes)
            return hash_obj.hexdigest()[:16]
        except Exception:
            return "hash_error"
    
    def log_operation(
        self,
        operation_type: str,
        operation_name: str,
        df_after: pd.DataFrame,
        parameters: Dict[str, Any] = None,
        description: str = None,
        user: str = "system",
        execution_time: float = 0.0
    ):
        """
        ë°ì´í„° ë³€í™˜ ì‘ì—… ê¸°ë¡
        Log a data transformation operation
        
        Parameters:
        -----------
        operation_type : str
            ì‘ì—… ìœ í˜• (ì˜ˆ: 'impute_missing', 'remove_outliers')
        operation_name : str
            ì‘ì—… ì´ë¦„ (ì‚¬ëŒì´ ì½ê¸° ì‰¬ìš´ ì´ë¦„)
        df_after : pd.DataFrame
            ë³€í™˜ í›„ ë°ì´í„°í”„ë ˆì„
        parameters : dict, optional
            ì‘ì—… íŒŒë¼ë¯¸í„°
        description : str, optional
            ì‘ì—… ì„¤ëª…
        user : str, optional
            ì‘ì—… ìˆ˜í–‰ì
        execution_time : float, optional
            ì‹¤í–‰ ì‹œê°„ (ì´ˆ)
            
        Example:
        --------
        >>> lineage.log_operation(
        ...     operation_type='impute_missing',
        ...     operation_name='KNN Imputation',
        ...     df_after=df_imputed,
        ...     parameters={'method': 'knn', 'n_neighbors': 5},
        ...     description='Imputed missing values using KNN'
        ... )
        """
        # Before ìŠ¤ëƒ…ìƒ· (í˜„ì¬ ìƒíƒœ)
        data_before = self._capture_data_snapshot(self.current_df)
        
        # After ìŠ¤ëƒ…ìƒ·
        data_after = self._capture_data_snapshot(df_after)
        
        # ë³€ê²½ ì‚¬í•­ ê³„ì‚°
        changes = self._calculate_changes(self.current_df, df_after)
        
        # ë¦¬ë‹ˆì§€ ì—”íŠ¸ë¦¬ ìƒì„±
        entry = {
            'operation_id': self._generate_id(),
            'timestamp': datetime.now().isoformat(),
            'operation_type': operation_type,
            'operation_name': operation_name,
            'parameters': parameters or {},
            'data_before': data_before,
            'data_after': data_after,
            'changes': changes,
            'execution_time_seconds': execution_time,
            'status': 'success',
            'error': None,
            'user': user,
            'description': description or operation_name
        }
        
        self.history.append(entry)
        
        # í˜„ì¬ ìƒíƒœ ì—…ë°ì´íŠ¸
        self.current_df = df_after.copy()
        
        print(f"âœ… Logged operation: {operation_name} (ID: {entry['operation_id']})")
    
    def _calculate_changes(
        self,
        df_before: pd.DataFrame,
        df_after: pd.DataFrame
    ) -> Dict[str, Any]:
        """
        ë³€ê²½ ì‚¬í•­ ê³„ì‚°
        Calculate changes between before and after
        """
        # í–‰ ë³€í™”
        rows_added = max(0, len(df_after) - len(df_before))
        rows_removed = max(0, len(df_before) - len(df_after))
        
        # ì»¬ëŸ¼ ë³€í™”
        cols_before = set(df_before.columns)
        cols_after = set(df_after.columns)
        
        columns_added = list(cols_after - cols_before)
        columns_removed = list(cols_before - cols_after)
        
        # ê³µí†µ ì»¬ëŸ¼ì˜ ë³€ê²½ëœ ì…€ ê°œìˆ˜
        common_cols = cols_before & cols_after
        cells_modified = 0
        
        for col in common_cols:
            if col in df_before.columns and col in df_after.columns:
                # ì¸ë±ìŠ¤ ì •ë ¬ í›„ ë¹„êµ
                min_len = min(len(df_before), len(df_after))
                if min_len > 0:
                    try:
                        diff = df_before[col].iloc[:min_len] != df_after[col].iloc[:min_len]
                        cells_modified += diff.sum()
                    except:
                        pass  # ë¹„êµ ë¶ˆê°€ëŠ¥í•œ íƒ€ì…ì€ ìŠ¤í‚µ
        
        changes = {
            'rows_added': int(rows_added),
            'rows_removed': int(rows_removed),
            'columns_added': columns_added,
            'columns_removed': columns_removed,
            'cells_modified': int(cells_modified)
        }
        
        return changes
    
    def get_lineage_report(self) -> pd.DataFrame:
        """
        ë¦¬ë‹ˆì§€ ë¦¬í¬íŠ¸ ìƒì„±
        Generate lineage report
        
        Returns:
        --------
        report_df : pd.DataFrame
            ë¦¬ë‹ˆì§€ ë¦¬í¬íŠ¸ í…Œì´ë¸”
        """
        if not self.history:
            return pd.DataFrame()
        
        # ì£¼ìš” í•„ë“œë§Œ ì¶”ì¶œ
        report_data = []
        for entry in self.history:
            report_data.append({
                'operation_id': entry['operation_id'],
                'timestamp': entry['timestamp'],
                'operation_name': entry['operation_name'],
                'operation_type': entry['operation_type'],
                'rows_before': entry['data_before']['rows'],
                'rows_after': entry['data_after']['rows'],
                'cols_before': entry['data_before']['columns'],
                'cols_after': entry['data_after']['columns'],
                'rows_changed': entry['changes']['rows_added'] - entry['changes']['rows_removed'],
                'execution_time': entry['execution_time_seconds'],
                'status': entry['status'],
                'user': entry['user']
            })
        
        report_df = pd.DataFrame(report_data)
        
        return report_df
    
    def get_operation_details(self, operation_id: str) -> Dict[str, Any]:
        """
        íŠ¹ì • ì‘ì—…ì˜ ìƒì„¸ ì •ë³´ ì¡°íšŒ
        Get detailed information about a specific operation
        """
        for entry in self.history:
            if entry['operation_id'] == operation_id:
                return entry
        
        return None
    
    def export_lineage(self, filepath: str):
        """
        ë¦¬ë‹ˆì§€ ì „ì²´ ë‚´ì—­ì„ íŒŒì¼ë¡œ ì €ì¥
        Export full lineage to file
        
        Parameters:
        -----------
        filepath : str
            ì €ì¥ ê²½ë¡œ (JSON í˜•ì‹)
        """
        export_data = {
            'project_name': self.project_name,
            'dataset_name': self.dataset_name,
            'created_at': self.history[0]['timestamp'] if self.history else None,
            'last_updated': self.history[-1]['timestamp'] if self.history else None,
            'total_operations': len(self.history),
            'history': self.history
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… Lineage exported to {filepath}")
    
    def import_lineage(self, filepath: str):
        """
        ë¦¬ë‹ˆì§€ ë‚´ì—­ ë¶ˆëŸ¬ì˜¤ê¸°
        Import lineage from file
        """
        with open(filepath, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        self.project_name = data['project_name']
        self.dataset_name = data['dataset_name']
        self.history = data['history']
        
        print(f"âœ… Lineage imported from {filepath}")
        print(f"   Total operations: {len(self.history)}")
    
    def get_summary(self) -> Dict[str, Any]:
        """
        ë¦¬ë‹ˆì§€ ìš”ì•½ ì •ë³´
        Get lineage summary
        """
        if not self.history:
            return {}
        
        first_entry = self.history[0]
        last_entry = self.history[-1]
        
        summary = {
            'project_name': self.project_name,
            'dataset_name': self.dataset_name,
            'total_operations': len(self.history),
            'start_time': first_entry['timestamp'],
            'end_time': last_entry['timestamp'],
            'initial_rows': first_entry['data_before']['rows'],
            'final_rows': last_entry['data_after']['rows'],
            'rows_changed': last_entry['data_after']['rows'] - first_entry['data_before']['rows'],
            'initial_columns': first_entry['data_before']['columns'],
            'final_columns': last_entry['data_after']['columns'],
            'total_execution_time': sum(e['execution_time_seconds'] for e in self.history),
            'operations_list': [e['operation_name'] for e in self.history]
        }
        
        return summary


# ì‚¬ìš© ì˜ˆì‹œ
def demo_lineage_tracking():
    """
    ë°ì´í„° ë¦¬ë‹ˆì§€ ì¶”ì  ë°ëª¨
    """
    # ìƒ˜í”Œ ë°ì´í„°
    np.random.seed(42)
    df = pd.DataFrame({
        'id': range(1, 1001),
        'value': np.random.randn(1000),
        'category': np.random.choice(['A', 'B', 'C'], 1000)
    })
    
    # ê²°ì¸¡ê°’ ì‚½ì…
    df.loc[df.sample(frac=0.1).index, 'value'] = np.nan
    
    # ë¦¬ë‹ˆì§€ ì¶”ì  ì‹œì‘
    lineage = DataLineage(
        dataset_name='sample_data',
        initial_df=df,
        project_name='Data Cleansing Demo'
    )
    
    # ì‘ì—… 1: ê²°ì¸¡ê°’ ëŒ€ì²´
    df_imputed = df.copy()
    df_imputed['value'].fillna(df_imputed['value'].mean(), inplace=True)
    
    lineage.log_operation(
        operation_type='impute_missing',
        operation_name='Mean Imputation',
        df_after=df_imputed,
        parameters={'method': 'mean', 'column': 'value'},
        description='Filled missing values with mean',
        execution_time=0.5
    )
    
    # ì‘ì—… 2: ì´ìƒì¹˜ ì œê±°
    Q1 = df_imputed['value'].quantile(0.25)
    Q3 = df_imputed['value'].quantile(0.75)
    IQR = Q3 - Q1
    df_no_outliers = df_imputed[
        (df_imputed['value'] >= Q1 - 1.5 * IQR) &
        (df_imputed['value'] <= Q3 + 1.5 * IQR)
    ]
    
    lineage.log_operation(
        operation_type='remove_outliers',
        operation_name='IQR Outlier Removal',
        df_after=df_no_outliers,
        parameters={'method': 'IQR', 'multiplier': 1.5},
        description='Removed outliers using IQR method',
        execution_time=0.3
    )
    
    # ì‘ì—… 3: ìƒˆ ì»¬ëŸ¼ ì¶”ê°€
    df_final = df_no_outliers.copy()
    df_final['value_squared'] = df_final['value'] ** 2
    
    lineage.log_operation(
        operation_type='feature_engineering',
        operation_name='Add Squared Feature',
        df_after=df_final,
        parameters={'new_column': 'value_squared', 'formula': 'value ** 2'},
        description='Added squared value feature',
        execution_time=0.1
    )
    
    # ë¦¬ë‹ˆì§€ ë¦¬í¬íŠ¸ í™•ì¸
    print("\n" + "="*80)
    print("LINEAGE REPORT")
    print("="*80)
    report = lineage.get_lineage_report()
    print(report)
    
    # ìš”ì•½ ì •ë³´
    print("\n" + "="*80)
    print("LINEAGE SUMMARY")
    print("="*80)
    summary = lineage.get_summary()
    for key, value in summary.items():
        print(f"{key}: {value}")
    
    # ë¦¬ë‹ˆì§€ ì €ì¥
    lineage.export_lineage('lineage_history.json')
    
    return lineage
```

### 3.2 ì‹œê°í™” ë„êµ¬

```python
import matplotlib.pyplot as plt
import networkx as nx
from matplotlib.patches import FancyBboxPatch

class LineageVisualizer:
    """
    ë°ì´í„° ë¦¬ë‹ˆì§€ ì‹œê°í™” í´ë˜ìŠ¤
    Visualize data lineage
    """
    
    def __init__(self, lineage: DataLineage):
        self.lineage = lineage
        self.history = lineage.history
    
    def plot_data_flow(self, save_path: str = None):
        """
        ë°ì´í„° íë¦„ ë‹¤ì´ì–´ê·¸ë¨
        Plot data flow diagram
        """
        if not self.history:
            print("No lineage history to visualize")
            return
        
        fig, ax = plt.subplots(figsize=(14, 8))
        
        # ë…¸ë“œ ìƒì„± (ê° ì‘ì—…)
        n_operations = len(self.history)
        node_positions = {}
        
        for i, entry in enumerate(self.history):
            x = i
            y = 0
            node_positions[i] = (x, y)
            
            # ë…¸ë“œ ê·¸ë¦¬ê¸°
            if entry['operation_type'] == 'initial_load':
                color = '#E8F5E9'
                edge_color = '#4CAF50'
            elif entry['status'] == 'success':
                color = '#E3F2FD'
                edge_color = '#2196F3'
            else:
                color = '#FFEBEE'
                edge_color = '#F44336'
            
            # ë°•ìŠ¤ ê·¸ë¦¬ê¸°
            box = FancyBboxPatch(
                (x - 0.4, y - 0.3),
                0.8,
                0.6,
                boxstyle="round,pad=0.05",
                facecolor=color,
                edgecolor=edge_color,
                linewidth=2
            )
            ax.add_patch(box)
            
            # ì‘ì—… ì´ë¦„
            ax.text(x, y + 0.1, entry['operation_name'],
                   ha='center', va='center', fontsize=9, fontweight='bold',
                   wrap=True)
            
            # ë°ì´í„° í¬ê¸°
            rows_after = entry['data_after']['rows']
            cols_after = entry['data_after']['columns']
            ax.text(x, y - 0.15, f"{rows_after} Ã— {cols_after}",
                   ha='center', va='center', fontsize=8, color='#666')
            
            # í™”ì‚´í‘œ (ë‹¤ìŒ ì‘ì—…ìœ¼ë¡œ)
            if i < n_operations - 1:
                ax.annotate('',
                          xy=(x + 0.6, y),
                          xytext=(x + 0.4, y),
                          arrowprops=dict(arrowstyle='->', lw=2, color='#333'))
        
        # ì¶• ì„¤ì •
        ax.set_xlim(-0.5, n_operations - 0.5)
        ax.set_ylim(-0.5, 0.5)
        ax.axis('off')
        
        plt.title('Data Transformation Flow', fontsize=16, fontweight='bold', pad=20)
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"Flow diagram saved to {save_path}")
        
        plt.show()
    
    def plot_metrics_timeline(self, save_path: str = None):
        """
        ë©”íŠ¸ë¦­ íƒ€ì„ë¼ì¸ ì°¨íŠ¸
        Plot metrics timeline
        """
        if not self.history:
            return
        
        fig, axes = plt.subplots(3, 1, figsize=(14, 10))
        
        # ë°ì´í„° ì¶”ì¶œ
        timestamps = [i for i in range(len(self.history))]
        operation_names = [e['operation_name'] for e in self.history]
        rows = [e['data_after']['rows'] for e in self.history]
        columns = [e['data_after']['columns'] for e in self.history]
        missing_rates = [e['data_after']['missing_rate'] for e in self.history]
        
        # 1. í–‰ ê°œìˆ˜ ë³€í™”
        ax = axes[0]
        ax.plot(timestamps, rows, marker='o', linewidth=2, markersize=8, color='#2196F3')
        ax.fill_between(timestamps, rows, alpha=0.3, color='#2196F3')
        ax.set_ylabel('Number of Rows', fontsize=12, fontweight='bold')
        ax.set_title('Data Volume Over Operations', fontsize=14, fontweight='bold')
        ax.grid(True, alpha=0.3)
        ax.set_xticks(timestamps)
        ax.set_xticklabels([])
        
        # 2. ì»¬ëŸ¼ ê°œìˆ˜ ë³€í™”
        ax = axes[1]
        ax.plot(timestamps, columns, marker='s', linewidth=2, markersize=8, color='#4CAF50')
        ax.fill_between(timestamps, columns, alpha=0.3, color='#4CAF50')
        ax.set_ylabel('Number of Columns', fontsize=12, fontweight='bold')
        ax.set_title('Feature Count Over Operations', fontsize=14, fontweight='bold')
        ax.grid(True, alpha=0.3)
        ax.set_xticks(timestamps)
        ax.set_xticklabels([])
        
        # 3. ê²°ì¸¡ë¥  ë³€í™”
        ax = axes[2]
        ax.plot(timestamps, missing_rates, marker='^', linewidth=2, markersize=8, color='#FF9800')
        ax.fill_between(timestamps, missing_rates, alpha=0.3, color='#FF9800')
        ax.set_ylabel('Missing Rate (%)', fontsize=12, fontweight='bold')
        ax.set_xlabel('Operations', fontsize=12, fontweight='bold')
        ax.set_title('Data Completeness Over Operations', fontsize=14, fontweight='bold')
        ax.grid(True, alpha=0.3)
        ax.set_xticks(timestamps)
        ax.set_xticklabels(operation_names, rotation=45, ha='right')
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"Timeline chart saved to {save_path}")
        
        plt.show()
    
    def plot_lineage_graph(self, save_path: str = None):
        """
        NetworkXë¥¼ ì‚¬ìš©í•œ ë¦¬ë‹ˆì§€ ê·¸ë˜í”„
        Plot lineage graph using NetworkX
        """
        if not self.history:
            return
        
        # ê·¸ë˜í”„ ìƒì„±
        G = nx.DiGraph()
        
        # ë…¸ë“œ ë° ì—£ì§€ ì¶”ê°€
        for i, entry in enumerate(self.history):
            node_id = f"op_{i}"
            G.add_node(
                node_id,
                label=entry['operation_name'],
                rows=entry['data_after']['rows'],
                cols=entry['data_after']['columns']
            )
            
            # ì´ì „ ì‘ì—…ê³¼ ì—°ê²°
            if i > 0:
                prev_node_id = f"op_{i-1}"
                G.add_edge(prev_node_id, node_id)
        
        # ë ˆì´ì•„ì›ƒ
        pos = nx.spring_layout(G, k=2, iterations=50)
        
        # ê·¸ë¦¬ê¸°
        fig, ax = plt.subplots(figsize=(16, 10))
        
        # ë…¸ë“œ
        nx.draw_networkx_nodes(
            G, pos, node_size=3000, node_color='#E3F2FD',
            edgecolors='#2196F3', linewidths=2, ax=ax
        )
        
        # ì—£ì§€
        nx.draw_networkx_edges(
            G, pos, edge_color='#666', arrows=True,
            arrowsize=20, arrowstyle='->', width=2, ax=ax
        )
        
        # ë ˆì´ë¸”
        labels = {node: G.nodes[node]['label'] for node in G.nodes()}
        nx.draw_networkx_labels(
            G, pos, labels, font_size=10, font_weight='bold', ax=ax
        )
        
        # ë°ì´í„° í¬ê¸° í‘œì‹œ
        for node in G.nodes():
            x, y = pos[node]
            rows = G.nodes[node]['rows']
            cols = G.nodes[node]['cols']
            ax.text(x, y - 0.15, f"{rows} Ã— {cols}",
                   ha='center', fontsize=8, color='#666')
        
        ax.axis('off')
        plt.title('Data Lineage Dependency Graph', fontsize=16, fontweight='bold', pad=20)
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"Lineage graph saved to {save_path}")
        
        plt.show()


# ì‚¬ìš© ì˜ˆì‹œ
def visualize_lineage_demo(lineage):
    """
    ë¦¬ë‹ˆì§€ ì‹œê°í™” ë°ëª¨
    """
    visualizer = LineageVisualizer(lineage)
    
    print("Creating lineage visualizations...")
    
    # 1. ë°ì´í„° íë¦„ ë‹¤ì´ì–´ê·¸ë¨
    print("\n1. Data flow diagram...")
    visualizer.plot_data_flow(save_path='lineage_flow.png')
    
    # 2. ë©”íŠ¸ë¦­ íƒ€ì„ë¼ì¸
    print("\n2. Metrics timeline...")
    visualizer.plot_metrics_timeline(save_path='lineage_timeline.png')
    
    # 3. ë¦¬ë‹ˆì§€ ê·¸ë˜í”„
    print("\n3. Lineage dependency graph...")
    visualizer.plot_lineage_graph(save_path='lineage_graph.png')
    
    print("\nâœ… All visualizations created successfully!")
```

### 3.3 ì¬í˜„ ê°€ëŠ¥ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±

```python
class ReproducibleScriptGenerator:
    """
    ì¬í˜„ ê°€ëŠ¥í•œ Python ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
    Generate reproducible Python scripts from lineage
    """
    
    def __init__(self, lineage: DataLineage):
        self.lineage = lineage
        self.history = lineage.history
    
    def generate_script(
        self,
        output_filepath: str = 'reproduce_cleansing.py',
        include_comments: bool = True
    ) -> str:
        """
        ì¬í˜„ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
        Generate reproduction script
        
        Parameters:
        -----------
        output_filepath : str
            ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
        include_comments : bool
            ì£¼ì„ í¬í•¨ ì—¬ë¶€
            
        Returns:
        --------
        script : str
            ìƒì„±ëœ ìŠ¤í¬ë¦½íŠ¸ ë‚´ìš©
        """
        script_lines = []
        
        # í—¤ë”
        script_lines.append("#!/usr/bin/env python3")
        script_lines.append('"""')
        script_lines.append(f"Data Cleansing Reproduction Script")
        script_lines.append(f"Project: {self.lineage.project_name}")
        script_lines.append(f"Dataset: {self.lineage.dataset_name}")
        script_lines.append(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        script_lines.append('"""')
        script_lines.append("")
        
        # Imports
        script_lines.append("import pandas as pd")
        script_lines.append("import numpy as np")
        script_lines.append("from sklearn.impute import SimpleImputer, KNNImputer")
        script_lines.append("")
        
        # Main function
        script_lines.append("def reproduce_cleansing(input_filepath):")
        script_lines.append('    """Reproduce data cleansing process"""')
        script_lines.append("")
        
        # ê° ì‘ì—…ì„ ì½”ë“œë¡œ ë³€í™˜
        for i, entry in enumerate(self.history):
            if entry['operation_type'] == 'initial_load':
                continue  # ì´ˆê¸° ë¡œë“œëŠ” ìŠ¤í‚µ
            
            if include_comments:
                script_lines.append(f"    # Operation {i}: {entry['operation_name']}")
                script_lines.append(f"    # {entry['description']}")
            
            # ì‘ì—… íƒ€ì…ë³„ ì½”ë“œ ìƒì„±
            code = self._generate_code_for_operation(entry)
            script_lines.extend(["    " + line for line in code.split("\n")])
            script_lines.append("")
        
        script_lines.append("    return df")
        script_lines.append("")
        
        # Main block
        script_lines.append("if __name__ == '__main__':")
        script_lines.append("    import sys")
        script_lines.append("    ")
        script_lines.append("    if len(sys.argv) < 2:")
        script_lines.append("        print('Usage: python reproduce_cleansing.py <input_file>')")
        script_lines.append("        sys.exit(1)")
        script_lines.append("    ")
        script_lines.append("    input_file = sys.argv[1]")
        script_lines.append("    df = pd.read_csv(input_file)")
        script_lines.append("    ")
        script_lines.append("    df_cleaned = reproduce_cleansing(df)")
        script_lines.append("    ")
        script_lines.append("    df_cleaned.to_csv('cleaned_output.csv', index=False)")
        script_lines.append("    print('âœ… Data cleansing completed!')")
        
        # ìŠ¤í¬ë¦½íŠ¸ ì¡°í•©
        script = "\n".join(script_lines)
        
        # íŒŒì¼ ì €ì¥
        with open(output_filepath, 'w', encoding='utf-8') as f:
            f.write(script)
        
        print(f"âœ… Reproducible script generated: {output_filepath}")
        
        return script
    
    def _generate_code_for_operation(self, entry: Dict[str, Any]) -> str:
        """
        ì‘ì—… íƒ€ì…ë³„ ì½”ë“œ ìƒì„±
        Generate code for specific operation type
        """
        op_type = entry['operation_type']
        params = entry['parameters']
        
        if op_type == 'impute_missing':
            method = params.get('method', 'mean')
            column = params.get('column', 'all')
            
            if method == 'mean':
                code = f"df['{column}'].fillna(df['{column}'].mean(), inplace=True)"
            elif method == 'median':
                code = f"df['{column}'].fillna(df['{column}'].median(), inplace=True)"
            elif method == 'mode':
                code = f"df['{column}'].fillna(df['{column}'].mode()[0], inplace=True)"
            elif method == 'knn':
                n_neighbors = params.get('n_neighbors', 5)
                code = f"""imputer = KNNImputer(n_neighbors={n_neighbors})
df[['{column}']] = imputer.fit_transform(df[['{column}']]) """
            else:
                code = f"# Custom imputation method: {method}"
        
        elif op_type == 'remove_outliers':
            method = params.get('method', 'IQR')
            
            if method == 'IQR':
                multiplier = params.get('multiplier', 1.5)
                code = f"""Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
IQR = Q3 - Q1
df = df[~((df < (Q1 - {multiplier} * IQR)) | (df > (Q3 + {multiplier} * IQR))).any(axis=1)]"""
            else:
                code = f"# Custom outlier removal method: {method}"
        
        elif op_type == 'remove_duplicates':
            subset = params.get('subset')
            keep = params.get('keep', 'first')
            
            if subset:
                code = f"df.drop_duplicates(subset={subset}, keep='{keep}', inplace=True)"
            else:
                code = f"df.drop_duplicates(keep='{keep}', inplace=True)"
        
        elif op_type == 'feature_engineering':
            new_column = params.get('new_column')
            formula = params.get('formula')
            
            if new_column and formula:
                code = f"df['{new_column}'] = {formula}"
            else:
                code = "# Feature engineering operation"
        
        else:
            code = f"# Operation: {op_type}\n# Parameters: {params}"
        
        return code


# ì‚¬ìš© ì˜ˆì‹œ
def generate_reproduction_script_demo(lineage):
    """
    ì¬í˜„ ìŠ¤í¬ë¦½íŠ¸ ìƒì„± ë°ëª¨
    """
    generator = ReproducibleScriptGenerator(lineage)
    
    script = generator.generate_script(
        output_filepath='reproduce_cleansing.py',
        include_comments=True
    )
    
    print("\n" + "="*80)
    print("GENERATED SCRIPT PREVIEW")
    print("="*80)
    print(script[:1000] + "\n...\n")
    
    return script
```

---

## 4. ì˜ˆì‹œ (Examples)

### 4.1 ì „ì²´ ë¦¬ë‹ˆì§€ ì¶”ì  ì˜ˆì‹œ

```python
# ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ: E-commerce ì£¼ë¬¸ ë°ì´í„° í´ë Œì§•

import pandas as pd
import numpy as np
import time

def complete_lineage_example():
    """
    ì™„ì „í•œ ë¦¬ë‹ˆì§€ ì¶”ì  ì˜ˆì‹œ
    """
    print("="*80)
    print("COMPLETE DATA LINEAGE TRACKING EXAMPLE")
    print("="*80)
    
    # 1. ì›ì‹œ ë°ì´í„° ìƒì„±
    print("\n1. Loading raw data...")
    np.random.seed(42)
    n = 5000
    
    df_raw = pd.DataFrame({
        'order_id': range(1, n + 1),
        'customer_id': np.random.randint(1, 1001, n),
        'product_id': np.random.randint(1, 201, n),
        'quantity': np.random.randint(1, 10, n),
        'unit_price': np.random.uniform(10, 500, n).round(2),
        'order_date': pd.date_range('2024-01-01', periods=n, freq='H')
    })
    
    # í’ˆì§ˆ ì´ìŠˆ ì‚½ì…
    df_raw.loc[df_raw.sample(frac=0.15).index, 'quantity'] = np.nan
    df_raw.loc[df_raw.sample(frac=0.10).index, 'unit_price'] = np.nan
    df_raw = pd.concat([df_raw, df_raw.sample(frac=0.05)], ignore_index=True)
    
    print(f"   Loaded {len(df_raw)} rows with quality issues")
    
    # 2. ë¦¬ë‹ˆì§€ ì¶”ì  ì‹œì‘
    print("\n2. Initializing lineage tracking...")
    lineage = DataLineage(
        dataset_name='ecommerce_orders',
        initial_df=df_raw,
        project_name='E-commerce Data Cleansing'
    )
    
    # 3. ê²°ì¸¡ê°’ ëŒ€ì²´
    print("\n3. Imputing missing values...")
    start_time = time.time()
    df_step1 = df_raw.copy()
    df_step1['quantity'].fillna(df_step1['quantity'].median(), inplace=True)
    df_step1['unit_price'].fillna(df_step1['unit_price'].mean(), inplace=True)
    execution_time = time.time() - start_time
    
    lineage.log_operation(
        operation_type='impute_missing',
        operation_name='Median/Mean Imputation',
        df_after=df_step1,
        parameters={
            'quantity_method': 'median',
            'unit_price_method': 'mean'
        },
        description='Imputed missing quantity with median and unit_price with mean',
        user='data_engineer',
        execution_time=execution_time
    )
    
    # 4. ì´ìƒì¹˜ ì œê±°
    print("\n4. Removing outliers...")
    start_time = time.time()
    Q1 = df_step1['unit_price'].quantile(0.25)
    Q3 = df_step1['unit_price'].quantile(0.75)
    IQR = Q3 - Q1
    df_step2 = df_step1[
        (df_step1['unit_price'] >= Q1 - 1.5 * IQR) &
        (df_step1['unit_price'] <= Q3 + 1.5 * IQR)
    ]
    execution_time = time.time() - start_time
    
    lineage.log_operation(
        operation_type='remove_outliers',
        operation_name='IQR Outlier Removal',
        df_after=df_step2,
        parameters={
            'method': 'IQR',
            'column': 'unit_price',
            'multiplier': 1.5
        },
        description='Removed outliers in unit_price using IQR method',
        user='data_engineer',
        execution_time=execution_time
    )
    
    # 5. ì¤‘ë³µ ì œê±°
    print("\n5. Removing duplicates...")
    start_time = time.time()
    df_step3 = df_step2.drop_duplicates()
    execution_time = time.time() - start_time
    
    lineage.log_operation(
        operation_type='remove_duplicates',
        operation_name='Remove Duplicate Rows',
        df_after=df_step3,
        parameters={'keep': 'first'},
        description='Removed exact duplicate rows',
        user='data_engineer',
        execution_time=execution_time
    )
    
    # 6. í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§
    print("\n6. Adding new features...")
    start_time = time.time()
    df_step4 = df_step3.copy()
    df_step4['total_amount'] = (df_step4['quantity'] * df_step4['unit_price']).round(2)
    df_step4['order_month'] = df_step4['order_date'].dt.month
    execution_time = time.time() - start_time
    
    lineage.log_operation(
        operation_type='feature_engineering',
        operation_name='Add Calculated Features',
        df_after=df_step4,
        parameters={
            'new_features': ['total_amount', 'order_month']
        },
        description='Added total_amount and order_month features',
        user='data_engineer',
        execution_time=execution_time
    )
    
    # 7. ë¦¬ë‹ˆì§€ ë¦¬í¬íŠ¸
    print("\n" + "="*80)
    print("LINEAGE REPORT")
    print("="*80)
    report = lineage.get_lineage_report()
    print(report.to_string())
    
    # 8. ìš”ì•½ ì •ë³´
    print("\n" + "="*80)
    print("LINEAGE SUMMARY")
    print("="*80)
    summary = lineage.get_summary()
    for key, value in summary.items():
        if key != 'operations_list':
            print(f"{key}: {value}")
    
    # 9. ë¦¬ë‹ˆì§€ ë‚´ë³´ë‚´ê¸°
    print("\n" + "="*80)
    print("EXPORTING LINEAGE")
    print("="*80)
    lineage.export_lineage('ecommerce_lineage.json')
    
    # 10. ì‹œê°í™” ìƒì„±
    print("\n" + "="*80)
    print("CREATING VISUALIZATIONS")
    print("="*80)
    visualizer = LineageVisualizer(lineage)
    visualizer.plot_data_flow(save_path='ecommerce_flow.png')
    visualizer.plot_metrics_timeline(save_path='ecommerce_timeline.png')
    visualizer.plot_lineage_graph(save_path='ecommerce_graph.png')
    
    # 11. ì¬í˜„ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±
    print("\n" + "="*80)
    print("GENERATING REPRODUCTION SCRIPT")
    print("="*80)
    generator = ReproducibleScriptGenerator(lineage)
    script = generator.generate_script(
        output_filepath='reproduce_ecommerce_cleansing.py',
        include_comments=True
    )
    
    print("\nâœ… Complete lineage tracking example finished!")
    print(f"ğŸ“ Generated files:")
    print("   - ecommerce_lineage.json")
    print("   - ecommerce_flow.png")
    print("   - ecommerce_timeline.png")
    print("   - ecommerce_graph.png")
    print("   - reproduce_ecommerce_cleansing.py")
    
    return lineage


# ì‹¤í–‰
if __name__ == "__main__":
    lineage = complete_lineage_example()
```

---

## 5. ì—ì´ì „íŠ¸ ë§¤í•‘ (Agent Mapping)

### 5.1 Primary Agent

**`data-cleaning-specialist`**
- ì—­í• : ë°ì´í„° ë¦¬ë‹ˆì§€ ì¶”ì  ë° ê´€ë¦¬
- ì±…ì„:
  - ëª¨ë“  ë³€í™˜ ì‘ì—… ê¸°ë¡
  - ë¦¬ë‹ˆì§€ ë°ì´í„° ì €ì¥
  - ì¬í˜„ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±

### 5.2 Supporting Agents

**`technical-documentation-writer`**
- ì—­í• : ë¦¬ë‹ˆì§€ ë¬¸ì„œí™”
- ì±…ì„:
  - ë¦¬ë‹ˆì§€ ë¦¬í¬íŠ¸ ì‘ì„±
  - ë³€í™˜ ì´ë ¥ ì„¤ëª…
  - ì‚¬ìš©ì ê°€ì´ë“œ ìƒì„±

**`data-visualization-specialist`**
- ì—­í• : ë¦¬ë‹ˆì§€ ì‹œê°í™”
- ì±…ì„:
  - ë°ì´í„° íë¦„ ë‹¤ì´ì–´ê·¸ë¨
  - ì˜ì¡´ì„± ê·¸ë˜í”„
  - íƒ€ì„ë¼ì¸ ì°¨íŠ¸

---

## 6. í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ (Required Libraries)

```bash
# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬
pip install pandas>=2.0.0
pip install numpy>=1.24.0
pip install matplotlib>=3.7.0
pip install networkx>=3.0

# ì„ íƒ ë¼ì´ë¸ŒëŸ¬ë¦¬
pip install graphviz>=0.20.0
pip install pydot>=1.4.0
```

---

## 7. ì²´í¬í¬ì¸íŠ¸ (Checkpoints)

### 7.1 ë¦¬ë‹ˆì§€ ì¶”ì  ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] ëª¨ë“  ë³€í™˜ ì‘ì—… ê¸°ë¡
- [ ] íŒŒë¼ë¯¸í„° ë¬¸ì„œí™”
- [ ] ì‹¤í–‰ ì‹œê°„ ì¸¡ì •
- [ ] ë°ì´í„° í•´ì‹œ ê³„ì‚°
- [ ] ì—ëŸ¬ ì²˜ë¦¬ í¬í•¨

---

## 8. íŠ¸ëŸ¬ë¸”ìŠˆíŒ… (Troubleshooting)

**ë¬¸ì œ: ëŒ€ìš©ëŸ‰ ë°ì´í„° í•´ì‹œ ê³„ì‚° ëŠë¦¼**
```python
# í•´ê²°: ìƒ˜í”Œë§ ì‚¬ìš©
def quick_hash(df, sample_size=1000):
    if len(df) > sample_size:
        df_sample = df.sample(sample_size, random_state=42)
    else:
        df_sample = df
    return hashlib.md5(pickle.dumps(df_sample)).hexdigest()
```

---

## 9. ì°¸ê³  ìë£Œ (References)

- NetworkX: https://networkx.org/
- Data Lineage Best Practices: https://www.dataversity.net/data-lineage-best-practices/
- Related: `11-data-validation.md`, `12-quality-reporting.md`

---

**ì‘ì„±ì**: Claude Code  
**ìµœì¢… ìˆ˜ì •ì¼**: 2025-01-26  
**ë²„ì „**: 1.0
