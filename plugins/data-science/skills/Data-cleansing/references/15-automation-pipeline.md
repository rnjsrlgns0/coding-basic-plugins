# 15. Automation Pipeline (ìë™í™” íŒŒì´í”„ë¼ì¸)

**ìƒì„±ì¼**: 2025-01-26  
**ë²„ì „**: 1.0  
**ì¹´í…Œê³ ë¦¬**: Data Pipeline & Automation

---

## 1. ê°œìš” (Overview)

### 1.1 ëª©ì  (Purpose)

ë°ì´í„° í´ë Œì§• ìë™í™” íŒŒì´í”„ë¼ì¸ì€ ë°˜ë³µì ì¸ ë°ì´í„° í’ˆì§ˆ ì‘ì—…ì„ ìë™í™”í•˜ê³ , ì—ì´ì „íŠ¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ì„ í†µí•´ íš¨ìœ¨ì ì¸ ì›Œí¬í”Œë¡œìš°ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤. ì´ ë ˆí¼ëŸ°ìŠ¤ëŠ” ì™„ì „ ìë™í™”ëœ ë°ì´í„° í´ë Œì§• íŒŒì´í”„ë¼ì¸ ì„¤ê³„ ë° êµ¬í˜„ ë°©ë²•ì„ ì œê³µí•©ë‹ˆë‹¤.

### 1.2 ì ìš© ì‹œê¸° (When to Apply)

**í•„ìˆ˜ ì ìš©**:
- âœ… ë°˜ë³µì ì¸ ë°ì´í„° í´ë Œì§• ì‘ì—…
- âœ… í”„ë¡œë•ì…˜ ë°ì´í„° íŒŒì´í”„ë¼ì¸
- âœ… ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬
- âœ… ì‹¤ì‹œê°„ ë°ì´í„° í’ˆì§ˆ ëª¨ë‹ˆí„°ë§

**ê¶Œì¥ ì ìš©**:
- ğŸ”¹ ì •ê¸°ì ì¸ ë°ì´í„° ì²˜ë¦¬ (ì¼ì¼, ì£¼ê°„, ì›”ê°„)
- ğŸ”¹ ì—¬ëŸ¬ ë°ì´í„° ì†ŒìŠ¤ í†µí•©
- ğŸ”¹ íŒ€ í˜‘ì—… í”„ë¡œì íŠ¸
- ğŸ”¹ CI/CD í†µí•©

### 1.3 íŒŒì´í”„ë¼ì¸ ì•„í‚¤í…ì²˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Data Cleansing Pipeline                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Data Sources â”‚ â”€â”€â”€> â”‚   Ingestion  â”‚ â”€â”€â”€> â”‚  Validation  â”‚
â”‚              â”‚      â”‚   (Extract)  â”‚      â”‚   (Quality)  â”‚
â”‚ - Files      â”‚      â”‚              â”‚      â”‚              â”‚
â”‚ - Databases  â”‚      â”‚ â€¢ Read data  â”‚      â”‚ â€¢ Profile    â”‚
â”‚ - APIs       â”‚      â”‚ â€¢ Parse      â”‚      â”‚ â€¢ Validate   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                     â”‚
                                                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Storage    â”‚ <â”€â”€â”€ â”‚  Transform   â”‚ <â”€â”€â”€ â”‚   Cleanse    â”‚
â”‚   (Load)     â”‚      â”‚  (Features)  â”‚      â”‚   (Fix)      â”‚
â”‚              â”‚      â”‚              â”‚      â”‚              â”‚
â”‚ â€¢ Save       â”‚      â”‚ â€¢ Normalize  â”‚      â”‚ â€¢ Missing    â”‚
â”‚ â€¢ Index      â”‚      â”‚ â€¢ Encode     â”‚      â”‚ â€¢ Outliers   â”‚
â”‚ â€¢ Version    â”‚      â”‚ â€¢ Aggregate  â”‚      â”‚ â€¢ Duplicates â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Monitoring  â”‚      â”‚   Alerting   â”‚      â”‚   Reporting  â”‚
â”‚              â”‚      â”‚              â”‚      â”‚              â”‚
â”‚ â€¢ Metrics    â”‚      â”‚ â€¢ Email      â”‚      â”‚ â€¢ Dashboard  â”‚
â”‚ â€¢ Logs       â”‚      â”‚ â€¢ Slack      â”‚      â”‚ â€¢ Reports    â”‚
â”‚ â€¢ Health     â”‚      â”‚ â€¢ PagerDuty  â”‚      â”‚ â€¢ Lineage    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. ì´ë¡ ì  ë°°ê²½ (Theoretical Background)

### 2.1 íŒŒì´í”„ë¼ì¸ ì„¤ê³„ ì›ì¹™

**1. Modularity (ëª¨ë“ˆì„±)**
- ê° ë‹¨ê³„ë¥¼ ë…ë¦½ì ì¸ ëª¨ë“ˆë¡œ ì„¤ê³„
- ì¬ì‚¬ìš© ê°€ëŠ¥í•œ ì»´í¬ë„ŒíŠ¸
- ì‰¬ìš´ ìœ ì§€ë³´ìˆ˜

**2. Scalability (í™•ì¥ì„±)**
- ë³‘ë ¬ ì²˜ë¦¬ ì§€ì›
- ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬
- ìˆ˜í‰ì  í™•ì¥ ê°€ëŠ¥

**3. Reliability (ì•ˆì •ì„±)**
- ì—ëŸ¬ í•¸ë“¤ë§
- ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜
- ë¡¤ë°± ê¸°ëŠ¥

**4. Observability (ê´€ì°° ê°€ëŠ¥ì„±)**
- ë¡œê¹… ë° ëª¨ë‹ˆí„°ë§
- ë©”íŠ¸ë¦­ ìˆ˜ì§‘
- ì•ŒëŒ ì‹œìŠ¤í…œ

**5. Reproducibility (ì¬í˜„ì„±)**
- ë²„ì „ ê´€ë¦¬
- ë¦¬ë‹ˆì§€ ì¶”ì 
- ê°ì‚¬ ë¡œê·¸

### 2.2 ì›Œí¬í”Œë¡œìš° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜

**ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ë„êµ¬**:
- **Airflow**: Apacheì˜ ì›Œí¬í”Œë¡œìš° ê´€ë¦¬ í”Œë«í¼
- **Prefect**: í˜„ëŒ€ì ì¸ ì›Œí¬í”Œë¡œìš° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜
- **Luigi**: Spotifyì˜ íŒŒì´í”„ë¼ì¸ ë¹Œë”
- **Dagster**: ë°ì´í„° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°

---

## 3. êµ¬í˜„ (Implementation)

### 3.1 CleansingPipeline í´ë˜ìŠ¤

```python
import pandas as pd
import numpy as np
from typing import Dict, List, Any, Callable, Optional
from datetime import datetime
import logging
import time
import traceback
from pathlib import Path

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

class CleansingPipeline:
    """
    ë°ì´í„° í´ë Œì§• ìë™í™” íŒŒì´í”„ë¼ì¸
    Automated data cleansing pipeline
    """
    
    def __init__(
        self,
        pipeline_name: str,
        config: Dict[str, Any] = None
    ):
        """
        Parameters:
        -----------
        pipeline_name : str
            íŒŒì´í”„ë¼ì¸ ì´ë¦„
        config : dict, optional
            íŒŒì´í”„ë¼ì¸ ì„¤ì •
        """
        self.pipeline_name = pipeline_name
        self.config = config or {}
        self.logger = logging.getLogger(pipeline_name)
        
        # ì‹¤í–‰ ìƒíƒœ
        self.execution_id = None
        self.start_time = None
        self.end_time = None
        self.status = 'initialized'
        
        # íŒŒì´í”„ë¼ì¸ ë‹¨ê³„
        self.stages = []
        
        # ë©”íŠ¸ë¦­
        self.metrics = {}
        
        # ì—ëŸ¬
        self.errors = []
    
    def add_stage(
        self,
        stage_name: str,
        stage_func: Callable,
        enabled: bool = True,
        retry_count: int = 0,
        retry_delay: int = 5
    ):
        """
        íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ ì¶”ê°€
        Add a pipeline stage
        
        Parameters:
        -----------
        stage_name : str
            ë‹¨ê³„ ì´ë¦„
        stage_func : Callable
            ì‹¤í–‰í•  í•¨ìˆ˜ (dfë¥¼ ë°›ì•„ì„œ df ë°˜í™˜)
        enabled : bool
            ë‹¨ê³„ í™œì„±í™” ì—¬ë¶€
        retry_count : int
            ì¬ì‹œë„ íšŸìˆ˜
        retry_delay : int
            ì¬ì‹œë„ ê°„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
        """
        stage = {
            'name': stage_name,
            'func': stage_func,
            'enabled': enabled,
            'retry_count': retry_count,
            'retry_delay': retry_delay,
            'status': 'pending',
            'execution_time': 0.0,
            'error': None
        }
        
        self.stages.append(stage)
        self.logger.info(f"Added stage: {stage_name}")
    
    def run(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        Run the pipeline
        
        Parameters:
        -----------
        df : pd.DataFrame
            ì…ë ¥ ë°ì´í„°
            
        Returns:
        --------
        df_result : pd.DataFrame
            í´ë Œì§•ëœ ë°ì´í„°
        """
        self.execution_id = datetime.now().strftime('%Y%m%d_%H%M%S')
        self.start_time = time.time()
        self.status = 'running'
        
        self.logger.info("="*80)
        self.logger.info(f"Pipeline: {self.pipeline_name}")
        self.logger.info(f"Execution ID: {self.execution_id}")
        self.logger.info(f"Input shape: {df.shape}")
        self.logger.info("="*80)
        
        df_current = df.copy()
        
        # ê° ë‹¨ê³„ ì‹¤í–‰
        for i, stage in enumerate(self.stages, 1):
            if not stage['enabled']:
                self.logger.info(f"\n[{i}/{len(self.stages)}] {stage['name']} - SKIPPED")
                stage['status'] = 'skipped'
                continue
            
            self.logger.info(f"\n[{i}/{len(self.stages)}] {stage['name']} - STARTING")
            
            # ë‹¨ê³„ ì‹¤í–‰ (ì¬ì‹œë„ í¬í•¨)
            success = False
            attempt = 0
            max_attempts = stage['retry_count'] + 1
            
            while attempt < max_attempts and not success:
                try:
                    attempt += 1
                    if attempt > 1:
                        self.logger.warning(f"Retry attempt {attempt-1}/{stage['retry_count']}")
                        time.sleep(stage['retry_delay'])
                    
                    stage_start = time.time()
                    df_current = stage['func'](df_current)
                    stage_time = time.time() - stage_start
                    
                    stage['execution_time'] = stage_time
                    stage['status'] = 'success'
                    success = True
                    
                    self.logger.info(f"âœ… {stage['name']} - COMPLETED ({stage_time:.2f}s)")
                    self.logger.info(f"   Output shape: {df_current.shape}")
                    
                except Exception as e:
                    error_msg = f"Error in {stage['name']}: {str(e)}"
                    self.logger.error(error_msg)
                    self.logger.error(traceback.format_exc())
                    
                    stage['error'] = error_msg
                    self.errors.append({
                        'stage': stage['name'],
                        'attempt': attempt,
                        'error': str(e),
                        'traceback': traceback.format_exc()
                    })
                    
                    if attempt >= max_attempts:
                        stage['status'] = 'failed'
                        
                        # ì‹¤íŒ¨ ì‹œ ì²˜ë¦¬ ë°©ë²•
                        if self.config.get('fail_fast', True):
                            self.status = 'failed'
                            raise RuntimeError(f"Pipeline failed at stage: {stage['name']}")
                        else:
                            self.logger.warning(f"Continuing despite failure in {stage['name']}")
        
        # ì™„ë£Œ
        self.end_time = time.time()
        self.status = 'completed'
        
        total_time = self.end_time - self.start_time
        
        self.logger.info("\n" + "="*80)
        self.logger.info("PIPELINE COMPLETED")
        self.logger.info("="*80)
        self.logger.info(f"Total time: {total_time:.2f}s")
        self.logger.info(f"Final shape: {df_current.shape}")
        
        # ë©”íŠ¸ë¦­ ì €ì¥
        self.metrics = {
            'execution_id': self.execution_id,
            'total_time': total_time,
            'input_rows': len(df),
            'output_rows': len(df_current),
            'input_columns': len(df.columns),
            'output_columns': len(df_current.columns),
            'rows_removed': len(df) - len(df_current),
            'stages_total': len(self.stages),
            'stages_success': sum(1 for s in self.stages if s['status'] == 'success'),
            'stages_failed': sum(1 for s in self.stages if s['status'] == 'failed'),
            'stages_skipped': sum(1 for s in self.stages if s['status'] == 'skipped')
        }
        
        return df_current
    
    def get_execution_report(self) -> Dict[str, Any]:
        """
        ì‹¤í–‰ ë¦¬í¬íŠ¸ ìƒì„±
        Generate execution report
        """
        report = {
            'pipeline_name': self.pipeline_name,
            'execution_id': self.execution_id,
            'status': self.status,
            'start_time': datetime.fromtimestamp(self.start_time).isoformat() if self.start_time else None,
            'end_time': datetime.fromtimestamp(self.end_time).isoformat() if self.end_time else None,
            'metrics': self.metrics,
            'stages': []
        }
        
        for stage in self.stages:
            report['stages'].append({
                'name': stage['name'],
                'status': stage['status'],
                'execution_time': stage['execution_time'],
                'error': stage['error']
            })
        
        return report
    
    def save_execution_report(self, filepath: str):
        """
        ì‹¤í–‰ ë¦¬í¬íŠ¸ ì €ì¥
        Save execution report
        """
        import json
        
        report = self.get_execution_report()
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"Execution report saved to {filepath}")


# ì‚¬ìš© ì˜ˆì‹œ: ê¸°ë³¸ íŒŒì´í”„ë¼ì¸ ìƒì„±
def create_basic_cleansing_pipeline():
    """
    ê¸°ë³¸ ë°ì´í„° í´ë Œì§• íŒŒì´í”„ë¼ì¸ ìƒì„±
    Create basic data cleansing pipeline
    """
    pipeline = CleansingPipeline(
        pipeline_name='basic_cleansing',
        config={'fail_fast': False}
    )
    
    # Stage 1: ë°ì´í„° í”„ë¡œíŒŒì¼ë§
    def stage_profiling(df):
        print(f"\nğŸ“Š Data Profiling:")
        print(f"   Shape: {df.shape}")
        print(f"   Missing: {df.isnull().sum().sum()}")
        print(f"   Duplicates: {df.duplicated().sum()}")
        return df
    
    pipeline.add_stage('data_profiling', stage_profiling, enabled=True)
    
    # Stage 2: ê²°ì¸¡ê°’ ëŒ€ì²´
    def stage_imputation(df):
        df_clean = df.copy()
        for col in df_clean.select_dtypes(include=[np.number]).columns:
            if df_clean[col].isnull().any():
                df_clean[col].fillna(df_clean[col].median(), inplace=True)
        return df_clean
    
    pipeline.add_stage('missing_imputation', stage_imputation, enabled=True)
    
    # Stage 3: ì´ìƒì¹˜ ì œê±°
    def stage_outliers(df):
        df_clean = df.copy()
        for col in df_clean.select_dtypes(include=[np.number]).columns:
            Q1 = df_clean[col].quantile(0.25)
            Q3 = df_clean[col].quantile(0.75)
            IQR = Q3 - Q1
            lower = Q1 - 1.5 * IQR
            upper = Q3 + 1.5 * IQR
            df_clean = df_clean[(df_clean[col] >= lower) & (df_clean[col] <= upper)]
        return df_clean
    
    pipeline.add_stage('outlier_removal', stage_outliers, enabled=True, retry_count=2)
    
    # Stage 4: ì¤‘ë³µ ì œê±°
    def stage_deduplication(df):
        return df.drop_duplicates()
    
    pipeline.add_stage('deduplication', stage_deduplication, enabled=True)
    
    return pipeline
```

### 3.2 ì—ì´ì „íŠ¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°

```python
from enum import Enum
from dataclasses import dataclass
from typing import List

class AgentType(Enum):
    """ì—ì´ì „íŠ¸ ìœ í˜•"""
    DATA_CLEANING_SPECIALIST = "data-cleaning-specialist"
    DATA_SCIENTIST = "data-scientist"
    DATA_VISUALIZATION_SPECIALIST = "data-visualization-specialist"
    FEATURE_ENGINEERING_SPECIALIST = "feature-engineering-specialist"
    TECHNICAL_DOCUMENTATION_WRITER = "technical-documentation-writer"


@dataclass
class Task:
    """ì‘ì—… ì •ì˜"""
    task_id: str
    task_type: str
    agent: AgentType
    parameters: Dict[str, Any]
    dependencies: List[str] = None
    status: str = 'pending'
    result: Any = None


class AgentOrchestrator:
    """
    ì—ì´ì „íŠ¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°
    Orchestrate multiple agents for data cleansing
    """
    
    def __init__(self):
        self.tasks = []
        self.completed_tasks = set()
        self.logger = logging.getLogger('AgentOrchestrator')
    
    def add_task(
        self,
        task_id: str,
        task_type: str,
        agent: AgentType,
        parameters: Dict[str, Any],
        dependencies: List[str] = None
    ):
        """
        ì‘ì—… ì¶”ê°€
        Add a task
        """
        task = Task(
            task_id=task_id,
            task_type=task_type,
            agent=agent,
            parameters=parameters,
            dependencies=dependencies or []
        )
        
        self.tasks.append(task)
        self.logger.info(f"Added task: {task_id} (Agent: {agent.value})")
    
    def can_execute(self, task: Task) -> bool:
        """
        ì‘ì—… ì‹¤í–‰ ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        Check if task can be executed
        """
        if not task.dependencies:
            return True
        
        return all(dep in self.completed_tasks for dep in task.dependencies)
    
    def execute_task(self, task: Task, df: pd.DataFrame) -> pd.DataFrame:
        """
        ì‘ì—… ì‹¤í–‰
        Execute a task
        """
        self.logger.info(f"\nğŸ¤– Executing task: {task.task_id}")
        self.logger.info(f"   Agent: {task.agent.value}")
        self.logger.info(f"   Type: {task.task_type}")
        
        # ì—ì´ì „íŠ¸ë³„ ì‘ì—… ì‹¤í–‰ (ì‹œë®¬ë ˆì´ì…˜)
        if task.agent == AgentType.DATA_CLEANING_SPECIALIST:
            df_result = self._execute_cleaning_task(df, task)
        elif task.agent == AgentType.DATA_SCIENTIST:
            df_result = self._execute_analysis_task(df, task)
        elif task.agent == AgentType.FEATURE_ENGINEERING_SPECIALIST:
            df_result = self._execute_feature_task(df, task)
        else:
            df_result = df  # ê¸°ë³¸: ë°ì´í„° ê·¸ëŒ€ë¡œ ë°˜í™˜
        
        task.status = 'completed'
        task.result = {'success': True}
        self.completed_tasks.add(task.task_id)
        
        self.logger.info(f"âœ… Task completed: {task.task_id}")
        
        return df_result
    
    def _execute_cleaning_task(self, df: pd.DataFrame, task: Task) -> pd.DataFrame:
        """
        ë°ì´í„° í´ë Œì§• ì‘ì—… ì‹¤í–‰
        Execute data cleaning task
        """
        task_type = task.task_type
        params = task.parameters
        
        if task_type == 'impute_missing':
            method = params.get('method', 'mean')
            for col in df.select_dtypes(include=[np.number]).columns:
                if df[col].isnull().any():
                    if method == 'mean':
                        df[col].fillna(df[col].mean(), inplace=True)
                    elif method == 'median':
                        df[col].fillna(df[col].median(), inplace=True)
        
        elif task_type == 'remove_outliers':
            method = params.get('method', 'IQR')
            if method == 'IQR':
                for col in df.select_dtypes(include=[np.number]).columns:
                    Q1 = df[col].quantile(0.25)
                    Q3 = df[col].quantile(0.75)
                    IQR = Q3 - Q1
                    df = df[(df[col] >= Q1 - 1.5*IQR) & (df[col] <= Q3 + 1.5*IQR)]
        
        elif task_type == 'remove_duplicates':
            df = df.drop_duplicates()
        
        return df
    
    def _execute_analysis_task(self, df: pd.DataFrame, task: Task) -> pd.DataFrame:
        """
        ë°ì´í„° ë¶„ì„ ì‘ì—… ì‹¤í–‰
        Execute data analysis task
        """
        # ë¶„ì„ë§Œ ìˆ˜í–‰, ë°ì´í„°ëŠ” ë³€ê²½ ì—†ìŒ
        self.logger.info(f"   Analysis: {task.task_type}")
        return df
    
    def _execute_feature_task(self, df: pd.DataFrame, task: Task) -> pd.DataFrame:
        """
        í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì‘ì—… ì‹¤í–‰
        Execute feature engineering task
        """
        params = task.parameters
        
        if task.task_type == 'create_feature':
            new_col = params.get('column_name')
            formula = params.get('formula')
            if new_col and formula:
                df[new_col] = eval(formula, {'df': df})
        
        return df
    
    def run(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        ëª¨ë“  ì‘ì—… ì‹¤í–‰ (ì˜ì¡´ì„± ìˆœì„œ ê³ ë ¤)
        Run all tasks respecting dependencies
        """
        self.logger.info("\n" + "="*80)
        self.logger.info("AGENT ORCHESTRATION STARTING")
        self.logger.info("="*80)
        
        df_current = df.copy()
        pending_tasks = [t for t in self.tasks if t.status == 'pending']
        
        while pending_tasks:
            executed_any = False
            
            for task in pending_tasks:
                if self.can_execute(task):
                    df_current = self.execute_task(task, df_current)
                    executed_any = True
            
            if not executed_any:
                # ìˆœí™˜ ì˜ì¡´ì„± ë˜ëŠ” ë¯¸í•´ê²° ì˜ì¡´ì„±
                remaining = [t.task_id for t in pending_tasks]
                self.logger.error(f"Cannot execute remaining tasks: {remaining}")
                break
            
            pending_tasks = [t for t in self.tasks if t.status == 'pending']
        
        self.logger.info("\n" + "="*80)
        self.logger.info("AGENT ORCHESTRATION COMPLETED")
        self.logger.info("="*80)
        self.logger.info(f"Total tasks: {len(self.tasks)}")
        self.logger.info(f"Completed: {len(self.completed_tasks)}")
        
        return df_current


# ì‚¬ìš© ì˜ˆì‹œ: ì—ì´ì „íŠ¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜
def demo_agent_orchestration():
    """
    ì—ì´ì „íŠ¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ ë°ëª¨
    """
    orchestrator = AgentOrchestrator()
    
    # Task 1: ë°ì´í„° í”„ë¡œíŒŒì¼ë§ (ì˜ì¡´ì„± ì—†ìŒ)
    orchestrator.add_task(
        task_id='task_001',
        task_type='profile_data',
        agent=AgentType.DATA_SCIENTIST,
        parameters={}
    )
    
    # Task 2: ê²°ì¸¡ê°’ ëŒ€ì²´ (Task 1 ì™„ë£Œ í›„)
    orchestrator.add_task(
        task_id='task_002',
        task_type='impute_missing',
        agent=AgentType.DATA_CLEANING_SPECIALIST,
        parameters={'method': 'median'},
        dependencies=['task_001']
    )
    
    # Task 3: ì´ìƒì¹˜ ì œê±° (Task 2 ì™„ë£Œ í›„)
    orchestrator.add_task(
        task_id='task_003',
        task_type='remove_outliers',
        agent=AgentType.DATA_CLEANING_SPECIALIST,
        parameters={'method': 'IQR'},
        dependencies=['task_002']
    )
    
    # Task 4: ì¤‘ë³µ ì œê±° (Task 3 ì™„ë£Œ í›„)
    orchestrator.add_task(
        task_id='task_004',
        task_type='remove_duplicates',
        agent=AgentType.DATA_CLEANING_SPECIALIST,
        parameters={},
        dependencies=['task_003']
    )
    
    # Task 5: í”¼ì²˜ ìƒì„± (Task 4 ì™„ë£Œ í›„)
    orchestrator.add_task(
        task_id='task_005',
        task_type='create_feature',
        agent=AgentType.FEATURE_ENGINEERING_SPECIALIST,
        parameters={
            'column_name': 'total_amount',
            'formula': "df['quantity'] * df['unit_price']"
        },
        dependencies=['task_004']
    )
    
    return orchestrator
```

### 3.3 CLI ì»¤ë§¨ë“œ ì¸í„°í˜ì´ìŠ¤

```python
import click
import json

@click.group()
def cli():
    """Data Cleansing Pipeline CLI"""
    pass


@cli.command()
@click.argument('input_file', type=click.Path(exists=True))
@click.option('--output', '-o', default='cleaned_output.csv', help='Output file path')
@click.option('--report', '-r', is_flag=True, help='Generate execution report')
def clean_full(input_file, output, report):
    """
    ì™„ì „ ìë™ í´ë Œì§• íŒŒì´í”„ë¼ì¸
    Full automatic cleansing pipeline
    
    Usage: python cleansing_cli.py clean-full data.csv -o cleaned.csv -r
    """
    click.echo(f"ğŸš€ Starting full cleansing pipeline...")
    click.echo(f"   Input: {input_file}")
    click.echo(f"   Output: {output}")
    
    # ë°ì´í„° ë¡œë“œ
    df = pd.read_csv(input_file)
    click.echo(f"   Loaded: {len(df)} rows")
    
    # íŒŒì´í”„ë¼ì¸ ìƒì„± ë° ì‹¤í–‰
    pipeline = create_basic_cleansing_pipeline()
    df_clean = pipeline.run(df)
    
    # ì €ì¥
    df_clean.to_csv(output, index=False)
    click.echo(f"âœ… Cleaned data saved to {output}")
    
    # ë¦¬í¬íŠ¸ ìƒì„±
    if report:
        report_file = output.replace('.csv', '_report.json')
        pipeline.save_execution_report(report_file)
        click.echo(f"ğŸ“„ Report saved to {report_file}")


@cli.command()
@click.argument('input_file', type=click.Path(exists=True))
@click.option('--method', '-m', default='median', help='Imputation method (mean/median/mode)')
@click.option('--output', '-o', default='imputed_output.csv', help='Output file path')
def clean_missing(input_file, method, output):
    """
    ê²°ì¸¡ê°’ ì§‘ì¤‘ ì²˜ë¦¬
    Focus on missing value imputation
    
    Usage: python cleansing_cli.py clean-missing data.csv -m median -o imputed.csv
    """
    click.echo(f"ğŸ”§ Imputing missing values using {method} method...")
    
    df = pd.read_csv(input_file)
    
    # ê²°ì¸¡ê°’ ëŒ€ì²´
    for col in df.select_dtypes(include=[np.number]).columns:
        if df[col].isnull().any():
            if method == 'mean':
                df[col].fillna(df[col].mean(), inplace=True)
            elif method == 'median':
                df[col].fillna(df[col].median(), inplace=True)
            elif method == 'mode':
                df[col].fillna(df[col].mode()[0], inplace=True)
    
    df.to_csv(output, index=False)
    click.echo(f"âœ… Imputed data saved to {output}")


@cli.command()
@click.argument('input_file', type=click.Path(exists=True))
@click.option('--method', '-m', default='IQR', help='Outlier detection method (IQR/zscore)')
@click.option('--output', '-o', default='no_outliers_output.csv', help='Output file path')
def clean_outliers(input_file, method, output):
    """
    ì´ìƒì¹˜ íƒì§€ ë° ì²˜ë¦¬
    Detect and handle outliers
    
    Usage: python cleansing_cli.py clean-outliers data.csv -m IQR -o cleaned.csv
    """
    click.echo(f"ğŸ¯ Removing outliers using {method} method...")
    
    df = pd.read_csv(input_file)
    
    if method == 'IQR':
        for col in df.select_dtypes(include=[np.number]).columns:
            Q1 = df[col].quantile(0.25)
            Q3 = df[col].quantile(0.75)
            IQR = Q3 - Q1
            df = df[(df[col] >= Q1 - 1.5*IQR) & (df[col] <= Q3 + 1.5*IQR)]
    
    df.to_csv(output, index=False)
    click.echo(f"âœ… Data without outliers saved to {output}")


@cli.command()
@click.argument('config_file', type=click.Path(exists=True))
@click.argument('input_file', type=click.Path(exists=True))
@click.option('--output', '-o', default='custom_output.csv', help='Output file path')
def clean_custom(config_file, input_file, output):
    """
    ì»¤ìŠ¤í…€ ì„¤ì • ê¸°ë°˜ í´ë Œì§•
    Custom configuration-based cleansing
    
    Usage: python cleansing_cli.py clean-custom config.json data.csv -o cleaned.csv
    """
    click.echo(f"âš™ï¸ Running custom cleansing with config: {config_file}")
    
    # ì„¤ì • ë¡œë“œ
    with open(config_file, 'r') as f:
        config = json.load(f)
    
    df = pd.read_csv(input_file)
    
    # ì„¤ì •ì— ë”°ë¼ íŒŒì´í”„ë¼ì¸ êµ¬ì„±
    pipeline = CleansingPipeline('custom_pipeline', config=config)
    
    # ë™ì ìœ¼ë¡œ ìŠ¤í…Œì´ì§€ ì¶”ê°€
    # (ì‹¤ì œë¡œëŠ” configì—ì„œ ì½ì–´ì„œ ì¶”ê°€)
    
    df_clean = pipeline.run(df)
    df_clean.to_csv(output, index=False)
    
    click.echo(f"âœ… Custom cleaned data saved to {output}")


if __name__ == '__main__':
    cli()
```

---

## 4. ì˜ˆì‹œ (Examples)

### 4.1 ì™„ì „í•œ ìë™í™” íŒŒì´í”„ë¼ì¸ ì˜ˆì‹œ

```python
def complete_automation_example():
    """
    ì™„ì „í•œ ìë™í™” íŒŒì´í”„ë¼ì¸ ì˜ˆì‹œ
    Complete automation pipeline example
    """
    print("="*80)
    print("COMPLETE AUTOMATION PIPELINE EXAMPLE")
    print("="*80)
    
    # 1. ìƒ˜í”Œ ë°ì´í„° ìƒì„±
    print("\n1. Creating sample data...")
    np.random.seed(42)
    n = 5000
    
    df = pd.DataFrame({
        'order_id': range(1, n + 1),
        'customer_id': np.random.randint(1, 1001, n),
        'quantity': np.random.randint(1, 10, n),
        'unit_price': np.random.uniform(10, 500, n).round(2),
        'order_date': pd.date_range('2024-01-01', periods=n, freq='H')
    })
    
    # í’ˆì§ˆ ì´ìŠˆ ì‚½ì…
    df.loc[df.sample(frac=0.15).index, 'quantity'] = np.nan
    df.loc[df.sample(frac=0.10).index, 'unit_price'] = np.nan
    df = pd.concat([df, df.sample(frac=0.05)], ignore_index=True)
    
    print(f"   Created {len(df)} rows with quality issues")
    
    # 2. íŒŒì´í”„ë¼ì¸ ìƒì„±
    print("\n2. Creating cleansing pipeline...")
    pipeline = create_basic_cleansing_pipeline()
    
    # 3. íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    print("\n3. Running pipeline...")
    df_clean = pipeline.run(df)
    
    # 4. ê²°ê³¼ ì €ì¥
    print("\n4. Saving results...")
    output_dir = Path('pipeline_output')
    output_dir.mkdir(exist_ok=True)
    
    df_clean.to_csv(output_dir / 'cleaned_data.csv', index=False)
    pipeline.save_execution_report(str(output_dir / 'execution_report.json'))
    
    # 5. ë©”íŠ¸ë¦­ ì¶œë ¥
    print("\n5. Pipeline metrics:")
    for key, value in pipeline.metrics.items():
        print(f"   {key}: {value}")
    
    print(f"\nâœ… Pipeline completed successfully!")
    print(f"ğŸ“ Output directory: {output_dir}")
    
    return pipeline


# ì‹¤í–‰
if __name__ == "__main__":
    pipeline = complete_automation_example()
```

---

## 5. ì—ì´ì „íŠ¸ ë§¤í•‘ (Agent Mapping)

### 5.1 Primary Agent

**`data-cleaning-specialist`**
- íŒŒì´í”„ë¼ì¸ ì„¤ê³„ ë° êµ¬í˜„
- ì—ì´ì „íŠ¸ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜
- ìë™í™” ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±

### 5.2 Supporting Agents

**All agents** (ì‘ì—… ìˆ˜í–‰)
- ê° ì—ì´ì „íŠ¸ëŠ” í• ë‹¹ëœ ì‘ì—… ì‹¤í–‰
- ê²°ê³¼ë¥¼ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì „ë‹¬

---

## 6. í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬ (Required Libraries)

```bash
# í•„ìˆ˜
pip install pandas>=2.0.0
pip install numpy>=1.24.0
pip install click>=8.1.0

# ì›Œí¬í”Œë¡œìš° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ (ì„ íƒ 1ê°œ)
pip install apache-airflow>=2.7.0
pip install prefect>=2.14.0
pip install luigi>=3.4.0

# ë³‘ë ¬ ì²˜ë¦¬
pip install dask>=2023.10.0
pip install joblib>=1.3.0
```

---

## 7. ì²´í¬í¬ì¸íŠ¸ (Checkpoints)

### 7.1 íŒŒì´í”„ë¼ì¸ ì„¤ê³„

- [ ] ë‹¨ê³„ë³„ ì‘ì—… ì •ì˜
- [ ] ì˜ì¡´ì„± ê´€ê³„ ëª…í™•í™”
- [ ] ì—ëŸ¬ í•¸ë“¤ë§ ì „ëµ
- [ ] ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜

### 7.2 ìš´ì˜

- [ ] ëª¨ë‹ˆí„°ë§ ì„¤ì •
- [ ] ì•ŒëŒ ì„¤ì •
- [ ] ë¡œê¹… êµ¬ì„±
- [ ] ì„±ëŠ¥ ìµœì í™”

---

## 8. íŠ¸ëŸ¬ë¸”ìŠˆíŒ… (Troubleshooting)

**ë¬¸ì œ: íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì¤‘ë‹¨**
```python
# í•´ê²°: ì²´í¬í¬ì¸íŠ¸ ì¶”ê°€
def save_checkpoint(df, stage_name):
    df.to_parquet(f'checkpoint_{stage_name}.parquet')

# ì¬ì‹œì‘ ì‹œ ì²´í¬í¬ì¸íŠ¸ì—ì„œ ë¡œë“œ
if os.path.exists(checkpoint_file):
    df = pd.read_parquet(checkpoint_file)
```

---

## 9. ì°¸ê³  ìë£Œ (References)

- Apache Airflow: https://airflow.apache.org/
- Prefect: https://www.prefect.io/
- Related: All previous Data-cleansing references

---

**ì‘ì„±ì**: Claude Code  
**ìµœì¢… ìˆ˜ì •ì¼**: 2025-01-26  
**ë²„ì „**: 1.0
