# Missing Data Patterns (ê²°ì¸¡ê°’ íŒ¨í„´ ë¶„ì„)

**ìƒì„±ì¼**: 2025-01-25  
**ë²„ì „**: 1.0  
**ë‹´ë‹¹ ì—ì´ì „íŠ¸**: `data-cleaning-specialist`, `data-visualization-specialist`

---

## 1. ê°œìš”

### 1.1 ëª©ì 

ê²°ì¸¡ê°’ íŒ¨í„´ ë¶„ì„ì€ ë°ì´í„° í´ë Œì§•ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ë‹¨ê³„ ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. ë‹¨ìˆœíˆ ê²°ì¸¡ê°’ì´ "ì–¼ë§ˆë‚˜ ìˆëŠ”ê°€"ë¥¼ ë„˜ì–´ì„œ "ì™œ ê²°ì¸¡ë˜ì—ˆëŠ”ê°€"ì™€ "ì–´ë–¤ íŒ¨í„´ìœ¼ë¡œ ê²°ì¸¡ë˜ì—ˆëŠ”ê°€"ë¥¼ ì´í•´í•˜ëŠ” ê²ƒì´ í•µì‹¬ì…ë‹ˆë‹¤. ì´ ë ˆí¼ëŸ°ìŠ¤ëŠ” ë‹¤ìŒ ëª©í‘œë¥¼ ë‹¬ì„±í•©ë‹ˆë‹¤:

- **ê²°ì¸¡ ë©”ì»¤ë‹ˆì¦˜ ì‹ë³„**: MCAR, MAR, MNAR êµ¬ë¶„ì„ í†µí•œ ì ì ˆí•œ ì²˜ë¦¬ ë°©ë²• ì„ íƒ
- **íŒ¨í„´ ì‹œê°í™”**: ê²°ì¸¡ê°’ì˜ ë¶„í¬ì™€ ê´€ê³„ë¥¼ ì§ê´€ì ìœ¼ë¡œ íŒŒì•…
- **ê³µë™ ê²°ì¸¡ ë¶„ì„**: ì—¬ëŸ¬ ë³€ìˆ˜ê°€ í•¨ê»˜ ê²°ì¸¡ë˜ëŠ” íŒ¨í„´ íƒì§€
- **ëŒ€ì²´ ì „ëµ ìˆ˜ë¦½**: ê²°ì¸¡ ë©”ì»¤ë‹ˆì¦˜ì— ë”°ë¥¸ ìµœì  imputation ë°©ë²• ê²°ì •

### 1.2 ì ìš© ì‹œê¸°

ê²°ì¸¡ê°’ íŒ¨í„´ ë¶„ì„ì€ ë‹¤ìŒ ìƒí™©ì—ì„œ í•„ìˆ˜ì ìœ¼ë¡œ ìˆ˜í–‰í•´ì•¼ í•©ë‹ˆë‹¤:

1. **ë°ì´í„° í’ˆì§ˆ í‰ê°€ ì§í›„**: ê²°ì¸¡ê°’ì´ 5% ì´ìƒ ë°œê²¬ëœ ê²½ìš°
2. **imputation ì „**: ì–´ë–¤ ëŒ€ì²´ ë°©ë²•ì„ ì‚¬ìš©í• ì§€ ê²°ì •í•˜ê¸° ì „
3. **ëª¨ë¸ ì„±ëŠ¥ ì €í•˜ ì‹œ**: ê²°ì¸¡ê°’ ì²˜ë¦¬ê°€ ë¶€ì ì ˆí–ˆì„ ê°€ëŠ¥ì„± ì¡°ì‚¬
4. **ìƒˆë¡œìš´ ë°ì´í„° ì†ŒìŠ¤**: ê²°ì¸¡ê°’ ìƒì„± ë©”ì»¤ë‹ˆì¦˜ì´ ë¶ˆëª…í™•í•œ ê²½ìš°
5. **ì„¤ë¬¸ì¡°ì‚¬ ë°ì´í„°**: ì‘ë‹µ ëˆ„ë½ íŒ¨í„´ì´ ì¤‘ìš”í•œ ì˜ë¯¸ë¥¼ ê°€ì§€ëŠ” ê²½ìš°

### 1.3 ì™œ ì¤‘ìš”í•œê°€?

ê²°ì¸¡ê°’ íŒ¨í„´ì„ ì œëŒ€ë¡œ ì´í•´í•˜ì§€ ëª»í•˜ë©´:
- **ë¶€ì ì ˆí•œ imputation**: ì˜ëª»ëœ ë°©ë²•ìœ¼ë¡œ ê²°ì¸¡ê°’ì„ ì±„ì›Œ í¸í–¥(bias) ë°œìƒ
- **ì •ë³´ ì†ì‹¤**: ê²°ì¸¡ê°’ì— ë‹´ê¸´ ì¤‘ìš”í•œ ì •ë³´(ì˜ˆ: ì‘ë‹µ ê±°ë¶€) ë¬´ì‹œ
- **ëª¨ë¸ ì„±ëŠ¥ ì €í•˜**: ML ëª¨ë¸ì´ ì˜ëª»ëœ ê°€ì •ì— ê¸°ë°˜í•˜ì—¬ í•™ìŠµ
- **ì˜ëª»ëœ ê²°ë¡ **: í†µê³„ì  ì¶”ë¡ ì´ ì™œê³¡ë¨

---

## 2. ì´ë¡ ì  ë°°ê²½

### 2.1 ê²°ì¸¡ ë©”ì»¤ë‹ˆì¦˜ (Missing Data Mechanisms)

ê²°ì¸¡ê°’ì€ ì™œ ë°œìƒí–ˆëŠ”ê°€ì— ë”°ë¼ ì„¸ ê°€ì§€ ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ ë¶„ë¥˜ë©ë‹ˆë‹¤ (Rubin, 1976).

#### 2.1.1 MCAR (Missing Completely At Random)

**ì •ì˜**: ê²°ì¸¡ê°’ì´ ì™„ì „íˆ ë¬´ì‘ìœ„ë¡œ ë°œìƒ. ê²°ì¸¡ ì—¬ë¶€ê°€ ê´€ì¸¡ëœ ë°ì´í„°ë‚˜ ê²°ì¸¡ëœ ë°ì´í„° ìì²´ì™€ ë¬´ê´€í•¨.

**ìˆ˜í•™ì  í‘œí˜„**:
```
P(Missing | X_observed, X_missing) = P(Missing)
```

**ì˜ˆì‹œ**:
- ì—°êµ¬ìê°€ ì‹¤ìˆ˜ë¡œ ë°ì´í„° ì¼ë¶€ë¥¼ ì…ë ¥í•˜ì§€ ì•ŠìŒ
- ì„¼ì„œê°€ ë¬´ì‘ìœ„ë¡œ ê³ ì¥ë‚¨
- ì„¤ë¬¸ì§€ ì¼ë¶€ê°€ ìš°ì—°íˆ ë¶„ì‹¤ë¨

**íŠ¹ì§•**:
- ê°€ì¥ ì´ìƒì ì¸ ê²°ì¸¡ ë©”ì»¤ë‹ˆì¦˜
- ê²°ì¸¡ëœ ë°ì´í„°ë¥¼ ì œê±°í•´ë„ í¸í–¥ì´ ë°œìƒí•˜ì§€ ì•ŠìŒ
- ë‹¨ìˆœ imputation(í‰ê· , ì¤‘ì•™ê°’)ë„ ë¹„êµì  ì•ˆì „

**ì‹ë³„ ë°©ë²•**:
- Little's MCAR Test
- ê²°ì¸¡ ì—¬ë¶€ì™€ ë‹¤ë¥¸ ë³€ìˆ˜ë“¤ ê°„ ìƒê´€ê´€ê³„ ê²€ì •

#### 2.1.2 MAR (Missing At Random)

**ì •ì˜**: ê²°ì¸¡ ì—¬ë¶€ê°€ ê´€ì¸¡ëœ ë°ì´í„°ì™€ëŠ” ê´€ë ¨ì´ ìˆì§€ë§Œ, ê²°ì¸¡ëœ ê°’ ìì²´ì™€ëŠ” ë¬´ê´€í•¨.

**ìˆ˜í•™ì  í‘œí˜„**:
```
P(Missing | X_observed, X_missing) = P(Missing | X_observed)
```

**ì˜ˆì‹œ**:
- ë‚¨ì„±ì´ ì—¬ì„±ë³´ë‹¤ ì†Œë“ì„ ëœ ë³´ê³ í•˜ëŠ” ê²½í–¥ (ì„±ë³„=ê´€ì¸¡ë¨, ì†Œë“=ê²°ì¸¡)
- ë‚˜ì´ê°€ ë§ì€ ì‚¬ëŒì´ ê±´ê°• ì„¤ë¬¸ì— ëœ ì‘ë‹µ (ë‚˜ì´=ê´€ì¸¡ë¨, ê±´ê°•=ê²°ì¸¡)
- ê³ ê°€ ì œí’ˆì˜ ê°€ê²© ì •ë³´ê°€ ë” ìì£¼ ëˆ„ë½ (ì¹´í…Œê³ ë¦¬=ê´€ì¸¡ë¨, ê°€ê²©=ê²°ì¸¡)

**íŠ¹ì§•**:
- ì‹¤ë¬´ì—ì„œ ê°€ì¥ í”í•œ ë©”ì»¤ë‹ˆì¦˜
- ê´€ì¸¡ëœ ë³€ìˆ˜ë¥¼ í™œìš©í•˜ì—¬ ê²°ì¸¡ê°’ ì˜ˆì¸¡ ê°€ëŠ¥
- Multiple Imputation, ML ê¸°ë°˜ imputation íš¨ê³¼ì 

**ì‹ë³„ ë°©ë²•**:
- ê²°ì¸¡ ì—¬ë¶€ë¥¼ ì¢…ì†ë³€ìˆ˜ë¡œ í•œ ë¡œì§€ìŠ¤í‹± íšŒê·€
- ê²°ì¸¡ ê·¸ë£¹ê³¼ ë¹„ê²°ì¸¡ ê·¸ë£¹ì˜ ê´€ì¸¡ëœ ë³€ìˆ˜ ë¹„êµ

#### 2.1.3 MNAR (Missing Not At Random)

**ì •ì˜**: ê²°ì¸¡ ì—¬ë¶€ê°€ ê²°ì¸¡ëœ ê°’ ìì²´ì™€ ê´€ë ¨ì´ ìˆìŒ.

**ìˆ˜í•™ì  í‘œí˜„**:
```
P(Missing | X_observed, X_missing) â‰  P(Missing | X_observed)
```

**ì˜ˆì‹œ**:
- ì†Œë“ì´ ë§¤ìš° ë†’ê±°ë‚˜ ë‚®ì€ ì‚¬ëŒì´ ì†Œë“ì„ ë³´ê³ í•˜ì§€ ì•ŠìŒ
- ìš°ìš¸ì¦ì´ ì‹¬í•œ ì‚¬ëŒì´ ìš°ìš¸ì¦ ì„¤ë¬¸ì— ì‘ë‹µí•˜ì§€ ì•ŠìŒ
- ì„±ì ì´ ë‚˜ìœ í•™ìƒì´ ì‹œí—˜ì„ ê²°ì‹œí•¨

**íŠ¹ì§•**:
- ê°€ì¥ ë‹¤ë£¨ê¸° ì–´ë ¤ìš´ ë©”ì»¤ë‹ˆì¦˜
- ë‹¨ìˆœ imputationì€ ì‹¬ê°í•œ í¸í–¥ ì•¼ê¸°
- ë„ë©”ì¸ ì§€ì‹ì´ í•„ìˆ˜ì 
- ë¯¼ê°ë„ ë¶„ì„(sensitivity analysis) í•„ìš”

**ì‹ë³„ ë°©ë²•**:
- ëª…í™•í•œ í†µê³„ì  ê²€ì • ì—†ìŒ
- ë„ë©”ì¸ ì§€ì‹ê³¼ ë…¼ë¦¬ì  ì¶”ë¡ ì— ì˜ì¡´
- íŒ¨í„´ ë¶„ì„ê³¼ ì „ë¬¸ê°€ íŒë‹¨

### 2.2 ê²°ì¸¡ê°’ íŒ¨í„´ ìœ í˜•

#### 2.2.1 ë‹¨ë³€ëŸ‰ íŒ¨í„´ (Univariate Pattern)
í•˜ë‚˜ì˜ ë³€ìˆ˜ë§Œ ê²°ì¸¡ê°’ì„ ê°€ì§.

```
X1  X2  X3  X4
10  5   3   ?
20  7   ?   8
30  ?   5   9
40  9   7   10
```

#### 2.2.2 ë‹¨ì¡° íŒ¨í„´ (Monotone Pattern)
ë³€ìˆ˜ë“¤ì„ ì •ë ¬í–ˆì„ ë•Œ ê²°ì¸¡ íŒ¨í„´ì´ ê³„ë‹¨ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚¨.

```
X1  X2  X3  X4
10  5   3   2
20  7   5   ?
30  9   ?   ?
40  ?   ?   ?
```

- ì¢…ë‹¨ ì—°êµ¬(longitudinal study)ì—ì„œ í”í•¨
- ì°¸ê°€ìê°€ ì¤‘ê°„ì— íƒˆë½í•˜ëŠ” ê²½ìš°

#### 2.2.3 ì„ì˜ íŒ¨í„´ (Arbitrary Pattern)
ë¶ˆê·œì¹™ì ì¸ ê²°ì¸¡ íŒ¨í„´.

```
X1  X2  X3  X4
10  ?   3   2
?   7   5   9
30  9   ?   ?
40  1   7   ?
```

- ê°€ì¥ ë³µì¡í•˜ê³  í”í•œ íŒ¨í„´
- ë‹¤ì–‘í•œ ê²°ì¸¡ ë©”ì»¤ë‹ˆì¦˜ì´ í˜¼ì¬

### 2.3 ì‹¤ì œ ì‹œë‚˜ë¦¬ì˜¤

#### ì‹œë‚˜ë¦¬ì˜¤ 1: ì˜ë£Œ ì„¤ë¬¸ì¡°ì‚¬
**ìƒí™©**: 5,000ëª…ì˜ ê±´ê°• ì„¤ë¬¸ ë°ì´í„°
**ê²°ì¸¡ íŒ¨í„´**:
- ì†Œë“ ì§ˆë¬¸: 15% ê²°ì¸¡ (ê³ ì†Œë“/ì €ì†Œë“ìê°€ ë” ë§ì´ ëˆ„ë½ - MNAR)
- ì²´ì¤‘: 8% ê²°ì¸¡ (ì—¬ì„±ì´ ë‚¨ì„±ë³´ë‹¤ ë” ë§ì´ ëˆ„ë½ - MAR)
- í˜ˆì••: 3% ê²°ì¸¡ (ì¸¡ì • ì¥ë¹„ ì˜¤ë¥˜ - MCAR)

**ë°œê²¬ëœ íŒ¨í„´**:
- ì†Œë“ê³¼ ì²´ì¤‘ì´ í•¨ê»˜ ê²°ì¸¡ë˜ëŠ” ë¹„ìœ¨ì´ ë†’ìŒ (ê³µë™ ê²°ì¸¡)
- ë‚˜ì´ê°€ ë§ì„ìˆ˜ë¡ ì „ë°˜ì ì¸ ê²°ì¸¡ë¥  ì¦ê°€

**ëŒ€ì‘ ì „ëµ**:
- MCAR (í˜ˆì••): í‰ê·  ëŒ€ì²´ ê°€ëŠ¥
- MAR (ì²´ì¤‘): ì„±ë³„ì„ ê³ ë ¤í•œ KNN imputation
- MNAR (ì†Œë“): ë³„ë„ 'ì‘ë‹µê±°ë¶€' ë²”ì£¼ ìƒì„± ë˜ëŠ” ì‚­ì œ

#### ì‹œë‚˜ë¦¬ì˜¤ 2: ì „ììƒê±°ë˜ ë¡œê·¸
**ìƒí™©**: 100ë§Œ ê±´ì˜ ê±°ë˜ ë°ì´í„°
**ê²°ì¸¡ íŒ¨í„´**:
- ë°°ì†¡ ì£¼ì†Œ: 0.1% ê²°ì¸¡ (ì‹œìŠ¤í…œ ì˜¤ë¥˜ - MCAR)
- ì œí’ˆ ë¦¬ë·°: 70% ê²°ì¸¡ (ê³ ê°ì´ ì‘ì„± ì•ˆ í•¨ - MNAR)
- í• ì¸ ì½”ë“œ: 85% ê²°ì¸¡ (ëŒ€ë¶€ë¶„ ì‚¬ìš© ì•ˆ í•¨ - MNAR, but ì˜ë„ì )

**ë°œê²¬ëœ íŒ¨í„´**:
- ë¦¬ë·° ê²°ì¸¡ ì—¬ë¶€ê°€ ì œí’ˆ ë§Œì¡±ë„ì™€ ê´€ë ¨ (MNAR)
- í• ì¸ ì½”ë“œ ê²°ì¸¡ì€ ì •ìƒì ì¸ ìƒí™©

**ëŒ€ì‘ ì „ëµ**:
- ë°°ì†¡ ì£¼ì†Œ: ë‹¤ë¥¸ ì£¼ì†Œ ì •ë³´ë¡œ imputation
- ì œí’ˆ ë¦¬ë·°: ê²°ì¸¡ì„ "ë¦¬ë·° ì—†ìŒ"ìœ¼ë¡œ ëª…ì‹œì  ì²˜ë¦¬
- í• ì¸ ì½”ë“œ: 0 ë˜ëŠ” "ë¯¸ì‚¬ìš©"ìœ¼ë¡œ ì±„ì›€

---

## 3. êµ¬í˜„: ìƒì„¸ Python ì½”ë“œ

### 3.1 ê²°ì¸¡ê°’ ì¢…í•© ë¶„ì„

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple
import warnings
warnings.filterwarnings('ignore')

def analyze_missing_patterns(df: pd.DataFrame, 
                            threshold: float = 0.0) -> Dict:
    """
    Comprehensive missing data pattern analysis
    
    Parameters:
    -----------
    df : pd.DataFrame
        Input dataframe
    threshold : float
        Only analyze columns with missing % >= threshold (default: 0)
        
    Returns:
    --------
    dict
        Complete missing data analysis including statistics, patterns, and mechanisms
        
    Example:
    --------
    >>> df = pd.read_csv('survey_data.csv')
    >>> missing_analysis = analyze_missing_patterns(df)
    >>> print(missing_analysis['summary'])
    """
    
    analysis = {
        'summary': {},
        'column_stats': pd.DataFrame(),
        'patterns': {},
        'correlations': pd.DataFrame(),
        'mechanisms': {}
    }
    
    # ===== 1. ê¸°ë³¸ í†µê³„ =====
    total_cells = df.size
    missing_cells = df.isnull().sum().sum()
    
    analysis['summary'] = {
        'total_rows': len(df),
        'total_columns': len(df.columns),
        'total_cells': total_cells,
        'missing_cells': missing_cells,
        'missing_pct': round(100 * missing_cells / total_cells, 2),
        'columns_with_missing': (df.isnull().sum() > 0).sum(),
        'rows_with_missing': (df.isnull().any(axis=1)).sum(),
        'rows_with_missing_pct': round(100 * (df.isnull().any(axis=1)).sum() / len(df), 2),
        'complete_rows': (~df.isnull().any(axis=1)).sum(),
        'complete_rows_pct': round(100 * (~df.isnull().any(axis=1)).sum() / len(df), 2)
    }
    
    # ===== 2. ì»¬ëŸ¼ë³„ ê²°ì¸¡ í†µê³„ =====
    column_stats = []
    
    for col in df.columns:
        missing_count = df[col].isnull().sum()
        missing_pct = 100 * missing_count / len(df)
        
        if missing_pct >= threshold:
            stats = {
                'column': col,
                'dtype': str(df[col].dtype),
                'missing_count': missing_count,
                'missing_pct': round(missing_pct, 2),
                'present_count': len(df) - missing_count,
                'present_pct': round(100 - missing_pct, 2),
                'first_missing_index': df[df[col].isnull()].index[0] if missing_count > 0 else None,
                'last_missing_index': df[df[col].isnull()].index[-1] if missing_count > 0 else None
            }
            
            column_stats.append(stats)
    
    analysis['column_stats'] = pd.DataFrame(column_stats).sort_values('missing_pct', ascending=False)
    
    # ===== 3. ê²°ì¸¡ íŒ¨í„´ ë¶„ì„ =====
    
    # 3.1 íŒ¨í„´ ìœ í˜• ì‹ë³„
    missing_mask = df.isnull()
    
    # íŒ¨í„´ë³„ í–‰ ê°œìˆ˜
    pattern_counts = missing_mask.groupby(list(missing_mask.columns)).size().sort_values(ascending=False)
    
    analysis['patterns']['unique_patterns'] = len(pattern_counts)
    analysis['patterns']['most_common_patterns'] = pattern_counts.head(10).to_dict()
    
    # 3.2 ë‹¨ì¡° íŒ¨í„´ ê²€ì‚¬
    analysis['patterns']['is_monotone'] = check_monotone_pattern(df)
    
    # 3.3 ê³µë™ ê²°ì¸¡ ë¶„ì„
    cooccurrence = analyze_cooccurrence(df)
    analysis['patterns']['cooccurrence_pairs'] = cooccurrence
    
    # ===== 4. ê²°ì¸¡ê°’ ìƒê´€ê´€ê³„ =====
    # ê²°ì¸¡ ì—¬ë¶€ë¥¼ ì´ì§„ ë³€ìˆ˜ë¡œ ë³€í™˜í•˜ì—¬ ìƒê´€ê´€ê³„ ê³„ì‚°
    missing_binary = df.isnull().astype(int)
    
    # ìƒê´€ê´€ê³„ê°€ ìˆëŠ” ì»¬ëŸ¼ë§Œ (ì ì–´ë„ í•˜ë‚˜ì— ê²°ì¸¡ê°’ì´ ìˆëŠ” ê²½ìš°)
    cols_with_missing = missing_binary.columns[missing_binary.sum() > 0].tolist()
    
    if len(cols_with_missing) > 1:
        missing_corr = missing_binary[cols_with_missing].corr()
        
        # ê°•í•œ ìƒê´€ê´€ê³„ë§Œ ì¶”ì¶œ (|r| > 0.3)
        strong_corr = []
        for i in range(len(missing_corr.columns)):
            for j in range(i+1, len(missing_corr.columns)):
                corr_value = missing_corr.iloc[i, j]
                if abs(corr_value) > 0.3:
                    strong_corr.append({
                        'column1': missing_corr.columns[i],
                        'column2': missing_corr.columns[j],
                        'correlation': round(corr_value, 3)
                    })
        
        analysis['correlations'] = pd.DataFrame(strong_corr)
    
    # ===== 5. ê²°ì¸¡ ë©”ì»¤ë‹ˆì¦˜ ì¶”ì • =====
    for col in cols_with_missing:
        mechanism = estimate_missing_mechanism(df, col)
        analysis['mechanisms'][col] = mechanism
    
    return analysis


def check_monotone_pattern(df: pd.DataFrame) -> bool:
    """
    Check if missing data follows monotone pattern
    
    Returns:
    --------
    bool
        True if pattern is monotone
    """
    
    missing_mask = df.isnull()
    
    # ê²°ì¸¡ë¥ ë¡œ ì»¬ëŸ¼ ì •ë ¬
    sorted_cols = missing_mask.sum().sort_values().index
    sorted_missing = missing_mask[sorted_cols]
    
    # ë‹¨ì¡°ì„± ê²€ì‚¬: ì´ì „ ì»¬ëŸ¼ì´ ê²°ì¸¡ì´ë©´ ì´í›„ ì»¬ëŸ¼ë„ ê²°ì¸¡ì´ì–´ì•¼ í•¨
    for i in range(len(sorted_cols) - 1):
        col1 = sorted_cols[i]
        col2 = sorted_cols[i + 1]
        
        # col1ì´ ê²°ì¸¡ì´ì§€ë§Œ col2ê°€ ê²°ì¸¡ì´ ì•„ë‹Œ ê²½ìš°ê°€ ìˆìœ¼ë©´ ë‹¨ì¡° íŒ¨í„´ ì•„ë‹˜
        if (sorted_missing[col1] & ~sorted_missing[col2]).any():
            return False
    
    return True


def analyze_cooccurrence(df: pd.DataFrame, 
                         min_count: int = 5) -> List[Dict]:
    """
    Analyze co-occurrence of missing values
    
    Parameters:
    -----------
    min_count : int
        Minimum co-occurrence count to report
        
    Returns:
    --------
    list
        List of column pairs with significant co-occurrence
    """
    
    missing_mask = df.isnull()
    cols_with_missing = missing_mask.columns[missing_mask.sum() > 0].tolist()
    
    cooccurrence_pairs = []
    
    for i in range(len(cols_with_missing)):
        for j in range(i+1, len(cols_with_missing)):
            col1 = cols_with_missing[i]
            col2 = cols_with_missing[j]
            
            # ë‘ ì»¬ëŸ¼ì´ ë™ì‹œì— ê²°ì¸¡ì¸ í–‰ ê°œìˆ˜
            both_missing = (missing_mask[col1] & missing_mask[col2]).sum()
            
            if both_missing >= min_count:
                # ê¸°ëŒ€ ë¹ˆë„ (ë…ë¦½ ê°€ì •)
                expected = (missing_mask[col1].sum() * missing_mask[col2].sum()) / len(df)
                
                # ê´€ì¸¡ ë¹ˆë„ / ê¸°ëŒ€ ë¹ˆë„
                ratio = both_missing / expected if expected > 0 else 0
                
                cooccurrence_pairs.append({
                    'column1': col1,
                    'column2': col2,
                    'both_missing_count': both_missing,
                    'both_missing_pct': round(100 * both_missing / len(df), 2),
                    'expected_count': round(expected, 1),
                    'obs_exp_ratio': round(ratio, 2),
                    'association': 'Strong' if ratio > 2 else ('Moderate' if ratio > 1.5 else 'Weak')
                })
    
    return sorted(cooccurrence_pairs, key=lambda x: x['obs_exp_ratio'], reverse=True)


def estimate_missing_mechanism(df: pd.DataFrame, 
                               target_col: str,
                               alpha: float = 0.05) -> Dict:
    """
    Estimate missing data mechanism for a column
    
    Parameters:
    -----------
    target_col : str
        Column to analyze
    alpha : float
        Significance level for tests
        
    Returns:
    --------
    dict
        Estimated mechanism and supporting evidence
    """
    
    from scipy import stats
    
    result = {
        'column': target_col,
        'mechanism': 'Unknown',
        'confidence': 'Low',
        'evidence': []
    }
    
    missing_mask = df[target_col].isnull()
    
    if missing_mask.sum() == 0:
        result['mechanism'] = 'No Missing Data'
        return result
    
    # Test 1: Little's MCAR Test (simplified version)
    # ì‹¤ì œë¡œëŠ” ì™„ì „í•œ Little's MCAR testê°€ ë³µì¡í•˜ë¯€ë¡œ ê°„ì†Œí™”ëœ ë²„ì „ ì‚¬ìš©
    
    # Test 2: Compare observed variables between missing and non-missing groups
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    numeric_cols = [c for c in numeric_cols if c != target_col]
    
    significant_differences = 0
    
    for col in numeric_cols[:10]:  # ì²˜ìŒ 10ê°œ ì»¬ëŸ¼ë§Œ í…ŒìŠ¤íŠ¸ (ì†ë„)
        group_missing = df[missing_mask][col].dropna()
        group_present = df[~missing_mask][col].dropna()
        
        if len(group_missing) > 0 and len(group_present) > 0:
            # T-test
            stat, p_value = stats.ttest_ind(group_missing, group_present, equal_var=False)
            
            if p_value < alpha:
                significant_differences += 1
                result['evidence'].append(f"{col}: significant difference (p={p_value:.4f})")
    
    # Mechanism estimation based on tests
    if significant_differences == 0:
        result['mechanism'] = 'MCAR'
        result['confidence'] = 'Medium'
        result['evidence'].append(f"No significant differences found in {len(numeric_cols)} variables")
    
    elif significant_differences < len(numeric_cols) * 0.3:
        result['mechanism'] = 'MAR'
        result['confidence'] = 'Medium'
        result['evidence'].append(f"{significant_differences}/{len(numeric_cols)} variables show differences")
    
    else:
        result['mechanism'] = 'MAR or MNAR'
        result['confidence'] = 'Low'
        result['evidence'].append(f"Many variables ({significant_differences}) show differences")
        result['evidence'].append("Domain knowledge required to distinguish MAR from MNAR")
    
    return result
```

### 3.2 ê²°ì¸¡ê°’ ì‹œê°í™” (missingno í™œìš©)

```python
import missingno as msno

def visualize_missing_bar(df: pd.DataFrame, 
                         figsize: Tuple[int, int] = (12, 6),
                         save_path: str = None) -> None:
    """
    Bar chart showing missing data counts
    
    Parameters:
    -----------
    df : pd.DataFrame
        Input dataframe
    figsize : tuple
        Figure size
    save_path : str, optional
        Path to save figure
        
    Example:
    --------
    >>> visualize_missing_bar(df, save_path='missing_bar.png')
    """
    
    fig, ax = plt.subplots(figsize=figsize)
    msno.bar(df, ax=ax, color='steelblue', fontsize=10)
    
    plt.title('Missing Data Count by Column', fontsize=14, fontweight='bold')
    plt.ylabel('Non-Missing Count', fontsize=12)
    plt.xlabel('Columns', fontsize=12)
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"âœ“ Bar chart saved to {save_path}")
    
    plt.show()


def visualize_missing_matrix(df: pd.DataFrame,
                             figsize: Tuple[int, int] = (12, 8),
                             sample: int = None,
                             save_path: str = None) -> None:
    """
    Matrix visualization of missing data patterns
    
    Shows data completeness pattern - useful for identifying:
    - Sequential patterns
    - Clusters of missing data
    - Relationships between columns
    
    Parameters:
    -----------
    sample : int, optional
        Number of rows to sample (for large datasets)
    """
    
    df_plot = df.sample(n=sample) if sample and len(df) > sample else df
    
    fig, ax = plt.subplots(figsize=figsize)
    msno.matrix(df_plot, ax=ax, sparkline=True, fontsize=10)
    
    plt.title('Missing Data Matrix (White = Missing)', fontsize=14, fontweight='bold')
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"âœ“ Matrix chart saved to {save_path}")
    
    plt.show()


def visualize_missing_heatmap(df: pd.DataFrame,
                              figsize: Tuple[int, int] = (10, 8),
                              save_path: str = None) -> None:
    """
    Heatmap showing correlation of missing values between columns
    
    Useful for identifying:
    - Columns that tend to be missing together
    - Strong co-occurrence patterns
    """
    
    fig, ax = plt.subplots(figsize=figsize)
    msno.heatmap(df, ax=ax, fontsize=10, cmap='RdYlGn_r')
    
    plt.title('Missing Data Correlation Heatmap', fontsize=14, fontweight='bold')
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"âœ“ Heatmap saved to {save_path}")
    
    plt.show()


def visualize_missing_dendrogram(df: pd.DataFrame,
                                 figsize: Tuple[int, int] = (12, 6),
                                 save_path: str = None) -> None:
    """
    Dendrogram showing hierarchical clustering of missing data patterns
    
    Useful for identifying:
    - Groups of columns with similar missingness patterns
    - Which columns' missingness can be predicted from others
    """
    
    fig, ax = plt.subplots(figsize=figsize)
    msno.dendrogram(df, ax=ax, fontsize=10)
    
    plt.title('Missing Data Dendrogram', fontsize=14, fontweight='bold')
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"âœ“ Dendrogram saved to {save_path}")
    
    plt.show()


def visualize_all_missing(df: pd.DataFrame,
                         sample: int = None,
                         save_dir: str = None) -> None:
    """
    Generate all four missing data visualizations
    
    Parameters:
    -----------
    save_dir : str, optional
        Directory to save all figures
    """
    
    print("Generating missing data visualizations...")
    print("=" * 60)
    
    # 1. Bar chart
    print("\n1. Bar Chart (Missing counts)")
    save_path = f"{save_dir}/missing_bar.png" if save_dir else None
    visualize_missing_bar(df, save_path=save_path)
    
    # 2. Matrix
    print("\n2. Matrix (Missingness pattern)")
    save_path = f"{save_dir}/missing_matrix.png" if save_dir else None
    visualize_missing_matrix(df, sample=sample, save_path=save_path)
    
    # 3. Heatmap
    print("\n3. Heatmap (Missing correlations)")
    save_path = f"{save_dir}/missing_heatmap.png" if save_dir else None
    visualize_missing_heatmap(df, save_path=save_path)
    
    # 4. Dendrogram
    print("\n4. Dendrogram (Hierarchical clustering)")
    save_path = f"{save_dir}/missing_dendrogram.png" if save_dir else None
    visualize_missing_dendrogram(df, save_path=save_path)
    
    print("\n" + "=" * 60)
    print("âœ“ All visualizations generated!")
```

### 3.3 Little's MCAR Test

```python
def littles_mcar_test(df: pd.DataFrame, 
                     alpha: float = 0.05) -> Dict:
    """
    Little's MCAR (Missing Completely At Random) Test
    
    Null Hypothesis: Data is MCAR
    If p-value < alpha: Reject null (data is NOT MCAR, likely MAR or MNAR)
    If p-value >= alpha: Fail to reject (data may be MCAR)
    
    Note: This is a simplified implementation. For production use,
    consider using specialized packages like 'statsmodels' or 'fancyimpute'.
    
    Parameters:
    -----------
    df : pd.DataFrame
        Input dataframe (numeric columns only)
    alpha : float
        Significance level
        
    Returns:
    --------
    dict
        Test results including chi-square statistic and p-value
        
    Example:
    --------
    >>> result = littles_mcar_test(df)
    >>> print(f"MCAR Test p-value: {result['p_value']:.4f}")
    >>> print(f"Conclusion: {result['conclusion']}")
    """
    
    from scipy import stats
    
    # Select only numeric columns
    df_numeric = df.select_dtypes(include=[np.number])
    
    if df_numeric.empty:
        return {'error': 'No numeric columns found'}
    
    # Remove columns with no missing data
    cols_with_missing = df_numeric.columns[df_numeric.isnull().any()].tolist()
    
    if not cols_with_missing:
        return {
            'test': "Little's MCAR Test",
            'conclusion': 'No missing data to test',
            'p_value': 1.0
        }
    
    df_test = df_numeric[cols_with_missing]
    
    # Create missing indicator matrix
    missing_patterns = df_test.isnull().astype(int)
    unique_patterns = missing_patterns.drop_duplicates()
    
    # Calculate chi-square statistic (simplified version)
    chi_square = 0
    df_freedom = 0
    
    for _, pattern in unique_patterns.iterrows():
        # Get rows matching this pattern
        mask = (missing_patterns == pattern).all(axis=1)
        pattern_data = df_test[mask]
        
        # Compare means of observed values
        for col in cols_with_missing:
            if not pattern[col]:  # If this column is observed in this pattern
                observed_mean = pattern_data[col].mean()
                overall_mean = df_test[col].mean()
                observed_var = pattern_data[col].var()
                
                if observed_var > 0:
                    chi_square += ((observed_mean - overall_mean) ** 2) / observed_var
                    df_freedom += 1
    
    # Calculate p-value
    p_value = 1 - stats.chi2.cdf(chi_square, df_freedom) if df_freedom > 0 else 1.0
    
    # Interpretation
    if p_value < alpha:
        conclusion = "Reject MCAR: Data is likely MAR or MNAR"
        recommendation = "Use advanced imputation methods (KNN, MICE)"
    else:
        conclusion = "Fail to reject MCAR: Data may be MCAR"
        recommendation = "Simple imputation (mean, median) may be acceptable"
    
    return {
        'test': "Little's MCAR Test (Simplified)",
        'chi_square': round(chi_square, 4),
        'degrees_of_freedom': df_freedom,
        'p_value': round(p_value, 4),
        'alpha': alpha,
        'conclusion': conclusion,
        'recommendation': recommendation
    }
```

### 3.4 ê²°ì¸¡ê°’ ê°„ ìƒê´€ê´€ê³„ ë¶„ì„

```python
def find_missing_correlations(df: pd.DataFrame,
                              threshold: float = 0.3,
                              method: str = 'pearson') -> pd.DataFrame:
    """
    Find correlations between missing indicators of different columns
    
    Parameters:
    -----------
    threshold : float
        Minimum absolute correlation to report
    method : str
        'pearson', 'spearman', or 'kendall'
        
    Returns:
    --------
    pd.DataFrame
        Pairs of columns with correlated missingness
        
    Example:
    --------
    >>> corr_pairs = find_missing_correlations(df, threshold=0.5)
    >>> print(corr_pairs)
    """
    
    # Convert missing to binary indicators
    missing_indicators = df.isnull().astype(int)
    
    # Calculate correlation matrix
    corr_matrix = missing_indicators.corr(method=method)
    
    # Extract pairs with correlation above threshold
    corr_pairs = []
    
    for i in range(len(corr_matrix.columns)):
        for j in range(i+1, len(corr_matrix.columns)):
            corr_value = corr_matrix.iloc[i, j]
            
            if abs(corr_value) >= threshold:
                col1 = corr_matrix.columns[i]
                col2 = corr_matrix.columns[j]
                
                # Additional statistics
                both_missing = (missing_indicators[col1] & missing_indicators[col2]).sum()
                either_missing = (missing_indicators[col1] | missing_indicators[col2]).sum()
                
                corr_pairs.append({
                    'column1': col1,
                    'column2': col2,
                    'correlation': round(corr_value, 3),
                    'both_missing': both_missing,
                    'either_missing': either_missing,
                    'jaccard_index': round(both_missing / either_missing if either_missing > 0 else 0, 3)
                })
    
    result_df = pd.DataFrame(corr_pairs).sort_values('correlation', 
                                                     key=abs, 
                                                     ascending=False)
    
    return result_df


def visualize_missing_correlation_network(df: pd.DataFrame,
                                         threshold: float = 0.5,
                                         figsize: Tuple[int, int] = (12, 10)) -> None:
    """
    Visualize missing data correlations as a network graph
    
    Requires: networkx, matplotlib
    """
    
    try:
        import networkx as nx
    except ImportError:
        print("NetworkX not installed. Install with: pip install networkx")
        return
    
    # Get correlated pairs
    corr_pairs = find_missing_correlations(df, threshold=threshold)
    
    if len(corr_pairs) == 0:
        print(f"No correlations found above threshold {threshold}")
        return
    
    # Create graph
    G = nx.Graph()
    
    for _, row in corr_pairs.iterrows():
        G.add_edge(row['column1'], row['column2'], 
                  weight=abs(row['correlation']))
    
    # Draw graph
    plt.figure(figsize=figsize)
    pos = nx.spring_layout(G, k=0.5, iterations=50)
    
    # Draw nodes
    nx.draw_networkx_nodes(G, pos, node_color='lightblue', 
                          node_size=3000, alpha=0.9)
    
    # Draw edges (thickness based on correlation strength)
    edges = G.edges()
    weights = [G[u][v]['weight'] for u, v in edges]
    nx.draw_networkx_edges(G, pos, width=[w*5 for w in weights], 
                          alpha=0.6, edge_color='gray')
    
    # Draw labels
    nx.draw_networkx_labels(G, pos, font_size=10, font_weight='bold')
    
    # Draw edge labels (correlations)
    edge_labels = {(row['column1'], row['column2']): f"{row['correlation']:.2f}" 
                   for _, row in corr_pairs.iterrows()}
    nx.draw_networkx_edge_labels(G, pos, edge_labels, font_size=8)
    
    plt.title(f'Missing Data Correlation Network (threshold={threshold})', 
             fontsize=14, fontweight='bold')
    plt.axis('off')
    plt.tight_layout()
    plt.show()
```

---

## 4. ì˜ˆì‹œ: ì…ì¶œë ¥ ìƒ˜í”Œ ë° ì‹œê°í™”

### 4.1 ìƒ˜í”Œ ë°ì´í„° ìƒì„±

```python
# Create sample dataset with intentional missing patterns
np.random.seed(42)

n_samples = 1000

# Create base data
data = {
    'age': np.random.randint(18, 80, n_samples),
    'income': np.random.uniform(20000, 150000, n_samples),
    'education_years': np.random.randint(8, 20, n_samples),
    'health_score': np.random.uniform(0, 100, n_samples),
    'exercise_hours': np.random.uniform(0, 20, n_samples),
    'bmi': np.random.uniform(18, 35, n_samples)
}

sample_df = pd.DataFrame(data)

# Introduce missing patterns

# 1. MCAR: Random 5% missing in age
mcar_mask = np.random.random(n_samples) < 0.05
sample_df.loc[mcar_mask, 'age'] = np.nan

# 2. MAR: Income missing more for older people
mar_mask = (sample_df['age'] > 60) & (np.random.random(n_samples) < 0.3)
sample_df.loc[mar_mask, 'income'] = np.nan

# 3. MNAR: Low health scores missing more often
mnar_mask = (sample_df['health_score'] < 30) & (np.random.random(n_samples) < 0.4)
sample_df.loc[mnar_mask, 'health_score'] = np.nan

# 4. Co-occurrence: Exercise and BMI missing together
cooccur_mask = np.random.random(n_samples) < 0.1
sample_df.loc[cooccur_mask, 'exercise_hours'] = np.nan
sample_df.loc[cooccur_mask, 'bmi'] = np.nan

print("Sample dataset created with missing data:")
print(sample_df.isnull().sum())
print(f"\nTotal missing cells: {sample_df.isnull().sum().sum()}")
```

### 4.2 ì¢…í•© ë¶„ì„ ì‹¤í–‰

```python
# Run comprehensive analysis
analysis = analyze_missing_patterns(sample_df)

# Print summary
print("=" * 80)
print("MISSING DATA PATTERN ANALYSIS")
print("=" * 80)

print("\nğŸ“Š SUMMARY STATISTICS")
print("-" * 80)
for key, value in analysis['summary'].items():
    print(f"{key:.<40} {value}")

print("\n\nğŸ“ˆ COLUMN-LEVEL STATISTICS")
print("-" * 80)
print(analysis['column_stats'].to_string(index=False))

print("\n\nğŸ”— CO-OCCURRENCE PATTERNS")
print("-" * 80)
if analysis['patterns']['cooccurrence_pairs']:
    for pair in analysis['patterns']['cooccurrence_pairs'][:5]:
        print(f"{pair['column1']} â†” {pair['column2']}: "
              f"{pair['both_missing_count']} rows ({pair['both_missing_pct']}%), "
              f"Association: {pair['association']}")
else:
    print("No significant co-occurrence found")

print("\n\nğŸ” MISSING MECHANISMS (Estimated)")
print("-" * 80)
for col, mech in analysis['mechanisms'].items():
    print(f"\n{col}:")
    print(f"  Mechanism: {mech['mechanism']}")
    print(f"  Confidence: {mech['confidence']}")
    if mech['evidence']:
        print(f"  Evidence:")
        for ev in mech['evidence'][:3]:
            print(f"    - {ev}")

print("\n\nğŸ“Š MISSING CORRELATIONS")
print("-" * 80)
if not analysis['correlations'].empty:
    print(analysis['correlations'].to_string(index=False))
else:
    print("No strong correlations found")
```

### 4.3 ì‹œê°í™” ìƒì„±

```python
# Generate all visualizations
print("\nGenerating visualizations...")
visualize_all_missing(sample_df, sample=500, save_dir='./missing_analysis')

# Network visualization
visualize_missing_correlation_network(sample_df, threshold=0.3)

# Little's MCAR Test
mcar_result = littles_mcar_test(sample_df)
print("\n" + "=" * 80)
print("LITTLE'S MCAR TEST")
print("=" * 80)
for key, value in mcar_result.items():
    print(f"{key}: {value}")
```

---

## 5. ì—ì´ì „íŠ¸ ë§¤í•‘

### 5.1 Primary Agent: `data-cleaning-specialist`

**ì—­í• **:
- ê²°ì¸¡ê°’ íŒ¨í„´ ë¶„ì„ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì£¼ë„
- ê²°ì¸¡ ë©”ì»¤ë‹ˆì¦˜ ì‹ë³„ ë° í•´ì„
- imputation ì „ëµ ìˆ˜ë¦½

**ì‚¬ìš© í•¨ìˆ˜**:
- `analyze_missing_patterns()`
- `estimate_missing_mechanism()`
- `littles_mcar_test()`
- `find_missing_correlations()`

### 5.2 Supporting Agent: `data-visualization-specialist`

**ì—­í• **:
- ê²°ì¸¡ê°’ ì‹œê°í™” ìƒì„±
- íŒ¨í„´ ë°œê²¬ì„ ìœ„í•œ ê·¸ë˜í”„ ì‘ì„±

**ì‚¬ìš© í•¨ìˆ˜**:
- `visualize_missing_bar()`
- `visualize_missing_matrix()`
- `visualize_missing_heatmap()`
- `visualize_missing_dendrogram()`
- `visualize_missing_correlation_network()`

### 5.3 Supporting Agent: `data-scientist`

**ì—­í• **:
- í†µê³„ì  ê²€ì • í•´ì„
- ë„ë©”ì¸ ì§€ì‹ì„ í™œìš©í•œ ë©”ì»¤ë‹ˆì¦˜ íŒë‹¨

**ì‚¬ìš© í•¨ìˆ˜**:
- `littles_mcar_test()`
- `estimate_missing_mechanism()`

---

## 6. í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬

### 6.1 ì„¤ì¹˜ ëª…ë ¹

```bash
# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬
pip install pandas>=2.0.0
pip install numpy>=1.24.0
pip install scipy>=1.10.0

# ì‹œê°í™”
pip install matplotlib>=3.7.0
pip install seaborn>=0.12.0
pip install missingno>=0.5.2

# ë„¤íŠ¸ì›Œí¬ ì‹œê°í™” (ì„ íƒ)
pip install networkx>=3.0
```

### 6.2 ë¼ì´ë¸ŒëŸ¬ë¦¬ ìƒì„¸

| ë¼ì´ë¸ŒëŸ¬ë¦¬ | ë²„ì „ | ìš©ë„ | í•µì‹¬ ê¸°ëŠ¥ |
|-----------|------|------|----------|
| pandas | >=2.0.0 | ë°ì´í„° ì¡°ì‘ | `isnull()`, `dropna()`, `groupby()` |
| numpy | >=1.24.0 | ìˆ˜ì¹˜ ì—°ì‚° | ë°°ì—´ ì—°ì‚°, í†µê³„ |
| scipy | >=1.10.0 | í†µê³„ ê²€ì • | `ttest_ind()`, `chi2.cdf()` |
| missingno | >=0.5.2 | ê²°ì¸¡ê°’ ì‹œê°í™” | `bar()`, `matrix()`, `heatmap()`, `dendrogram()` |
| matplotlib | >=3.7.0 | ê¸°ë³¸ ì‹œê°í™” | í”Œë¡¯ ìƒì„± |
| seaborn | >=0.12.0 | í†µê³„ ì‹œê°í™” | í–¥ìƒëœ ìŠ¤íƒ€ì¼ |
| networkx | >=3.0 | ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„ | ìƒê´€ê´€ê³„ ë„¤íŠ¸ì›Œí¬ |

---

## 7. ì²´í¬í¬ì¸íŠ¸

### 7.1 ë¶„ì„ ì™„ë£Œ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] ëª¨ë“  ì»¬ëŸ¼ì˜ ê²°ì¸¡ë¥ ì„ í™•ì¸í–ˆëŠ”ê°€?
- [ ] ê²°ì¸¡ íŒ¨í„´ì„ ì‹œê°í™”í–ˆëŠ”ê°€? (bar, matrix, heatmap, dendrogram)
- [ ] ê³µë™ ê²°ì¸¡ íŒ¨í„´ì„ ì‹ë³„í–ˆëŠ”ê°€?
- [ ] ê²°ì¸¡ê°’ ê°„ ìƒê´€ê´€ê³„ë¥¼ ë¶„ì„í–ˆëŠ”ê°€?
- [ ] ê° ì»¬ëŸ¼ì˜ ê²°ì¸¡ ë©”ì»¤ë‹ˆì¦˜ì„ ì¶”ì •í–ˆëŠ”ê°€?
- [ ] Little's MCAR testë¥¼ ìˆ˜í–‰í–ˆëŠ”ê°€?

### 7.2 ë©”ì»¤ë‹ˆì¦˜ íŒë‹¨ ê°€ì´ë“œ

| ë©”ì»¤ë‹ˆì¦˜ | íŠ¹ì§• | ê²€ì¦ ë°©ë²• | ê¶Œì¥ ëŒ€ì‘ |
|---------|------|-----------|----------|
| MCAR | ì™„ì „ ë¬´ì‘ìœ„ | Little's test (p>0.05), ê·¸ë£¹ ê°„ ì°¨ì´ ì—†ìŒ | ë‹¨ìˆœ imputation ê°€ëŠ¥ |
| MAR | ê´€ì¸¡ ë³€ìˆ˜ì™€ ê´€ë ¨ | ê·¸ë£¹ ê°„ ìœ ì˜í•œ ì°¨ì´, ì˜ˆì¸¡ ê°€ëŠ¥ | KNN, MICE ë“± ê³ ê¸‰ imputation |
| MNAR | ê²°ì¸¡ê°’ ìì²´ì™€ ê´€ë ¨ | ë…¼ë¦¬ì  ì¶”ë¡ , ë„ë©”ì¸ ì§€ì‹ | ë³„ë„ ë²”ì£¼, ë¯¼ê°ë„ ë¶„ì„ |

### 7.3 í’ˆì§ˆ ê¸°ì¤€

**ìš°ìˆ˜ (Excellent)**:
- ê²°ì¸¡ë¥  < 5%
- MCAR ë©”ì»¤ë‹ˆì¦˜
- ê³µë™ ê²°ì¸¡ íŒ¨í„´ ì—†ìŒ

**ì–‘í˜¸ (Good)**:
- ê²°ì¸¡ë¥  5-15%
- MAR ë©”ì»¤ë‹ˆì¦˜
- ì•½í•œ ê³µë™ ê²°ì¸¡ íŒ¨í„´

**ë¯¸í¡ (Poor)**:
- ê²°ì¸¡ë¥  > 15%
- MNAR ë©”ì»¤ë‹ˆì¦˜
- ê°•í•œ ê³µë™ ê²°ì¸¡ íŒ¨í„´

---

## 8. íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 8.1 ì¼ë°˜ì  ë¬¸ì œ

#### ë¬¸ì œ 1: missingno ì‹œê°í™”ê°€ ëŠë¦¼
**ì¦ìƒ**: ëŒ€ìš©ëŸ‰ ë°ì´í„°ì—ì„œ ì‹œê°í™” ìƒì„±ì´ ì˜¤ë˜ ê±¸ë¦¼
**í•´ê²°**:
```python
# ìƒ˜í”Œë§ ì‚¬ìš©
sample_df = df.sample(n=10000, random_state=42)
visualize_missing_matrix(sample_df)
```

#### ë¬¸ì œ 2: Little's MCAR test ì˜¤ë¥˜
**ì¦ìƒ**: Chi-square ê³„ì‚° ì¤‘ ì˜¤ë¥˜
**í•´ê²°**:
```python
# ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë§Œ ì„ íƒ
df_numeric = df.select_dtypes(include=[np.number])
result = littles_mcar_test(df_numeric)
```

#### ë¬¸ì œ 3: ê²°ì¸¡ ìƒê´€ê´€ê³„ê°€ ëª¨ë‘ NaN
**ì¦ìƒ**: ê²°ì¸¡ê°’ì´ ë„ˆë¬´ ì ê±°ë‚˜ ë§ìŒ
**í•´ê²°**:
```python
# ê²°ì¸¡ë¥ ì´ 5-95% ì‚¬ì´ì¸ ì»¬ëŸ¼ë§Œ ë¶„ì„
cols_to_analyze = [col for col in df.columns 
                   if 5 < df[col].isnull().mean()*100 < 95]
result = find_missing_correlations(df[cols_to_analyze])
```

### 8.2 ë©”ì»¤ë‹ˆì¦˜ íŒë‹¨ ì‹œ ì£¼ì˜ì‚¬í•­

1. **MCARê³¼ MAR êµ¬ë¶„ì˜ ì–´ë ¤ì›€**:
   - í†µê³„ì  ê²€ì •ë§Œìœ¼ë¡œëŠ” ë¶ˆì¶©ë¶„
   - ë„ë©”ì¸ ì§€ì‹ í•„ìˆ˜
   - ë³´ìˆ˜ì ìœ¼ë¡œ ì ‘ê·¼ (ì˜ì‹¬ìŠ¤ëŸ¬ìš°ë©´ MARë¡œ ê°€ì •)

2. **MNAR ì‹ë³„ì˜ í•œê³„**:
   - ê²°ì¸¡ëœ ê°’ ìì²´ë¥¼ ê´€ì¸¡í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ëª…í™•í•œ ê²€ì¦ ë¶ˆê°€
   - ë…¼ë¦¬ì  ì¶”ë¡ ê³¼ ì „ë¬¸ê°€ íŒë‹¨ í•„ìš”
   - ë¯¼ê°ë„ ë¶„ì„ìœ¼ë¡œ ì˜í–¥ í‰ê°€

3. **í˜¼í•© ë©”ì»¤ë‹ˆì¦˜**:
   - ì‹¤ì œ ë°ì´í„°ëŠ” ì¢…ì¢… ì—¬ëŸ¬ ë©”ì»¤ë‹ˆì¦˜ì´ í˜¼ì¬
   - ì»¬ëŸ¼ë³„ë¡œ ë‹¤ë¥¸ ë©”ì»¤ë‹ˆì¦˜ ê°€ëŠ¥
   - ê°€ì¥ ë³´ìˆ˜ì ì¸ ê°€ì • ì±„íƒ

---

## 9. ì°¸ê³  ìë£Œ

### 9.1 ê³µì‹ ë¬¸ì„œ

- **missingno**: https://github.com/ResidentMario/missingno
  - Visualization gallery
  - API reference

- **Pandas Missing Data**: https://pandas.pydata.org/docs/user_guide/missing_data.html
  - Working with missing data
  - Best practices

- **SciPy Stats**: https://docs.scipy.org/doc/scipy/reference/stats.html
  - Statistical tests
  - Distributions

### 9.2 í•™ìˆ  ìë£Œ

- **Rubin, D. B. (1976)**. "Inference and missing data." Biometrika, 63(3), 581-592.
  - ê²°ì¸¡ ë©”ì»¤ë‹ˆì¦˜ì˜ ê³ ì „ì  ì •ì˜

- **Little, R. J. (1988)**. "A test of missing completely at random for multivariate data with missing values." Journal of the American Statistical Association, 83(404), 1198-1202.
  - Little's MCAR test ì›ë…¼ë¬¸

- **Schafer, J. L., & Graham, J. W. (2002)**. "Missing data: our view of the state of the art." Psychological methods, 7(2), 147.
  - ê²°ì¸¡ê°’ ì²˜ë¦¬ ì¢…í•© ë¦¬ë·°

### 9.3 ì¶”ì²œ ë„ì„œ

- **"Flexible Imputation of Missing Data"** by Stef van Buuren
  - MICE ì•Œê³ ë¦¬ì¦˜ ìƒì„¸ ì„¤ëª…
  - R ì¤‘ì‹¬ì´ì§€ë§Œ ê°œë…ì€ Pythonì—ë„ ì ìš© ê°€ëŠ¥

---

## 10. ìš”ì•½

### 10.1 í•µì‹¬ í¬ì¸íŠ¸

1. **ë©”ì»¤ë‹ˆì¦˜ ì´í•´**: MCAR, MAR, MNARì˜ ì°¨ì´ë¥¼ ì´í•´í•˜ê³  ì˜¬ë°”ë¥´ê²Œ ì‹ë³„
2. **íŒ¨í„´ ì‹œê°í™”**: 4ê°€ì§€ ì‹œê°í™”(bar, matrix, heatmap, dendrogram)ë¡œ ì¢…í•© íŒŒì•…
3. **ê³µë™ ê²°ì¸¡**: ì—¬ëŸ¬ ë³€ìˆ˜ê°€ í•¨ê»˜ ê²°ì¸¡ë˜ëŠ” íŒ¨í„´ íƒì§€
4. **í†µê³„ì  ê²€ì •**: Little's MCAR testë¡œ ê°ê´€ì  í‰ê°€

### 10.2 ì˜ì‚¬ê²°ì • í”Œë¡œìš°

```
1. ê²°ì¸¡ê°’ ë°œê²¬
   â†“
2. íŒ¨í„´ ë¶„ì„ (ì‹œê°í™” + í†µê³„)
   â†“
3. ë©”ì»¤ë‹ˆì¦˜ ì¶”ì •
   â”œâ”€ MCAR â†’ ë‹¨ìˆœ imputation ê°€ëŠ¥
   â”œâ”€ MAR â†’ ê³ ê¸‰ imputation í•„ìš”
   â””â”€ MNAR â†’ ë„ë©”ì¸ ì§€ì‹ + ë¯¼ê°ë„ ë¶„ì„
   â†“
4. imputation ì „ëµ ì„ íƒ (Reference 03)
```

### 10.3 ë‹¤ìŒ ë‹¨ê³„

íŒ¨í„´ ë¶„ì„ ì™„ë£Œ í›„:
- **Reference 03**: Imputation Strategies - ì ì ˆí•œ ëŒ€ì²´ ë°©ë²• ì„ íƒ ë° ì ìš©
- **Reference 02 ê²°ê³¼ í™œìš©**: ë©”ì»¤ë‹ˆì¦˜ì— ë”°ë¥¸ ìµœì  ì „ëµ ê²°ì •

---

**ì‘ì„±ì**: Claude Code  
**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2025-01-25  
**ì´ì „ ë ˆí¼ëŸ°ìŠ¤**: 01-data-quality-assessment.md  
**ë‹¤ìŒ ë ˆí¼ëŸ°ìŠ¤**: 03-imputation-strategies.md
