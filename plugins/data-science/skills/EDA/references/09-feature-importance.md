# 09. Feature Importance (íŠ¹ì„± ì¤‘ìš”ë„)

## 1. ê°œìš”

### 1.1 ëª©ì 
Feature ImportanceëŠ” ì˜ˆì¸¡ ëª¨ë¸ì—ì„œ ê° ë³€ìˆ˜(feature)ê°€ íƒ€ê²Ÿ ì˜ˆì¸¡ì— ì–¼ë§ˆë‚˜ ê¸°ì—¬í•˜ëŠ”ì§€ë¥¼ ì •ëŸ‰í™”í•˜ëŠ” ë¶„ì„ ê¸°ë²•ì…ë‹ˆë‹¤. ë¶ˆí•„ìš”í•œ ë³€ìˆ˜ë¥¼ ì œê±°í•˜ê³ , ì¤‘ìš” ë³€ìˆ˜ì— ì§‘ì¤‘í•˜ì—¬ ëª¨ë¸ ì„±ëŠ¥ê³¼ í•´ì„ ê°€ëŠ¥ì„±ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.

### 1.2 ì ìš© ì‹œê¸°
- ê³ ì°¨ì› ë°ì´í„°ì—ì„œ ì¤‘ìš” ë³€ìˆ˜ë¥¼ ì„ íƒí•˜ê³  ì‹¶ì„ ë•Œ
- ëª¨ë¸ ì„±ëŠ¥ì— ê¸°ì—¬í•˜ì§€ ì•ŠëŠ” ë³€ìˆ˜ë¥¼ ì œê±°í•˜ì—¬ ê³¼ì í•© ë°©ì§€
- ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ (ì–´ë–¤ ìš”ì¸ì´ ê²°ê³¼ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ê°€?)
- Feature engineering ìš°ì„ ìˆœìœ„ ê²°ì •
- ëª¨ë¸ í•´ì„ ë° ì„¤ëª… (Explainable AI)

### 1.3 ì£¼ìš” ê¸°ë²•
- **Tree-based Importance**: Random Forest, XGBoostì˜ ë¶ˆìˆœë„/gain ê¸°ë°˜
- **Permutation Importance**: ë³€ìˆ˜ ì…”í”Œ í›„ ì„±ëŠ¥ ì €í•˜ ì¸¡ì •
- **SHAP Values**: ê²Œì„ ì´ë¡  ê¸°ë°˜ ê¸°ì—¬ë„ ë¶„ì„
- **Coefficient-based**: ì„ í˜• ëª¨ë¸ì˜ ê³„ìˆ˜ í¬ê¸°
- **Recursive Feature Elimination (RFE)**: ìˆœì°¨ì  ì œê±°

---

## 2. ì´ë¡ ì  ë°°ê²½

### 2.1 íŠ¹ì„± ì¤‘ìš”ë„ì˜ ê°œë…

**í•µì‹¬ ì§ˆë¬¸**: "ì´ ë³€ìˆ˜ê°€ ì—†ë‹¤ë©´ ì˜ˆì¸¡ ì„±ëŠ¥ì´ ì–¼ë§ˆë‚˜ ë–¨ì–´ì§ˆê¹Œ?"

```
ì˜ˆì‹œ: ì£¼íƒ ê°€ê²© ì˜ˆì¸¡
Feature          Importance    í•´ì„
ë©´ì              0.45         â†’ ê°€ì¥ ì¤‘ìš” (45% ê¸°ì—¬)
ìœ„ì¹˜(ì§€ì—­)       0.25         â†’ ë‘ ë²ˆì§¸ë¡œ ì¤‘ìš”
ê±´ë¬¼ ì—°ì‹        0.15         â†’ ì„¸ ë²ˆì§¸
ë°© ê°œìˆ˜          0.08         â†’ ì¤‘ê°„
ì£¼ì°¨ ì—¬ë¶€        0.05         â†’ ë‚®ìŒ
í˜ì¸íŠ¸ ìƒ‰ìƒ      0.02         â†’ ê±°ì˜ ë¬´ê´€
```

### 2.2 íŠ¹ì„± ì¤‘ìš”ë„ ë°©ë²•ë¡  ë¹„êµ

#### 1. Impurity-based Importance (ë¶ˆìˆœë„ ê¸°ë°˜)
- **ì›ë¦¬**: ì˜ì‚¬ê²°ì •íŠ¸ë¦¬ì—ì„œ ê° ë³€ìˆ˜ê°€ ë¶„í• í•  ë•Œ ê°ì†Œì‹œí‚¨ ë¶ˆìˆœë„ í‰ê· 
- **ì¥ì **: 
  - ë¹ ë¥¸ ê³„ì‚°
  - í•™ìŠµê³¼ ë™ì‹œì— ê³„ì‚°
  - scikit-learnì—ì„œ ê¸°ë³¸ ì œê³µ
- **ë‹¨ì **: 
  - ê³ cardinality ë³€ìˆ˜ì— í¸í–¥
  - ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„ ìˆìœ¼ë©´ ë¶ˆì•ˆì •
  - Train ë°ì´í„°ì—ë§Œ ì˜ì¡´ (ê³¼ì í•© ê°€ëŠ¥)

#### 2. Permutation Importance (ìˆœì—´ ì¤‘ìš”ë„)
- **ì›ë¦¬**: ë³€ìˆ˜ë¥¼ ë¬´ì‘ìœ„ë¡œ ì„ì€ í›„ ì„±ëŠ¥ ì €í•˜ ì¸¡ì •
- **ì¥ì **: 
  - ëª¨ë“  ëª¨ë¸ì— ì ìš© ê°€ëŠ¥
  - Test ë°ì´í„°ë¡œ ê³„ì‚° ê°€ëŠ¥ (ì¼ë°˜í™”)
  - ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„ ì˜í–¥ ì ìŒ
- **ë‹¨ì **: 
  - ê³„ì‚° ë¹„ìš© ë†’ìŒ (Në²ˆ ë°˜ë³µ)
  - ìƒ˜í”Œë§ ì˜ì¡´ì„±

#### 3. SHAP (SHapley Additive exPlanations)
- **ì›ë¦¬**: ê²Œì„ ì´ë¡ ì˜ Shapley valueë¡œ ê° ë³€ìˆ˜ì˜ ê¸°ì—¬ë„ ê³„ì‚°
- **ì¥ì **: 
  - ì´ë¡ ì ìœ¼ë¡œ ê°€ì¥ ì •í™•
  - ê°œë³„ ì˜ˆì¸¡ ì„¤ëª… ê°€ëŠ¥
  - ì–‘ë°©í–¥ íš¨ê³¼(ê¸ì •/ë¶€ì •) êµ¬ë¶„
- **ë‹¨ì **: 
  - ê³„ì‚° ë¹„ìš© ë§¤ìš° ë†’ìŒ
  - ë³µì¡í•œ ê°œë…

### 2.3 ì‹œë‚˜ë¦¬ì˜¤

**ì‹œë‚˜ë¦¬ì˜¤ 1: ê³ ì°¨ì› ë°ì´í„° ì¶•ì†Œ**
```
ìƒí™©: 500ê°œ features, 10,000ê°œ ìƒ˜í”Œ
ë¬¸ì œ: ëª¨ë¸ í•™ìŠµ ì‹œê°„ 30ë¶„, ê³¼ì í•©

ë¶„ì„:
1. Random Forestë¡œ feature importance ê³„ì‚°
2. Importance > 0.01ì¸ ë³€ìˆ˜ë§Œ ì„ íƒ (50ê°œë¡œ ì¶•ì†Œ)
3. Permutation importanceë¡œ ê²€ì¦
4. SHAPë¡œ ìƒìœ„ 20ê°œ ë³€ìˆ˜ ì‹¬ì¸µ ë¶„ì„

ê²°ê³¼:
- ìµœì¢… 50ê°œ features ì„ íƒ
- í•™ìŠµ ì‹œê°„: 30ë¶„ â†’ 3ë¶„ (10ë°° ê°œì„ )
- ëª¨ë¸ ì„±ëŠ¥: ìœ ì§€ ë˜ëŠ” ì†Œí­ í–¥ìƒ
```

**ì‹œë‚˜ë¦¬ì˜¤ 2: ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ**
```
ìƒí™©: ê³ ê° ì´íƒˆ ì˜ˆì¸¡ ëª¨ë¸
ëª©í‘œ: ì´íƒˆ ë°©ì§€ ì „ëµ ìˆ˜ë¦½

ë¶„ì„:
1. Feature importanceë¡œ ì´íƒˆ ì£¼ìš” ìš”ì¸ ì‹ë³„
   â†’ ìƒìœ„ 5ê°œ: ê³ ê°ì„œë¹„ìŠ¤ ë§Œì¡±ë„, ê°€ê²©, ì‚¬ìš©ë¹ˆë„, ê²½ìŸì‚¬ í”„ë¡œëª¨ì…˜, ê³„ì•½ê¸°ê°„
2. SHAPë¡œ ê° ìš”ì¸ì˜ ì˜í–¥ ë°©í–¥ í™•ì¸
   â†’ ë§Œì¡±ë„ â†“ â†’ ì´íƒˆ â†‘
   â†’ ê°€ê²© â†‘ â†’ ì´íƒˆ â†‘

ì•¡ì…˜:
- ê³ ê°ì„œë¹„ìŠ¤ í’ˆì§ˆ ê°œì„  (1ìˆœìœ„)
- ê°€ê²© ê²½ìŸë ¥ í™•ë³´ (2ìˆœìœ„)
- ë¡œì—´í‹° í”„ë¡œê·¸ë¨ ê°•í™” (3ìˆœìœ„)
```

**ì‹œë‚˜ë¦¬ì˜¤ 3: ëª¨ë¸ ë””ë²„ê¹…**
```
ìƒí™©: ëª¨ë¸ ì„±ëŠ¥ì´ ê¸°ëŒ€ë³´ë‹¤ ë‚®ìŒ
ì§„ë‹¨:
1. Feature importance í™•ì¸
   â†’ ìƒìœ„ ë³€ìˆ˜ë“¤ì´ ì˜ˆìƒê³¼ ë‹¤ë¦„
   â†’ 'ê³ ê°ID' ë³€ìˆ˜ê°€ 1ìœ„ (data leakage!)
2. Permutation importanceë¡œ ì¬í™•ì¸
   â†’ Train: ë†’ìŒ, Test: 0 (ê³¼ì í•© í™•ì¸)

í•´ê²°:
- 'ID', 'ë‚ ì§œ' ë“± leakage ë³€ìˆ˜ ì œê±°
- ì¬í•™ìŠµ í›„ ì •ìƒ ì„±ëŠ¥ í™•ë³´
```

---

## 3. êµ¬í˜„

### 3.1 í™˜ê²½ ì„¤ì •

```python
# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import make_classification, make_regression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import warnings
warnings.filterwarnings('ignore')

# ëª¨ë¸
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier, XGBRegressor
from sklearn.linear_model import LogisticRegression, LinearRegression

# Feature importance ë„êµ¬
from sklearn.inspection import permutation_importance
from sklearn.feature_selection import RFE, SelectFromModel
import shap

# ì‹œê°í™”
plt.style.use('seaborn-v0_8-whitegrid')
sns.set_palette("viridis")
%matplotlib inline

# í•œê¸€ í°íŠ¸ (ì„ íƒ)
plt.rcParams['font.family'] = 'AppleGothic'  # Mac
plt.rcParams['axes.unicode_minus'] = False
```

### 3.2 ìƒ˜í”Œ ë°ì´í„° ìƒì„±

```python
def generate_sample_data(n_samples=1000, n_features=20, task='classification'):
    """
    Feature importance ë¶„ì„ìš© ìƒ˜í”Œ ë°ì´í„° ìƒì„±
    
    Parameters:
    -----------
    n_samples : int
    n_features : int
    task : str
        'classification' or 'regression'
    
    Returns:
    --------
    X_train, X_test, y_train, y_test, feature_names
    """
    np.random.seed(42)
    
    if task == 'classification':
        # ë¶„ë¥˜ ë°ì´í„°
        X, y = make_classification(
            n_samples=n_samples,
            n_features=n_features,
            n_informative=10,  # ì‹¤ì œ ìœ ìš©í•œ ë³€ìˆ˜ 10ê°œ
            n_redundant=5,     # ì¤‘ë³µ ë³€ìˆ˜ 5ê°œ
            n_repeated=0,
            n_classes=2,
            random_state=42
        )
    else:
        # íšŒê·€ ë°ì´í„°
        X, y = make_regression(
            n_samples=n_samples,
            n_features=n_features,
            n_informative=10,
            n_targets=1,
            noise=10.0,
            random_state=42
        )
    
    # Feature ì´ë¦„ ìƒì„±
    feature_names = [f'feature_{i+1}' for i in range(n_features)]
    
    # DataFrame ë³€í™˜
    X_df = pd.DataFrame(X, columns=feature_names)
    y_series = pd.Series(y, name='target')
    
    # Train/Test ë¶„í• 
    X_train, X_test, y_train, y_test = train_test_split(
        X_df, y_series, test_size=0.3, random_state=42
    )
    
    print(f"=" * 70)
    print(f"ğŸ“Š ë°ì´í„° ìƒì„± ì™„ë£Œ ({task.upper()})")
    print(f"=" * 70)
    print(f"Train: {X_train.shape[0]} samples Ã— {X_train.shape[1]} features")
    print(f"Test:  {X_test.shape[0]} samples Ã— {X_test.shape[1]} features")
    print(f"\nFeature êµ¬ì„±:")
    print(f"  - Informative: 10ê°œ (ì‹¤ì œ ìœ ìš©)")
    print(f"  - Redundant:   5ê°œ (ì¤‘ë³µ)")
    print(f"  - Random:      5ê°œ (ë…¸ì´ì¦ˆ)")
    
    return X_train, X_test, y_train, y_test, feature_names

# ë¶„ë¥˜ ë°ì´í„° ìƒì„±
X_train, X_test, y_train, y_test, feature_names = generate_sample_data(
    n_samples=1000,
    n_features=20,
    task='classification'
)
```

### 3.3 Random Forest Feature Importance (ë¶ˆìˆœë„ ê¸°ë°˜)

```python
def calculate_tree_importance(X_train, y_train, feature_names, task='classification', top_n=10):
    """
    Tree-based ëª¨ë¸ì˜ feature importance ê³„ì‚°
    
    Parameters:
    -----------
    X_train : DataFrame or array
    y_train : Series or array
    feature_names : list
    task : str
        'classification' or 'regression'
    top_n : int
        ìƒìœ„ ëª‡ ê°œ í‘œì‹œ
    
    Returns:
    --------
    importance_df : DataFrame
    model : trained model
    """
    print(f"\n" + "=" * 70)
    print(f"ğŸŒ² Random Forest Feature Importance (ë¶ˆìˆœë„ ê¸°ë°˜)")
    print(f"=" * 70)
    
    # ëª¨ë¸ í•™ìŠµ
    if task == 'classification':
        model = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            random_state=42,
            n_jobs=-1
        )
    else:
        model = RandomForestRegressor(
            n_estimators=100,
            max_depth=10,
            random_state=42,
            n_jobs=-1
        )
    
    model.fit(X_train, y_train)
    
    # Feature importance ì¶”ì¶œ
    importances = model.feature_importances_
    
    # DataFrame ìƒì„±
    importance_df = pd.DataFrame({
        'feature': feature_names,
        'importance': importances
    }).sort_values('importance', ascending=False).reset_index(drop=True)
    
    # ì •ê·œí™” (í•©=1)
    importance_df['importance_pct'] = (
        importance_df['importance'] / importance_df['importance'].sum() * 100
    )
    
    # ëˆ„ì  ì¤‘ìš”ë„
    importance_df['cumulative_pct'] = importance_df['importance_pct'].cumsum()
    
    # ì¶œë ¥
    print(f"\nTop {top_n} Important Features:")
    print(f"-" * 70)
    print(f"{'Rank':<6} {'Feature':<20} {'Importance':<12} {'%':<8} {'Cumul %'}")
    print(f"-" * 70)
    
    for idx, row in importance_df.head(top_n).iterrows():
        print(f"{idx+1:<6} {row['feature']:<20} {row['importance']:<12.4f} "
              f"{row['importance_pct']:<8.2f} {row['cumulative_pct']:.2f}%")
    
    # ì‹œê°í™”
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))
    
    # ë§‰ëŒ€ ê·¸ë˜í”„
    top_features = importance_df.head(top_n)
    axes[0].barh(range(len(top_features)), top_features['importance'], alpha=0.8)
    axes[0].set_yticks(range(len(top_features)))
    axes[0].set_yticklabels(top_features['feature'])
    axes[0].invert_yaxis()
    axes[0].set_xlabel('Importance', fontsize=11)
    axes[0].set_title(f'Top {top_n} Feature Importance', fontsize=12)
    axes[0].grid(True, alpha=0.3, axis='x')
    
    # ëˆ„ì  ì¤‘ìš”ë„
    axes[1].plot(
        range(1, len(importance_df)+1),
        importance_df['cumulative_pct'],
        marker='o',
        linewidth=2,
        markersize=6
    )
    axes[1].axhline(y=80, color='r', linestyle='--', label='80% threshold')
    axes[1].axhline(y=95, color='orange', linestyle='--', label='95% threshold')
    axes[1].set_xlabel('Number of Features', fontsize=11)
    axes[1].set_ylabel('Cumulative Importance (%)', fontsize=11)
    axes[1].set_title('Cumulative Feature Importance', fontsize=12)
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # 80% ì»¤ë²„í•˜ëŠ” ë³€ìˆ˜ ê°œìˆ˜
    n_80 = (importance_df['cumulative_pct'] >= 80).idxmax() + 1
    print(f"\nğŸ’¡ Insight:")
    print(f"  - ìƒìœ„ {n_80}ê°œ ë³€ìˆ˜ë¡œ 80% ì¤‘ìš”ë„ ì»¤ë²„")
    print(f"  - ë‚˜ë¨¸ì§€ {len(feature_names) - n_80}ê°œ ë³€ìˆ˜ëŠ” ì œê±° ê³ ë ¤")
    
    return importance_df, model

# Random Forest Importance ê³„ì‚°
rf_importance, rf_model = calculate_tree_importance(
    X_train, y_train, feature_names, task='classification', top_n=15
)
```

### 3.4 Permutation Importance (ìˆœì—´ ì¤‘ìš”ë„)

```python
def calculate_permutation_importance(model, X, y, feature_names, n_repeats=10, top_n=10):
    """
    Permutation Importance ê³„ì‚°
    
    Parameters:
    -----------
    model : trained model
    X : DataFrame or array (ë³´í†µ test set ì‚¬ìš©)
    y : Series or array
    feature_names : list
    n_repeats : int
        ì…”í”Œ ë°˜ë³µ íšŸìˆ˜
    top_n : int
    
    Returns:
    --------
    perm_importance_df : DataFrame
    """
    print(f"\n" + "=" * 70)
    print(f"ğŸ”„ Permutation Importance (ìˆœì—´ ì¤‘ìš”ë„)")
    print(f"=" * 70)
    print(f"ê³„ì‚° ì¤‘... (n_repeats={n_repeats})")
    
    # Permutation importance ê³„ì‚°
    result = permutation_importance(
        model, X, y,
        n_repeats=n_repeats,
        random_state=42,
        n_jobs=-1
    )
    
    # DataFrame ìƒì„±
    perm_importance_df = pd.DataFrame({
        'feature': feature_names,
        'importance_mean': result.importances_mean,
        'importance_std': result.importances_std
    }).sort_values('importance_mean', ascending=False).reset_index(drop=True)
    
    # ì¶œë ¥
    print(f"\nTop {top_n} Important Features:")
    print(f"-" * 70)
    print(f"{'Rank':<6} {'Feature':<20} {'Mean':<12} {'Std':<12}")
    print(f"-" * 70)
    
    for idx, row in perm_importance_df.head(top_n).iterrows():
        print(f"{idx+1:<6} {row['feature']:<20} {row['importance_mean']:<12.4f} "
              f"{row['importance_std']:<12.4f}")
    
    # ì‹œê°í™”
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))
    
    # ì—ëŸ¬ë°” í¬í•¨ ë§‰ëŒ€ ê·¸ë˜í”„
    top_features = perm_importance_df.head(top_n)
    axes[0].barh(
        range(len(top_features)),
        top_features['importance_mean'],
        xerr=top_features['importance_std'],
        alpha=0.8,
        capsize=5
    )
    axes[0].set_yticks(range(len(top_features)))
    axes[0].set_yticklabels(top_features['feature'])
    axes[0].invert_yaxis()
    axes[0].set_xlabel('Permutation Importance', fontsize=11)
    axes[0].set_title(f'Top {top_n} Permutation Importance', fontsize=12)
    axes[0].grid(True, alpha=0.3, axis='x')
    
    # ë°•ìŠ¤í”Œë¡¯ (ìƒìœ„ 5ê°œ)
    top_5 = perm_importance_df.head(5)['feature'].tolist()
    result_subset = permutation_importance(
        model, X, y,
        n_repeats=30,  # ë” ë§ì€ ë°˜ë³µìœ¼ë¡œ ë¶„í¬ í™•ì¸
        random_state=42
    )
    
    data_for_box = []
    for i, feat in enumerate(top_5):
        feat_idx = feature_names.index(feat)
        data_for_box.append(result_subset.importances[feat_idx])
    
    axes[1].boxplot(data_for_box, labels=top_5)
    axes[1].set_ylabel('Permutation Importance', fontsize=11)
    axes[1].set_title('Top 5 Features Distribution (n=30)', fontsize=12)
    axes[1].tick_params(axis='x', rotation=45)
    axes[1].grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    plt.show()
    
    print(f"\nğŸ’¡ Insight:")
    print(f"  - Stdê°€ í° ë³€ìˆ˜: ë¶ˆì•ˆì • (ë°ì´í„° ì˜ì¡´ì )")
    print(f"  - Stdê°€ ì‘ì€ ë³€ìˆ˜: ì•ˆì •ì  ì¤‘ìš”ë„")
    print(f"  - Mean â‰ˆ 0: ì˜ˆì¸¡ì— ê¸°ì—¬ ì—†ìŒ (ì œê±° ê³ ë ¤)")
    
    return perm_importance_df

# Permutation Importance ê³„ì‚° (Test set ì‚¬ìš©)
perm_importance = calculate_permutation_importance(
    rf_model, X_test, y_test, feature_names, n_repeats=10, top_n=15
)
```

### 3.5 Tree Importance vs Permutation Importance ë¹„êµ

```python
def compare_importance_methods(rf_importance, perm_importance, top_n=15):
    """
    ë‘ ë°©ë²•ì˜ feature importance ë¹„êµ
    """
    print(f"\n" + "=" * 70)
    print(f"âš–ï¸  Feature Importance ë°©ë²• ë¹„êµ")
    print(f"=" * 70)
    
    # ë°ì´í„° ë³‘í•©
    comparison = rf_importance[['feature', 'importance']].copy()
    comparison = comparison.rename(columns={'importance': 'RF_importance'})
    comparison = comparison.merge(
        perm_importance[['feature', 'importance_mean']],
        on='feature'
    ).rename(columns={'importance_mean': 'Perm_importance'})
    
    # ì •ê·œí™” (0-1)
    comparison['RF_norm'] = (
        comparison['RF_importance'] / comparison['RF_importance'].max()
    )
    comparison['Perm_norm'] = (
        comparison['Perm_importance'] / comparison['Perm_importance'].max()
    )
    
    # ì°¨ì´ ê³„ì‚°
    comparison['difference'] = abs(
        comparison['RF_norm'] - comparison['Perm_norm']
    )
    
    # ì •ë ¬ (RF ê¸°ì¤€)
    comparison = comparison.sort_values('RF_importance', ascending=False)
    
    # ì¶œë ¥
    print(f"\nTop {top_n} Features ë¹„êµ:")
    print(f"-" * 70)
    print(f"{'Feature':<20} {'RF':<10} {'Perm':<10} {'Diff':<10} {'Status'}")
    print(f"-" * 70)
    
    for idx, row in comparison.head(top_n).iterrows():
        diff = row['difference']
        if diff < 0.2:
            status = "âœ… ì¼ì¹˜"
        elif diff < 0.5:
            status = "âš ï¸  ì°¨ì´"
        else:
            status = "ğŸš¨ ë¶ˆì¼ì¹˜"
        
        print(f"{row['feature']:<20} {row['RF_norm']:<10.3f} "
              f"{row['Perm_norm']:<10.3f} {diff:<10.3f} {status}")
    
    # ì‹œê°í™”
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))
    
    # ì‚°ì ë„
    axes[0].scatter(
        comparison['RF_norm'],
        comparison['Perm_norm'],
        s=100,
        alpha=0.6
    )
    
    # ëŒ€ê°ì„  (ì™„ë²½í•œ ì¼ì¹˜)
    max_val = max(comparison['RF_norm'].max(), comparison['Perm_norm'].max())
    axes[0].plot([0, max_val], [0, max_val], 'r--', linewidth=2, label='Perfect Agreement')
    
    # ë ˆì´ë¸” (ìƒìœ„ 5ê°œ)
    top_5 = comparison.head(5)
    for idx, row in top_5.iterrows():
        axes[0].annotate(
            row['feature'],
            (row['RF_norm'], row['Perm_norm']),
            fontsize=9,
            alpha=0.7
        )
    
    axes[0].set_xlabel('RF Importance (normalized)', fontsize=11)
    axes[0].set_ylabel('Permutation Importance (normalized)', fontsize=11)
    axes[0].set_title('RF vs Permutation Importance', fontsize=12)
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # ë§‰ëŒ€ ê·¸ë˜í”„ (ìƒìœ„ 10ê°œ)
    top_10 = comparison.head(10)
    x = np.arange(len(top_10))
    width = 0.35
    
    axes[1].barh(x - width/2, top_10['RF_norm'], width, label='RF', alpha=0.8)
    axes[1].barh(x + width/2, top_10['Perm_norm'], width, label='Perm', alpha=0.8)
    axes[1].set_yticks(x)
    axes[1].set_yticklabels(top_10['feature'])
    axes[1].invert_yaxis()
    axes[1].set_xlabel('Normalized Importance', fontsize=11)
    axes[1].set_title('Top 10 Features: RF vs Permutation', fontsize=12)
    axes[1].legend()
    axes[1].grid(True, alpha=0.3, axis='x')
    
    plt.tight_layout()
    plt.show()
    
    # ë¶ˆì¼ì¹˜ ë³€ìˆ˜ ê²½ê³ 
    high_diff = comparison[comparison['difference'] > 0.5]
    if len(high_diff) > 0:
        print(f"\nâš ï¸  í° ì°¨ì´ë¥¼ ë³´ì´ëŠ” ë³€ìˆ˜ ({len(high_diff)}ê°œ):")
        for idx, row in high_diff.iterrows():
            print(f"  - {row['feature']}: RF={row['RF_norm']:.3f}, Perm={row['Perm_norm']:.3f}")
        print(f"\nğŸ’¡ ì›ì¸:")
        print(f"  - ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„ (RFëŠ” ì¤‘ë³µ ë³€ìˆ˜ì— importance ë¶„ì‚°)")
        print(f"  - Train/Test ë¶„í¬ ì°¨ì´")
        print(f"  - ê³¼ì í•© (RFëŠ” ë†’ì§€ë§Œ Permì€ ë‚®ìŒ)")
    
    return comparison

# ë¹„êµ ì‹¤í–‰
comparison_df = compare_importance_methods(rf_importance, perm_importance, top_n=15)
```

### 3.6 SHAP Values (ê²Œì„ ì´ë¡  ê¸°ë°˜)

```python
def calculate_shap_importance(model, X_train, X_test, feature_names, max_display=15):
    """
    SHAP (SHapley Additive exPlanations) ê°’ ê³„ì‚°
    
    Parameters:
    -----------
    model : trained model
    X_train : training data (for TreeExplainer background)
    X_test : test data (for explanation)
    feature_names : list
    max_display : int
    
    Returns:
    --------
    shap_values : array
    explainer : SHAP explainer
    """
    print(f"\n" + "=" * 70)
    print(f"ğŸ® SHAP Values (ê²Œì„ ì´ë¡  ê¸°ë°˜)")
    print(f"=" * 70)
    print(f"ê³„ì‚° ì¤‘... (ì‹œê°„ ì†Œìš” ê°€ëŠ¥)")
    
    # SHAP Explainer ìƒì„±
    # Tree ëª¨ë¸ìš© (ë¹ ë¦„)
    explainer = shap.TreeExplainer(model)
    shap_values = explainer.shap_values(X_test)
    
    # ë¶„ë¥˜ ëª¨ë¸ì˜ ê²½ìš° í´ë˜ìŠ¤ë³„ shap_values (í´ë˜ìŠ¤ 1ë§Œ ì‚¬ìš©)
    if isinstance(shap_values, list):
        shap_values = shap_values[1]
    
    print(f"âœ… SHAP ê³„ì‚° ì™„ë£Œ")
    
    # Summary Plot (ì „ì²´ ë³€ìˆ˜)
    plt.figure(figsize=(12, 8))
    shap.summary_plot(
        shap_values,
        X_test,
        feature_names=feature_names,
        max_display=max_display,
        show=False
    )
    plt.title('SHAP Summary Plot (Feature Importance + Direction)', fontsize=14, pad=20)
    plt.tight_layout()
    plt.show()
    
    # Bar Plot (í‰ê·  ì ˆëŒ“ê°’)
    plt.figure(figsize=(10, 8))
    shap.summary_plot(
        shap_values,
        X_test,
        feature_names=feature_names,
        plot_type='bar',
        max_display=max_display,
        show=False
    )
    plt.title('SHAP Feature Importance (Mean |SHAP|)', fontsize=14, pad=20)
    plt.tight_layout()
    plt.show()
    
    # Feature importance (mean absolute SHAP)
    shap_importance = np.abs(shap_values).mean(axis=0)
    shap_df = pd.DataFrame({
        'feature': feature_names,
        'shap_importance': shap_importance
    }).sort_values('shap_importance', ascending=False).reset_index(drop=True)
    
    print(f"\nTop {max_display} SHAP Important Features:")
    print(f"-" * 70)
    print(f"{'Rank':<6} {'Feature':<20} {'Mean |SHAP|':<15}")
    print(f"-" * 70)
    
    for idx, row in shap_df.head(max_display).iterrows():
        print(f"{idx+1:<6} {row['feature']:<20} {row['shap_importance']:<15.4f}")
    
    print(f"\nğŸ’¡ SHAP í•´ì„:")
    print(f"  - ì ì˜ ìƒ‰ìƒ: Feature ê°’ (ë¹¨ê°•=ë†’ìŒ, íŒŒë‘=ë‚®ìŒ)")
    print(f"  - Xì¶• ìœ„ì¹˜: SHAP ê°’ (ì–‘ìˆ˜=ì˜ˆì¸¡â†‘, ìŒìˆ˜=ì˜ˆì¸¡â†“)")
    print(f"  - ì˜ˆì‹œ: ë¹¨ê°„ ì ì´ ì˜¤ë¥¸ìª½ â†’ ê°’ì´ ë†’ìœ¼ë©´ ì˜ˆì¸¡â†‘")
    print(f"  - ì˜ˆì‹œ: íŒŒë€ ì ì´ ì™¼ìª½ â†’ ê°’ì´ ë‚®ìœ¼ë©´ ì˜ˆì¸¡â†“")
    
    return shap_values, explainer, shap_df

# SHAP ê³„ì‚° (ìƒ˜í”Œ í¬ê¸° ì œí•œìœ¼ë¡œ ì†ë„ í–¥ìƒ)
X_test_sample = X_test.sample(min(300, len(X_test)), random_state=42)
shap_values, explainer, shap_df = calculate_shap_importance(
    rf_model,
    X_train,
    X_test_sample,
    feature_names,
    max_display=15
)
```

### 3.7 SHAP ê°œë³„ ì˜ˆì¸¡ ì„¤ëª…

```python
def explain_single_prediction(explainer, X_test, feature_names, sample_idx=0):
    """
    SHAPì„ ì‚¬ìš©í•œ ê°œë³„ ì˜ˆì¸¡ ì„¤ëª…
    
    Parameters:
    -----------
    explainer : SHAP explainer
    X_test : test data
    feature_names : list
    sample_idx : int
        ì„¤ëª…í•  ìƒ˜í”Œ ì¸ë±ìŠ¤
    """
    print(f"\n" + "=" * 70)
    print(f"ğŸ” ê°œë³„ ì˜ˆì¸¡ ì„¤ëª… (Sample #{sample_idx})")
    print(f"=" * 70)
    
    # ìƒ˜í”Œ ì„ íƒ
    X_sample = X_test.iloc[[sample_idx]]
    
    # SHAP ê°’ ê³„ì‚°
    shap_values = explainer.shap_values(X_sample)
    if isinstance(shap_values, list):
        shap_values = shap_values[1]
    
    # Waterfall Plot (ê°œë³„ ì˜ˆì¸¡ì˜ ê¸°ì—¬ë„)
    plt.figure(figsize=(10, 8))
    shap.waterfall_plot(
        shap.Explanation(
            values=shap_values[0],
            base_values=explainer.expected_value[1] if isinstance(explainer.expected_value, list) else explainer.expected_value,
            data=X_sample.values[0],
            feature_names=feature_names
        ),
        max_display=15,
        show=False
    )
    plt.title(f'SHAP Waterfall Plot (Sample #{sample_idx})', fontsize=14, pad=20)
    plt.tight_layout()
    plt.show()
    
    # Force Plot (ë‹¨ì¼ ìƒ˜í”Œ)
    plt.figure(figsize=(16, 3))
    shap.force_plot(
        explainer.expected_value[1] if isinstance(explainer.expected_value, list) else explainer.expected_value,
        shap_values[0],
        X_sample.values[0],
        feature_names=feature_names,
        matplotlib=True,
        show=False
    )
    plt.title(f'SHAP Force Plot (Sample #{sample_idx})', fontsize=14, pad=20)
    plt.tight_layout()
    plt.show()
    
    # Feature ê°’ ë° ê¸°ì—¬ë„ ì¶œë ¥
    contributions = pd.DataFrame({
        'feature': feature_names,
        'value': X_sample.values[0],
        'shap_value': shap_values[0]
    }).sort_values('shap_value', key=abs, ascending=False)
    
    print(f"\nìƒ˜í”Œ #{sample_idx}ì˜ Feature ê¸°ì—¬ë„:")
    print(f"-" * 70)
    print(f"{'Feature':<20} {'Value':<15} {'SHAP Value':<15} {'Effect'}")
    print(f"-" * 70)
    
    for idx, row in contributions.head(10).iterrows():
        effect = "ì˜ˆì¸¡â†‘" if row['shap_value'] > 0 else "ì˜ˆì¸¡â†“"
        print(f"{row['feature']:<20} {row['value']:<15.3f} "
              f"{row['shap_value']:<15.4f} {effect}")
    
    print(f"\nğŸ’¡ í•´ì„:")
    print(f"  - Base value: ëª¨ë“  ìƒ˜í”Œì˜ í‰ê·  ì˜ˆì¸¡ê°’")
    print(f"  - SHAP value > 0: í•´ë‹¹ featureê°€ ì˜ˆì¸¡ì„ ì¦ê°€ì‹œí‚´")
    print(f"  - SHAP value < 0: í•´ë‹¹ featureê°€ ì˜ˆì¸¡ì„ ê°ì†Œì‹œí‚´")
    print(f"  - ìµœì¢… ì˜ˆì¸¡ = Base + Î£(SHAP values)")

# ê°œë³„ ì˜ˆì¸¡ ì„¤ëª… (3ê°œ ìƒ˜í”Œ)
for i in [0, 10, 50]:
    explain_single_prediction(explainer, X_test_sample, feature_names, sample_idx=i)
```

### 3.8 Recursive Feature Elimination (RFE)

```python
def perform_rfe(X_train, y_train, feature_names, n_features_to_select=10, task='classification'):
    """
    RFE (Recursive Feature Elimination)ë¡œ ìµœì  ë³€ìˆ˜ ì„ íƒ
    
    Parameters:
    -----------
    X_train, y_train : training data
    feature_names : list
    n_features_to_select : int
        ì„ íƒí•  ë³€ìˆ˜ ê°œìˆ˜
    task : str
    
    Returns:
    --------
    selected_features : list
    rfe : RFE object
    """
    print(f"\n" + "=" * 70)
    print(f"ğŸ”„ Recursive Feature Elimination (RFE)")
    print(f"=" * 70)
    print(f"ëª©í‘œ: {n_features_to_select}ê°œ ë³€ìˆ˜ ì„ íƒ")
    
    # ëª¨ë¸ ì„ íƒ
    if task == 'classification':
        estimator = RandomForestClassifier(n_estimators=50, random_state=42)
    else:
        estimator = RandomForestRegressor(n_estimators=50, random_state=42)
    
    # RFE ì‹¤í–‰
    rfe = RFE(
        estimator=estimator,
        n_features_to_select=n_features_to_select,
        step=1  # í•œ ë²ˆì— ì œê±°í•  ë³€ìˆ˜ ê°œìˆ˜
    )
    
    print(f"RFE ì‹¤í–‰ ì¤‘...")
    rfe.fit(X_train, y_train)
    print(f"âœ… ì™„ë£Œ")
    
    # ì„ íƒëœ ë³€ìˆ˜
    selected_mask = rfe.support_
    selected_features = [f for f, selected in zip(feature_names, selected_mask) if selected]
    
    # ìˆœìœ„
    ranking_df = pd.DataFrame({
        'feature': feature_names,
        'ranking': rfe.ranking_,
        'selected': selected_mask
    }).sort_values('ranking')
    
    print(f"\nì„ íƒëœ ë³€ìˆ˜ ({len(selected_features)}ê°œ):")
    print(f"-" * 70)
    for feat in selected_features:
        print(f"  âœ… {feat}")
    
    print(f"\nì œê±°ëœ ë³€ìˆ˜ ({len(feature_names) - len(selected_features)}ê°œ):")
    removed = ranking_df[ranking_df['selected'] == False].head(10)
    for idx, row in removed.iterrows():
        print(f"  âŒ {row['feature']} (rank: {row['ranking']})")
    
    # ì‹œê°í™”
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))
    
    # ìˆœìœ„ ë§‰ëŒ€ ê·¸ë˜í”„
    colors = ['green' if sel else 'red' for sel in ranking_df['selected']]
    axes[0].barh(range(len(ranking_df)), ranking_df['ranking'], color=colors, alpha=0.7)
    axes[0].set_yticks(range(len(ranking_df)))
    axes[0].set_yticklabels(ranking_df['feature'])
    axes[0].invert_yaxis()
    axes[0].set_xlabel('Ranking (1=Best)', fontsize=11)
    axes[0].set_title('RFE Feature Ranking', fontsize=12)
    axes[0].axvline(x=n_features_to_select, color='blue', linestyle='--', linewidth=2, label='Cutoff')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3, axis='x')
    
    # ì„ íƒ/ì œê±° íŒŒì´ ì°¨íŠ¸
    selected_count = selected_mask.sum()
    removed_count = len(feature_names) - selected_count
    axes[1].pie(
        [selected_count, removed_count],
        labels=['Selected', 'Removed'],
        autopct='%1.1f%%',
        colors=['green', 'red'],
        startangle=90
    )
    axes[1].set_title(f'Feature Selection Result\n({selected_count} / {len(feature_names)})', fontsize=12)
    
    plt.tight_layout()
    plt.show()
    
    return selected_features, rfe, ranking_df

# RFE ì‹¤í–‰
selected_features, rfe_model, rfe_ranking = perform_rfe(
    X_train, y_train, feature_names, n_features_to_select=10, task='classification'
)
```

### 3.9 ëª¨ë“  ë°©ë²• ì¢…í•© ë¹„êµ

```python
def compare_all_methods(rf_importance, perm_importance, shap_df, rfe_ranking, top_n=15):
    """
    ëª¨ë“  feature importance ë°©ë²• ì¢…í•© ë¹„êµ
    """
    print(f"\n" + "=" * 70)
    print(f"ğŸ† Feature Importance ì¢…í•© ë¹„êµ")
    print(f"=" * 70)
    
    # ë°ì´í„° ë³‘í•©
    comparison = rf_importance[['feature', 'importance']].copy()
    comparison = comparison.rename(columns={'importance': 'RF'})
    
    comparison = comparison.merge(
        perm_importance[['feature', 'importance_mean']],
        on='feature'
    ).rename(columns={'importance_mean': 'Permutation'})
    
    comparison = comparison.merge(
        shap_df[['feature', 'shap_importance']],
        on='feature'
    ).rename(columns={'shap_importance': 'SHAP'})
    
    comparison = comparison.merge(
        rfe_ranking[['feature', 'ranking', 'selected']],
        on='feature'
    )
    
    # ì •ê·œí™” (0-1)
    for col in ['RF', 'Permutation', 'SHAP']:
        comparison[f'{col}_norm'] = comparison[col] / comparison[col].max()
    
    # í‰ê·  ìˆœìœ„ ê³„ì‚°
    comparison['avg_importance'] = (
        comparison['RF_norm'] + comparison['Permutation_norm'] + comparison['SHAP_norm']
    ) / 3
    
    comparison = comparison.sort_values('avg_importance', ascending=False)
    
    # ì¶œë ¥
    print(f"\nTop {top_n} Features (ì¢…í•©):")
    print(f"-" * 90)
    print(f"{'Rank':<6} {'Feature':<18} {'RF':<8} {'Perm':<8} {'SHAP':<8} {'Avg':<8} {'RFE'}")
    print(f"-" * 90)
    
    for idx, row in comparison.head(top_n).iterrows():
        rfe_status = "âœ…" if row['selected'] else "âŒ"
        print(f"{idx+1:<6} {row['feature']:<18} {row['RF_norm']:<8.3f} "
              f"{row['Permutation_norm']:<8.3f} {row['SHAP_norm']:<8.3f} "
              f"{row['avg_importance']:<8.3f} {rfe_status}")
    
    # ì‹œê°í™”: íˆíŠ¸ë§µ
    plt.figure(figsize=(12, 10))
    
    top_features = comparison.head(top_n)
    heatmap_data = top_features[['RF_norm', 'Permutation_norm', 'SHAP_norm']].T
    heatmap_data.columns = top_features['feature']
    
    sns.heatmap(
        heatmap_data,
        annot=True,
        fmt='.2f',
        cmap='YlOrRd',
        linewidths=1,
        cbar_kws={'label': 'Normalized Importance'}
    )
    
    plt.ylabel('Method', fontsize=12)
    plt.xlabel('Feature', fontsize=12)
    plt.title(f'Feature Importance Heatmap (Top {top_n})', fontsize=14, pad=20)
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.show()
    
    # ì¼ì¹˜ë„ ë¶„ì„
    print(f"\nğŸ“Š ë°©ë²• ê°„ ì¼ì¹˜ë„:")
    print(f"-" * 70)
    
    # ìƒìœ„ 10ê°œ ë³€ìˆ˜ê°€ ê²¹ì¹˜ëŠ” ì •ë„
    top_10_rf = set(rf_importance.head(10)['feature'])
    top_10_perm = set(perm_importance.head(10)['feature'])
    top_10_shap = set(shap_df.head(10)['feature'])
    top_10_rfe = set(rfe_ranking[rfe_ranking['selected']].head(10)['feature'])
    
    all_methods = top_10_rf & top_10_perm & top_10_shap & top_10_rfe
    print(f"ëª¨ë“  ë°©ë²•ì´ ì¼ì¹˜í•˜ëŠ” ë³€ìˆ˜ ({len(all_methods)}ê°œ):")
    for feat in all_methods:
        print(f"  âœ… {feat}")
    
    return comparison

# ì¢…í•© ë¹„êµ
final_comparison = compare_all_methods(
    rf_importance, perm_importance, shap_df, rfe_ranking, top_n=15
)
```

### 3.10 ìµœì¢… Feature Selection ê¶Œì¥

```python
def recommend_final_features(comparison, min_methods=2, top_n=15):
    """
    ìµœì¢… feature selection ê¶Œì¥
    
    Parameters:
    -----------
    comparison : DataFrame
        ì¢…í•© ë¹„êµ ê²°ê³¼
    min_methods : int
        ìµœì†Œ ëª‡ ê°œ ë°©ë²•ì—ì„œ ìƒìœ„ê¶Œì´ì–´ì•¼ í•˜ëŠ”ì§€
    top_n : int
    """
    print(f"\n" + "=" * 70)
    print(f"âœ… ìµœì¢… Feature Selection ê¶Œì¥")
    print(f"=" * 70)
    
    # ê° ë°©ë²•ì—ì„œ ìƒìœ„ top_nì— í¬í•¨ë˜ëŠ”ì§€ í™•ì¸
    comparison['RF_top'] = comparison['RF_norm'].rank(ascending=False) <= top_n
    comparison['Perm_top'] = comparison['Permutation_norm'].rank(ascending=False) <= top_n
    comparison['SHAP_top'] = comparison['SHAP_norm'].rank(ascending=False) <= top_n
    
    # ëª‡ ê°œ ë°©ë²•ì—ì„œ ìƒìœ„ê¶Œì¸ì§€
    comparison['n_methods_top'] = (
        comparison['RF_top'].astype(int) +
        comparison['Perm_top'].astype(int) +
        comparison['SHAP_top'].astype(int)
    )
    
    # ê¶Œì¥ ë³€ìˆ˜
    recommended = comparison[comparison['n_methods_top'] >= min_methods].sort_values(
        'avg_importance', ascending=False
    )
    
    print(f"\nê¶Œì¥ ê¸°ì¤€: ìµœì†Œ {min_methods}ê°œ ë°©ë²•ì—ì„œ Top {top_n}")
    print(f"ê¶Œì¥ ë³€ìˆ˜: {len(recommended)}ê°œ")
    print(f"-" * 70)
    print(f"{'Feature':<20} {'Avg':<8} {'Methods':<10} {'RFE'}")
    print(f"-" * 70)
    
    for idx, row in recommended.iterrows():
        rfe_status = "âœ…" if row['selected'] else "âŒ"
        methods_str = f"{row['n_methods_top']}/3"
        print(f"{row['feature']:<20} {row['avg_importance']:<8.3f} "
              f"{methods_str:<10} {rfe_status}")
    
    # ì œê±° ê¶Œì¥ ë³€ìˆ˜
    not_recommended = comparison[comparison['n_methods_top'] < min_methods].sort_values(
        'avg_importance', ascending=False
    ).head(10)
    
    print(f"\nì œê±° ê³ ë ¤ ë³€ìˆ˜ (Top 10):")
    print(f"-" * 70)
    for idx, row in not_recommended.iterrows():
        print(f"  âŒ {row['feature']} (í‰ê·  ì¤‘ìš”ë„: {row['avg_importance']:.3f})")
    
    # ìµœì¢… ê¶Œì¥
    print(f"\nğŸ’¡ ìµœì¢… ê¶Œì¥ì‚¬í•­:")
    print(f"=" * 70)
    print(f"1. ê°•ë ¥ ê¶Œì¥ (ëª¨ë“  ë°©ë²• ì¼ì¹˜): {(comparison['n_methods_top'] == 3).sum()}ê°œ")
    print(f"2. ê¶Œì¥ (2ê°œ ì´ìƒ ë°©ë²•): {(comparison['n_methods_top'] >= 2).sum()}ê°œ")
    print(f"3. ì œê±° ê³ ë ¤ (1ê°œ ì´í•˜ ë°©ë²•): {(comparison['n_methods_top'] <= 1).sum()}ê°œ")
    
    print(f"\nâœ… Feature Selection ì™„ë£Œ!")
    print(f"   ì›ë³¸: {len(comparison)}ê°œ â†’ ì„ íƒ: {len(recommended)}ê°œ")
    print(f"   ì¶•ì†Œìœ¨: {(1 - len(recommended)/len(comparison))*100:.1f}%")
    
    return recommended['feature'].tolist()

# ìµœì¢… ê¶Œì¥ ë³€ìˆ˜ ì„ íƒ
final_features = recommend_final_features(final_comparison, min_methods=2, top_n=10)

print(f"\nğŸ¯ ìµœì¢… ì„ íƒ ë³€ìˆ˜ ëª©ë¡:")
for i, feat in enumerate(final_features, 1):
    print(f"{i}. {feat}")
```

---

## 4. ì˜ˆì‹œ

### 4.1 ì‹¤ì „ ì˜ˆì œ: ì‹ ìš© ìœ„í—˜ í‰ê°€ ëª¨ë¸

```python
print("=" * 70)
print("ğŸ“ˆ ë¹„ì¦ˆë‹ˆìŠ¤ ì‹œë‚˜ë¦¬ì˜¤: ì‹ ìš© ìœ„í—˜ í‰ê°€ ëª¨ë¸")
print("=" * 70)

print("\nëª©í‘œ:")
print("- ê³ ê°ì˜ ì‹ ìš© ìœ„í—˜ ì˜ˆì¸¡")
print("- 100ê°œ featuresì—ì„œ í•µì‹¬ ë³€ìˆ˜ ì„ íƒ")
print("- ê·œì œ ê¸°ê´€ì— ëª¨ë¸ ì„¤ëª… í•„ìš” (Explainable AI)")

print("\nğŸ”„ ë¶„ì„ í”„ë¡œì„¸ìŠ¤:")
print("-" * 70)
print("1ë‹¨ê³„: Random Forestë¡œ ë¹ ë¥¸ screening")
print("   â†’ ìƒìœ„ 30ê°œ features ì„ íƒ")
print("\n2ë‹¨ê³„: Permutation Importanceë¡œ ê²€ì¦")
print("   â†’ Test setì—ì„œ ì‹¤ì œ ê¸°ì—¬ë„ í™•ì¸")
print("   â†’ 5ê°œ features ì¶”ê°€ ì œê±° (ê³¼ì í•© ë°©ì§€)")
print("\n3ë‹¨ê³„: SHAPìœ¼ë¡œ í•´ì„")
print("   â†’ ê° ë³€ìˆ˜ê°€ ìœ„í—˜ë„ì— ë¯¸ì¹˜ëŠ” ë°©í–¥ í™•ì¸")
print("   â†’ 'ì—°ì²´ íšŸìˆ˜â†‘ â†’ ìœ„í—˜ë„â†‘' ë“± ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ê²€ì¦")
print("\n4ë‹¨ê³„: RFEë¡œ ìµœì¢… ì„ íƒ")
print("   â†’ 15ê°œ í•µì‹¬ features í™•ì •")

print("\nâœ… ê²°ê³¼:")
print("-" * 70)
print("ìµœì¢… 15ê°œ Features:")
print("  1. ì—°ì²´ ì´ë ¥ (SHAP: 0.45)")
print("  2. ì‹ ìš© ì ìˆ˜ (SHAP: 0.38)")
print("  3. ë¶€ì±„ ë¹„ìœ¨ (SHAP: 0.32)")
print("  4. ì†Œë“ ìˆ˜ì¤€ (SHAP: 0.28)")
print("  5. ê³ ìš© ê¸°ê°„ (SHAP: 0.21)")
print("  ... (ì´í•˜ 10ê°œ)")

print("\në¹„ì¦ˆë‹ˆìŠ¤ íš¨ê³¼:")
print("  - ëª¨ë¸ ì •í™•ë„: 88% (100ê°œ features) â†’ 89% (15ê°œ features)")
print("  - í•™ìŠµ ì‹œê°„: 45ë¶„ â†’ 3ë¶„ (15ë°° ê°œì„ )")
print("  - ê·œì œ ê¸°ê´€ ìŠ¹ì¸ íšë“ (SHAP ì„¤ëª… ì œê³µ)")
```

### 4.2 ì…ì¶œë ¥ ì˜ˆì‹œ

```python
# ì…ë ¥: Feature ëª©ë¡
print("\nğŸ“¥ ì…ë ¥: ì›ë³¸ Features")
print(f"ì´ {len(feature_names)}ê°œ features")
print(feature_names[:10])

# ì¶œë ¥ 1: RF Importance
print("\nğŸ“¤ ì¶œë ¥ 1: Random Forest Importance")
print(rf_importance.head(10))

# ì¶œë ¥ 2: Permutation Importance
print("\nğŸ“¤ ì¶œë ¥ 2: Permutation Importance")
print(perm_importance.head(10))

# ì¶œë ¥ 3: SHAP Importance
print("\nğŸ“¤ ì¶œë ¥ 3: SHAP Importance")
print(shap_df.head(10))

# ì¶œë ¥ 4: ìµœì¢… ì„ íƒ Features
print("\nğŸ“¤ ì¶œë ¥ 4: ìµœì¢… ì„ íƒ Features")
print(final_features)
```

---

## 5. ì—ì´ì „íŠ¸ ë§¤í•‘

### 5.1 ë‹´ë‹¹ ì—ì´ì „íŠ¸

| ì‘ì—… | Primary Agent | Supporting Agents |
|------|--------------|-------------------|
| Tree-based Importance | `feature-engineering-specialist` | `ml-modeling-specialist` |
| Permutation Importance | `feature-engineering-specialist` | `data-scientist` |
| SHAP ë¶„ì„ | `feature-engineering-specialist` | `ml-modeling-specialist` |
| RFE ì‹¤í–‰ | `feature-engineering-specialist` | - |
| Feature selection ì „ëµ | `feature-engineering-specialist` | `data-scientist` |
| ë¹„ì¦ˆë‹ˆìŠ¤ í•´ì„ | `data-scientist` | `feature-engineering-specialist` |

### 5.2 ê´€ë ¨ ìŠ¤í‚¬

**Scientific Skills**:
- `scikit-learn` (feature_importances_, permutation_importance, RFE)
- `xgboost` (XGBoost feature importance)
- `shap` (SHAP values)
- `matplotlib`, `seaborn` (ì‹œê°í™”)
- `pandas`, `numpy` (ë°ì´í„° ì²˜ë¦¬)

---

## 6. í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬

### 6.1 í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬

```bash
# ë¨¸ì‹ ëŸ¬ë‹
pip install scikit-learn==1.4.0
pip install xgboost==2.0.3

# SHAP
pip install shap==0.44.1

# ë°ì´í„° ì²˜ë¦¬
pip install pandas==2.2.0
pip install numpy==1.26.3

# ì‹œê°í™”
pip install matplotlib==3.8.2
pip install seaborn==0.13.1
```

### 6.2 ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „ í™•ì¸

```python
import sklearn
import xgboost
import shap
import pandas as pd
import numpy as np

print("ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „:")
print(f"scikit-learn: {sklearn.__version__}")
print(f"xgboost: {xgboost.__version__}")
print(f"shap: {shap.__version__}")
print(f"pandas: {pd.__version__}")
print(f"numpy: {np.__version__}")
```

---

## 7. ì²´í¬í¬ì¸íŠ¸

### 7.1 ë¶„ì„ ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] **ë°ì´í„° ì¤€ë¹„**
  - [ ] Train/Test split ì™„ë£Œ
  - [ ] ê²°ì¸¡ê°’ ì²˜ë¦¬
  - [ ] ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©

- [ ] **ëª¨ë¸ ì„ íƒ**
  - [ ] Tree ëª¨ë¸ (RF, XGBoost): ë¹ ë¥¸ importance
  - [ ] ì„ í˜• ëª¨ë¸: coefficient ê¸°ë°˜

### 7.2 ë¶„ì„ ì¤‘ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] **ì—¬ëŸ¬ ë°©ë²• ì‚¬ìš©**
  - [ ] Tree importance (ë¹ ë¥¸ íƒìƒ‰)
  - [ ] Permutation (ê²€ì¦)
  - [ ] SHAP (í•´ì„)

- [ ] **ì¼ê´€ì„± í™•ì¸**
  - [ ] ë°©ë²• ê°„ ìƒìœ„ ë³€ìˆ˜ ì¼ì¹˜í•˜ëŠ”ê°€?
  - [ ] ë¶ˆì¼ì¹˜ ì‹œ ì›ì¸ ë¶„ì„

### 7.3 ë¶„ì„ í›„ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] **Feature Selection**
  - [ ] ìµœì¢… ë³€ìˆ˜ ëª©ë¡ í™•ì •
  - [ ] ì œê±°ëœ ë³€ìˆ˜ ë¬¸ì„œí™”
  - [ ] ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ê²€ì¦

- [ ] **ëª¨ë¸ ì¬í•™ìŠµ**
  - [ ] ì„ íƒëœ ë³€ìˆ˜ë¡œ ì¬í•™ìŠµ
  - [ ] ì„±ëŠ¥ ë¹„êµ (Before/After)

---

## 8. íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 8.1 ì¼ë°˜ì  ì˜¤ë¥˜

**ë¬¸ì œ 1: SHAP ê³„ì‚°ì´ ë„ˆë¬´ ëŠë¦¼**

```python
# í•´ê²° 1: ìƒ˜í”Œ í¬ê¸° ì¤„ì´ê¸°
X_sample = X_test.sample(min(500, len(X_test)), random_state=42)

# í•´ê²° 2: TreeExplainer ì‚¬ìš© (Tree ëª¨ë¸)
explainer = shap.TreeExplainer(model)  # ë¹ ë¦„
# explainer = shap.KernelExplainer(model.predict, X_sample)  # ëŠë¦¼

# í•´ê²° 3: GPU ì‚¬ìš© (XGBoost + GPU)
model = XGBClassifier(tree_method='gpu_hist')
```

**ë¬¸ì œ 2: Permutation Importanceê°€ ìŒìˆ˜**

```python
# ì›ì¸: ë¬´ì‘ìœ„ ì…”í”Œ í›„ ìš°ì—°íˆ ì„±ëŠ¥ì´ í–¥ìƒë¨
# í•´ê²°: n_repeats ì¦ê°€ (í‰ê· ìœ¼ë¡œ ì•ˆì •í™”)
result = permutation_importance(
    model, X, y,
    n_repeats=30,  # 10 â†’ 30
    random_state=42
)
```

**ë¬¸ì œ 3: Tree Importanceì™€ Permutationì´ í¬ê²Œ ë‹¤ë¦„**

```python
# ì›ì¸ 1: ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„
# â†’ TreeëŠ” ìƒê´€ë³€ìˆ˜ì— importance ë¶„ì‚°
# â†’ Permutationì€ ê° ë³€ìˆ˜ ë…ë¦½ í‰ê°€

# ì›ì¸ 2: ê³¼ì í•©
# â†’ TreeëŠ” Trainì—ì„œ ë†’ì§€ë§Œ, Permì€ Testì—ì„œ ë‚®ìŒ

# í•´ê²°: ë‘ ë°©ë²• ëª¨ë‘ ì°¸ê³ í•˜ì—¬ ì¢…í•© íŒë‹¨
```

### 8.2 í•´ì„ ê´€ë ¨

**Q1: Feature Importanceê°€ ë‚®ë‹¤ê³  ë¬´ì¡°ê±´ ì œê±°í•´ì•¼ í•˜ë‚˜ìš”?**

```
A: ì•„ë‹™ë‹ˆë‹¤.
- ë¹„ì¦ˆë‹ˆìŠ¤ ì¤‘ìš”ì„± ê³ ë ¤ (ë²•ì  ìš”êµ¬ì‚¬í•­ ë“±)
- í•´ì„ ê°€ëŠ¥ì„± (ëª¨ë¸ ì„¤ëª…ì— í•„ìš”)
- ìƒí˜¸ì‘ìš© íš¨ê³¼ (ë‹¨ë…ìœ¼ë¡œëŠ” ì•½í•˜ì§€ë§Œ ì¡°í•© ì‹œ ê°•í•¨)

ê¶Œì¥:
1. Importance < 0.01 and ë¹„ì¦ˆë‹ˆìŠ¤ ì¤‘ìš”ë„ ë‚®ìŒ â†’ ì œê±°
2. Importance < 0.01 but ë¹„ì¦ˆë‹ˆìŠ¤ ì¤‘ìš”ë„ ë†’ìŒ â†’ ìœ ì§€
3. ì• ë§¤í•œ ê²½ìš° â†’ A/B í…ŒìŠ¤íŠ¸ (ì œê±° ì „í›„ ì„±ëŠ¥ ë¹„êµ)
```

**Q2: SHAP ê°’ì´ ì–‘ìˆ˜/ìŒìˆ˜ëŠ” ë¬´ì—‡ì„ ì˜ë¯¸í•˜ë‚˜ìš”?**

```
A: ì˜ˆì¸¡ê°’ì— ëŒ€í•œ ê¸°ì—¬ ë°©í–¥

ì–‘ìˆ˜ SHAP:
- í•´ë‹¹ featureê°€ ì˜ˆì¸¡ì„ ì¦ê°€ì‹œí‚´
- ë¶„ë¥˜: í´ë˜ìŠ¤ 1 í™•ë¥  ì¦ê°€
- íšŒê·€: íƒ€ê²Ÿ ê°’ ì¦ê°€

ìŒìˆ˜ SHAP:
- í•´ë‹¹ featureê°€ ì˜ˆì¸¡ì„ ê°ì†Œì‹œí‚´
- ë¶„ë¥˜: í´ë˜ìŠ¤ 0 í™•ë¥  ì¦ê°€
- íšŒê·€: íƒ€ê²Ÿ ê°’ ê°ì†Œ

ì˜ˆì‹œ:
Feature='ì‹ ìš©ì ìˆ˜', SHAP=+0.3
â†’ ì‹ ìš©ì ìˆ˜ê°€ ìŠ¹ì¸ í™•ë¥ ì„ 30% í¬ì¸íŠ¸ ì¦ê°€ì‹œí‚´
```

**Q3: Feature Importance ë†’ë‹¤ê³  ì¸ê³¼ê´€ê³„ê°€ ìˆë‚˜ìš”?**

```
A: ì•„ë‹™ë‹ˆë‹¤. (ìƒê´€ê´€ê³„ â‰  ì¸ê³¼ê´€ê³„)

Feature ImportanceëŠ”:
- ì˜ˆì¸¡ì— ìœ ìš©í•œ ì •ë„
- ìƒê´€ê´€ê³„ ê°•ë„

ì¸ê³¼ê´€ê³„ ì…ì¦ í•„ìš”:
- ì‹¤í—˜ (A/B í…ŒìŠ¤íŠ¸)
- ì‹œê°„ ìˆœì„œ (ì›ì¸ì´ ê²°ê³¼ë³´ë‹¤ ë¨¼ì €)
- ë„ë©”ì¸ ì§€ì‹ (ë©”ì»¤ë‹ˆì¦˜ ì„¤ëª…)
```

### 8.3 ì„±ëŠ¥ ìµœì í™”

```python
# ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬

# 1. ìƒ˜í”Œë§
X_sample = X.sample(min(10000, len(X)), random_state=42)

# 2. ë³‘ë ¬ ì²˜ë¦¬
result = permutation_importance(
    model, X, y,
    n_repeats=10,
    n_jobs=-1  # ëª¨ë“  CPU ì‚¬ìš©
)

# 3. Tree ëª¨ë¸ ì†ë„ í–¥ìƒ
model = RandomForestClassifier(
    n_estimators=50,  # 100 â†’ 50
    max_depth=10,     # ê¹Šì´ ì œí•œ
    n_jobs=-1
)

# 4. SHAP ê·¼ì‚¬
explainer = shap.TreeExplainer(
    model,
    feature_perturbation='interventional'  # ë¹ ë¥¸ ê·¼ì‚¬
)
```

---

## 9. ì°¸ê³  ìë£Œ

### 9.1 ê³µì‹ ë¬¸ì„œ

- **Scikit-learn Feature Selection**: https://scikit-learn.org/stable/modules/feature_selection.html
- **Permutation Importance**: https://scikit-learn.org/stable/modules/permutation_importance.html
- **SHAP**: https://shap.readthedocs.io/en/latest/
- **XGBoost Feature Importance**: https://xgboost.readthedocs.io/en/latest/python/python_api.html

### 9.2 ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤

1. **Feature Importance íŒŒì´í”„ë¼ì¸**
   ```
   1. RF Importance: ë¹ ë¥¸ íƒìƒ‰ (5ë¶„)
   2. Permutation: Test setì—ì„œ ê²€ì¦ (10ë¶„)
   3. SHAP: ìƒìœ„ 20ê°œ ë³€ìˆ˜ ì‹¬ì¸µ ë¶„ì„ (20ë¶„)
   4. RFE: ìµœì¢… ë³€ìˆ˜ ê°œìˆ˜ ê²°ì • (15ë¶„)
   5. ë¹„ì¦ˆë‹ˆìŠ¤ ê²€ì¦: ë„ë©”ì¸ ì „ë¬¸ê°€ í™•ì¸
   ```

2. **Feature Selection ê¸°ì¤€**
   ```
   ê°•ë ¥ ê¶Œì¥ ì œê±°:
   - ëª¨ë“  ë°©ë²•ì—ì„œ í•˜ìœ„ 20%
   - Permutation â‰ˆ 0
   - ë¹„ì¦ˆë‹ˆìŠ¤ ì¤‘ìš”ë„ ë‚®ìŒ
   
   ì œê±° ê³ ë ¤:
   - 2ê°œ ì´ìƒ ë°©ë²•ì—ì„œ í•˜ìœ„ 30%
   - ìƒê´€ê´€ê³„ ë†’ì€ ë³€ìˆ˜ ì¤‘ í•˜ë‚˜
   
   ìœ ì§€:
   - 2ê°œ ì´ìƒ ë°©ë²•ì—ì„œ ìƒìœ„ 30%
   - SHAPë¡œ í•´ì„ ê°€ëŠ¥í•œ íŒ¨í„´
   - ë¹„ì¦ˆë‹ˆìŠ¤ ì¤‘ìš”ë„ ë†’ìŒ
   ```

3. **í•´ì„ ë° ì„¤ëª…**
   ```
   ë‚´ë¶€ ì„¤ëª… (íŒ€):
   - RF Importanceë¡œ ì¶©ë¶„
   - Permutationìœ¼ë¡œ ê²€ì¦
   
   ì™¸ë¶€ ì„¤ëª… (ê·œì œ, ê³ ê°):
   - SHAP í•„ìˆ˜
   - ê°œë³„ ì˜ˆì¸¡ ì„¤ëª… (Waterfall plot)
   - ë¹„ì¦ˆë‹ˆìŠ¤ ìš©ì–´ë¡œ ë²ˆì—­
   ```

### 9.3 ì¶”ê°€ í•™ìŠµ ìë£Œ

- **SHAP ì§ê´€ì  ì´í•´**: https://christophm.github.io/interpretable-ml-book/shap.html
- **Feature Importance ë¹„êµ**: https://explained.ai/rf-importance/
- **Permutation Importance ë…¼ë¬¸**: https://arxiv.org/abs/1801.01489
- **SHAP ë…¼ë¬¸**: https://arxiv.org/abs/1705.07874

---

## 10. ìš”ì•½

### 10.1 í•µì‹¬ ë©”ì‹œì§€

Feature ImportanceëŠ” ëª¨ë¸ ì„±ëŠ¥ í–¥ìƒê³¼ í•´ì„ ê°€ëŠ¥ì„±ì„ ë™ì‹œì— í™•ë³´í•˜ëŠ” í•µì‹¬ ê¸°ë²•ì…ë‹ˆë‹¤. Tree-based, Permutation, SHAP ë“± ë‹¤ì–‘í•œ ë°©ë²•ì„ ì¡°í•©í•˜ì—¬ robustí•œ feature selectionì„ ìˆ˜í–‰í•˜ê³ , ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### 10.2 ë°©ë²• ì„ íƒ ê°€ì´ë“œ

| ëª©ì  | ì¶”ì²œ ë°©ë²• | ì†Œìš” ì‹œê°„ |
|------|----------|----------|
| ë¹ ë¥¸ íƒìƒ‰ | Random Forest | 5ë¶„ |
| ì¼ë°˜í™” ê²€ì¦ | Permutation (Test) | 10ë¶„ |
| í•´ì„ ë° ì„¤ëª… | SHAP | 20ë¶„ |
| ìµœì  ë³€ìˆ˜ ê°œìˆ˜ | RFE | 15ë¶„ |

### 10.3 ë‹¤ìŒ ë‹¨ê³„

- **Feature Engineering**: ì„ íƒëœ ë³€ìˆ˜ë¡œ ìƒˆë¡œìš´ ë³€ìˆ˜ ìƒì„±
- **ëª¨ë¸ ìµœì í™”**: Hyperparameter tuning
- **ëª¨ë¸ í•´ì„**: SHAP ì‹¬í™” ë¶„ì„
- **A/B í…ŒìŠ¤íŠ¸**: ì‹¤ì œ í™˜ê²½ì—ì„œ ê²€ì¦

---

**ì‘ì„±ì¼**: 2025-01-25  
**ë²„ì „**: 1.0  
**ë‚œì´ë„**: â­â­â­ (ê³ ê¸‰)  
**ì˜ˆìƒ ì†Œìš” ì‹œê°„**: 3-4ì‹œê°„ (í•™ìŠµ ë° ì‹¤ìŠµ)
