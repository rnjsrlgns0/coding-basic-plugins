# 10. Correlation Analysis (ìƒê´€ê´€ê³„ ë¶„ì„)

## 1. ê°œìš”

### 1.1 ëª©ì 
ìƒê´€ê´€ê³„ ë¶„ì„(Correlation Analysis)ì€ ë‘ ê°œ ì´ìƒì˜ ë³€ìˆ˜ ê°„ ì„ í˜•/ë¹„ì„ í˜• ê´€ê³„ì˜ ê°•ë„ì™€ ë°©í–¥ì„ ì •ëŸ‰í™”í•˜ëŠ” ë¶„ì„ ê¸°ë²•ì…ë‹ˆë‹¤. ë³€ìˆ˜ ê°„ ê´€ê³„ë¥¼ ì´í•´í•˜ê³ , ë‹¤ì¤‘ê³µì„ ì„±ì„ íƒì§€í•˜ë©°, feature selectionì˜ ê¸°ì´ˆë¥¼ ì œê³µí•©ë‹ˆë‹¤.

### 1.2 ì ìš© ì‹œê¸°
- ë³€ìˆ˜ ê°„ ê´€ê³„ì˜ ê°•ë„ë¥¼ ì •ëŸ‰ì ìœ¼ë¡œ ì¸¡ì •í•˜ê³  ì‹¶ì„ ë•Œ
- íƒ€ê²Ÿ ë³€ìˆ˜ì™€ ê°•í•˜ê²Œ ì—°ê´€ëœ featuresë¥¼ ì°¾ì„ ë•Œ
- ë‹¤ì¤‘ê³µì„ ì„±(multicollinearity) ë¬¸ì œë¥¼ ì§„ë‹¨í•  ë•Œ
- Feature selection ë° ì°¨ì› ì¶•ì†Œ ì „ ë³€ìˆ˜ ê°„ ì¤‘ë³µì„± í™•ì¸
- ì¸ê³¼ê´€ê³„(causation)ê°€ ì•„ë‹Œ ì—°ê´€ì„±(association) íƒìƒ‰

### 1.3 ì£¼ìš” ê¸°ë²•
- **Pearson ìƒê´€ê³„ìˆ˜**: ì„ í˜• ê´€ê³„ ì¸¡ì •
- **Spearman ìƒê´€ê³„ìˆ˜**: ë‹¨ì¡° ê´€ê³„ ì¸¡ì • (ìˆœìœ„ ê¸°ë°˜)
- **Kendall Tau**: ìˆœìœ„ ì¼ì¹˜ë„ ì¸¡ì •
- **ë¶€ë¶„ ìƒê´€ê´€ê³„**: ì œ3ë³€ìˆ˜ í†µì œ í›„ ê´€ê³„
- **VIF (ë¶„ì‚°íŒ½ì°½ê³„ìˆ˜)**: ë‹¤ì¤‘ê³µì„ ì„± ì§„ë‹¨

---

## 2. ì´ë¡ ì  ë°°ê²½

### 2.1 ìƒê´€ê´€ê³„ vs ì¸ê³¼ê´€ê³„

**í•µì‹¬ ì›ì¹™**: **"Correlation does not imply causation"**

```
ì˜ˆì‹œ 1: ì•„ì´ìŠ¤í¬ë¦¼ íŒë§¤ëŸ‰ â†” ìˆ˜ì˜ì¥ ìµì‚¬ ì‚¬ê³ 
- ê°•í•œ ì–‘ì˜ ìƒê´€ê´€ê³„ (r=0.85)
- í•˜ì§€ë§Œ ì¸ê³¼ê´€ê³„ ì—†ìŒ
- ì œ3ë³€ìˆ˜: ì—¬ë¦„ ë‚ ì”¨ (confounding variable)

ì˜ˆì‹œ 2: í¡ì—° â†” íì•”
- ê°•í•œ ì–‘ì˜ ìƒê´€ê´€ê³„
- ì¸ê³¼ê´€ê³„ ì¡´ì¬ (í¡ì—°ì´ íì•” ìœ ë°œ)
- ìˆ˜ì‹­ ë…„ì˜ ì—°êµ¬ë¡œ ì¸ê³¼ê´€ê³„ ì…ì¦
```

**ìƒê´€ê´€ê³„ ë¶„ì„ì˜ ì—­í• **:
- âœ… ë³€ìˆ˜ ê°„ ì—°ê´€ì„± ë°œê²¬ (ê°€ì„¤ ìƒì„±)
- âœ… ì˜ˆì¸¡ ëª¨ë¸ì˜ feature selection
- âŒ ì¸ê³¼ê´€ê³„ ì…ì¦ (ì¶”ê°€ ì—°êµ¬ í•„ìš”)

### 2.2 ìƒê´€ê³„ìˆ˜ì˜ ì¢…ë¥˜

#### Pearson ìƒê´€ê³„ìˆ˜ (r)
- **ì¸¡ì •**: ì„ í˜• ê´€ê³„ì˜ ê°•ë„ì™€ ë°©í–¥
- **ë²”ìœ„**: -1 â‰¤ r â‰¤ 1
- **ê°€ì •**: 
  - ë³€ìˆ˜ê°€ ì—°ì†í˜•
  - ì„ í˜• ê´€ê³„
  - ì •ê·œë¶„í¬ (ê²€ì • ì‹œ)
  - ì´ìƒì¹˜ì— ë¯¼ê°

#### Spearman ìƒê´€ê³„ìˆ˜ (Ï)
- **ì¸¡ì •**: ë‹¨ì¡° ê´€ê³„ (ìˆœìœ„ ê¸°ë°˜)
- **ë²”ìœ„**: -1 â‰¤ Ï â‰¤ 1
- **ì¥ì **:
  - ë¹„ì„ í˜• ë‹¨ì¡° ê´€ê³„ íƒì§€
  - ì´ìƒì¹˜ì— ê°•ê±´
  - ì •ê·œì„± ê°€ì • ë¶ˆí•„ìš”

#### Kendall Tau (Ï„)
- **ì¸¡ì •**: ìˆœìœ„ ì¼ì¹˜ë„
- **ë²”ìœ„**: -1 â‰¤ Ï„ â‰¤ 1
- **ì¥ì **:
  - ì‘ì€ ìƒ˜í”Œì—ì„œ ë” ì •í™•
  - í•´ì„ì´ ì§ê´€ì 
  - ë™ì (tie)ì´ ë§ì„ ë•Œ ìœ ë¦¬

### 2.3 ìƒê´€ê³„ìˆ˜ í•´ì„ ê¸°ì¤€

```
|r| ê°’         ê°•ë„          í•´ì„
0.0 - 0.1      ì—†ìŒ          ê´€ê³„ ì—†ìŒ
0.1 - 0.3      ì•½í•¨          ì•½í•œ ê´€ê³„
0.3 - 0.5      ì¤‘ê°„          ì¤‘ê°„ ì •ë„ ê´€ê³„
0.5 - 0.7      ê°•í•¨          ê°•í•œ ê´€ê³„
0.7 - 0.9      ë§¤ìš° ê°•í•¨     ë§¤ìš° ê°•í•œ ê´€ê³„
0.9 - 1.0      ê±°ì˜ ì™„ë²½     ê±°ì˜ ì™„ë²½í•œ ê´€ê³„

ì£¼ì˜: ë„ë©”ì¸ì— ë”°ë¼ ê¸°ì¤€ì´ ë‹¤ë¦„
- ì‚¬íšŒê³¼í•™: |r| > 0.3ì´ë©´ ì˜ë¯¸ ìˆìŒ
- ë¬¼ë¦¬/ê³µí•™: |r| > 0.7 ì •ë„ëŠ” ë˜ì–´ì•¼ ì˜ë¯¸ ìˆìŒ
```

### 2.4 ì‹œë‚˜ë¦¬ì˜¤

**ì‹œë‚˜ë¦¬ì˜¤ 1: Feature Selection**
```
ìƒí™©: 100ê°œ features, 1ê°œ target
ëª©í‘œ: ëª¨ë¸ ì„±ëŠ¥ í–¥ìƒ ë° ê³¼ì í•© ë°©ì§€

ë¶„ì„:
1. ê° featureì™€ targetì˜ ìƒê´€ê³„ìˆ˜ ê³„ì‚°
2. |r| > 0.3ì¸ featuresë§Œ ì„ íƒ (20ê°œë¡œ ì¶•ì†Œ)
3. ì„ íƒëœ features ê°„ ìƒê´€ê´€ê³„ í™•ì¸
4. |r| > 0.8ì¸ ìŒ ì¤‘ í•˜ë‚˜ ì œê±° (ë‹¤ì¤‘ê³µì„ ì„±)

ê²°ê³¼:
- ìµœì¢… 15ê°œ features ì„ íƒ
- ëª¨ë¸ ì •í™•ë„ ìœ ì§€í•˜ë©´ì„œ í•™ìŠµ ì†ë„ 3ë°° í–¥ìƒ
```

**ì‹œë‚˜ë¦¬ì˜¤ 2: ë‹¤ì¤‘ê³µì„ ì„± íƒì§€**
```
ìƒí™©: íšŒê·€ ëª¨ë¸ì—ì„œ ê³„ìˆ˜ê°€ ë¶ˆì•ˆì •
ì¦ìƒ: ë³€ìˆ˜ ì¶”ê°€ ì‹œ ë‹¤ë¥¸ ë³€ìˆ˜ ê³„ìˆ˜ê°€ í¬ê²Œ ë³€í•¨

ì§„ë‹¨:
1. ìƒê´€ê´€ê³„ í–‰ë ¬ í™•ì¸ â†’ age & years_experience: r=0.95
2. VIF ê³„ì‚° â†’ age: VIF=18 (ê¸°ì¤€ 10 ì´ˆê³¼)

í•´ê²°:
- ë‘ ë³€ìˆ˜ ì¤‘ í•˜ë‚˜ ì œê±° ë˜ëŠ”
- PCAë¡œ í†µí•©í•˜ì—¬ í•˜ë‚˜ì˜ ë³€ìˆ˜ë¡œ ì¶•ì†Œ
```

**ì‹œë‚˜ë¦¬ì˜¤ 3: ë¹„ì„ í˜• ê´€ê³„ íƒì§€**
```
ìƒí™©: ì‚°ì ë„ì—ì„œ ëª…í™•í•œ ê³¡ì„  íŒ¨í„´
ë¶„ì„:
- Pearson r = 0.05 (ì•½í•¨)
- Spearman Ï = 0.78 (ê°•í•¨)

í•´ì„:
- ì„ í˜• ê´€ê³„ëŠ” ì•½í•˜ì§€ë§Œ ë‹¨ì¡° ì¦ê°€ íŒ¨í„´ ì¡´ì¬
- ë¡œê·¸ ë³€í™˜ ë˜ëŠ” ë‹¤í•­ì‹ features ê³ ë ¤
```

---

## 3. êµ¬í˜„

### 3.1 í™˜ê²½ ì„¤ì •

```python
# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.stats import pearsonr, spearmanr, kendalltau
import warnings
warnings.filterwarnings('ignore')

# í†µê³„ ëª¨ë¸ë§
from statsmodels.stats.outliers_influence import variance_inflation_factor
import statsmodels.api as sm

# ì‹œê°í™” ì„¤ì •
plt.style.use('seaborn-v0_8-whitegrid')
sns.set_palette("coolwarm")
%matplotlib inline

# í•œê¸€ í°íŠ¸ (ì„ íƒ)
plt.rcParams['font.family'] = 'AppleGothic'  # Mac
# plt.rcParams['font.family'] = 'Malgun Gothic'  # Windows
plt.rcParams['axes.unicode_minus'] = False

# ì¶œë ¥ ì˜µì…˜
pd.set_option('display.max_columns', None)
pd.set_option('display.precision', 3)
```

### 3.2 ìƒ˜í”Œ ë°ì´í„° ìƒì„±

```python
# ë‹¤ì–‘í•œ ìƒê´€ê´€ê³„ íŒ¨í„´ì„ ê°€ì§„ ë°ì´í„° ìƒì„±
np.random.seed(42)

def generate_correlation_data(n=500):
    """
    ë‹¤ì–‘í•œ ìƒê´€ê´€ê³„ íŒ¨í„´ì„ ê°€ì§„ ë°ì´í„° ìƒì„±
    """
    # ê¸°ë³¸ ë³€ìˆ˜
    x1 = np.random.normal(100, 15, n)
    
    # ê°•í•œ ì–‘ì˜ ì„ í˜• ê´€ê³„ (r â‰ˆ 0.9)
    x2 = x1 + np.random.normal(0, 5, n)
    
    # ì¤‘ê°„ ì–‘ì˜ ì„ í˜• ê´€ê³„ (r â‰ˆ 0.5)
    x3 = x1 * 0.5 + np.random.normal(50, 20, n)
    
    # ì•½í•œ ìŒì˜ ì„ í˜• ê´€ê³„ (r â‰ˆ -0.2)
    x4 = -0.2 * x1 + np.random.normal(100, 30, n)
    
    # ë¹„ì„ í˜• ê´€ê³„ (2ì°¨)
    x5 = 0.01 * (x1 - 100)**2 + np.random.normal(0, 10, n)
    
    # ë¡œê·¸ ê´€ê³„
    x6 = 20 * np.log(x1) + np.random.normal(0, 5, n)
    
    # ê´€ê³„ ì—†ìŒ (r â‰ˆ 0)
    x7 = np.random.normal(50, 15, n)
    
    # ì´ìƒì¹˜ í¬í•¨ ë³€ìˆ˜
    x8 = x1 * 0.6 + np.random.normal(0, 10, n)
    outlier_idx = np.random.choice(n, 10, replace=False)
    x8[outlier_idx] = np.random.uniform(200, 300, 10)
    
    df = pd.DataFrame({
        'x1_base': x1,
        'x2_strong_pos': x2,
        'x3_medium_pos': x3,
        'x4_weak_neg': x4,
        'x5_quadratic': x5,
        'x6_log': x6,
        'x7_no_corr': x7,
        'x8_outliers': x8
    })
    
    return df

# ë°ì´í„° ìƒì„±
df = generate_correlation_data(500)
print(f"ë°ì´í„° í¬ê¸°: {df.shape}")
print(f"\nê¸°ë³¸ í†µê³„:")
print(df.describe())
```

### 3.3 Pearson ìƒê´€ê³„ìˆ˜

```python
def calculate_pearson_correlation(df, method='pearson'):
    """
    Pearson ìƒê´€ê³„ìˆ˜ í–‰ë ¬ ê³„ì‚° ë° ì‹œê°í™”
    
    Parameters:
    -----------
    df : DataFrame
        ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë§Œ í¬í•¨
    method : str
        'pearson', 'spearman', 'kendall'
    
    Returns:
    --------
    corr_matrix : DataFrame
        ìƒê´€ê³„ìˆ˜ í–‰ë ¬
    """
    # ìƒê´€ê³„ìˆ˜ í–‰ë ¬ ê³„ì‚°
    corr_matrix = df.corr(method=method)
    
    print(f"=" * 70)
    print(f"ğŸ“Š {method.upper()} ìƒê´€ê³„ìˆ˜ í–‰ë ¬")
    print(f"=" * 70)
    print(corr_matrix.round(3))
    
    # íˆíŠ¸ë§µ ì‹œê°í™”
    plt.figure(figsize=(12, 10))
    
    # Mask for upper triangle (ëŒ€ê°ì„  ìœ„ìª½ ì œê±°)
    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
    
    sns.heatmap(
        corr_matrix,
        mask=mask,
        annot=True,
        fmt='.2f',
        cmap='coolwarm',
        center=0,
        vmin=-1,
        vmax=1,
        square=True,
        linewidths=1,
        cbar_kws={"shrink": 0.8, "label": f"{method.capitalize()} Correlation"}
    )
    
    plt.title(f'{method.capitalize()} Correlation Heatmap', fontsize=14, pad=20)
    plt.tight_layout()
    plt.show()
    
    return corr_matrix

# Pearson ìƒê´€ê³„ìˆ˜ ê³„ì‚°
pearson_corr = calculate_pearson_correlation(df, method='pearson')
```

### 3.4 Spearman ë° Kendall ìƒê´€ê³„ìˆ˜

```python
# Spearman ìƒê´€ê³„ìˆ˜ (ë‹¨ì¡° ê´€ê³„)
spearman_corr = calculate_pearson_correlation(df, method='spearman')

# Kendall Tau (ìˆœìœ„ ì¼ì¹˜ë„)
kendall_corr = calculate_pearson_correlation(df, method='kendall')

# 3ê°€ì§€ ë°©ë²• ë¹„êµ
def compare_correlation_methods(df, var1, var2):
    """
    3ê°€ì§€ ìƒê´€ê³„ìˆ˜ ë°©ë²• ë¹„êµ
    """
    x = df[var1]
    y = df[var2]
    
    # ê° ìƒê´€ê³„ìˆ˜ ê³„ì‚°
    pearson_r, pearson_p = pearsonr(x, y)
    spearman_r, spearman_p = spearmanr(x, y)
    kendall_r, kendall_p = kendalltau(x, y)
    
    print(f"\n" + "=" * 70)
    print(f"ğŸ“Š ìƒê´€ê³„ìˆ˜ ë¹„êµ: {var1} vs {var2}")
    print(f"=" * 70)
    print(f"{'Method':<15} {'Coefficient':<15} {'P-value':<15} {'Interpretation'}")
    print(f"-" * 70)
    print(f"{'Pearson':<15} {pearson_r:<15.3f} {pearson_p:<15.3e} {'ì„ í˜• ê´€ê³„'}")
    print(f"{'Spearman':<15} {spearman_r:<15.3f} {spearman_p:<15.3e} {'ë‹¨ì¡° ê´€ê³„'}")
    print(f"{'Kendall':<15} {kendall_r:<15.3f} {kendall_p:<15.3e} {'ìˆœìœ„ ì¼ì¹˜ë„'}")
    
    # ì‚°ì ë„ ì‹œê°í™”
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # ì‚°ì ë„
    axes[0].scatter(x, y, alpha=0.5, s=30)
    axes[0].set_xlabel(var1, fontsize=11)
    axes[0].set_ylabel(var2, fontsize=11)
    axes[0].set_title(f'Scatter Plot\nPearson r={pearson_r:.3f}', fontsize=12)
    axes[0].grid(True, alpha=0.3)
    
    # íšŒê·€ì„  ì¶”ê°€
    z = np.polyfit(x, y, 1)
    p = np.poly1d(z)
    axes[0].plot(x, p(x), "r--", alpha=0.8, linewidth=2, label='Linear Fit')
    axes[0].legend()
    
    # ìˆœìœ„ ì‚°ì ë„ (Spearmanìš©)
    rank_x = stats.rankdata(x)
    rank_y = stats.rankdata(y)
    axes[1].scatter(rank_x, rank_y, alpha=0.5, s=30, color='green')
    axes[1].set_xlabel(f'{var1} (Rank)', fontsize=11)
    axes[1].set_ylabel(f'{var2} (Rank)', fontsize=11)
    axes[1].set_title(f'Rank Plot\nSpearman Ï={spearman_r:.3f}', fontsize=12)
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()

# ë¹„ì„ í˜• ê´€ê³„ ì˜ˆì‹œ
compare_correlation_methods(df, 'x1_base', 'x5_quadratic')

# ì„ í˜• ê´€ê³„ ì˜ˆì‹œ
compare_correlation_methods(df, 'x1_base', 'x2_strong_pos')

# ê´€ê³„ ì—†ìŒ ì˜ˆì‹œ
compare_correlation_methods(df, 'x1_base', 'x7_no_corr')
```

### 3.5 íƒ€ê²Ÿ ë³€ìˆ˜ì™€ì˜ ìƒê´€ê´€ê³„ ë¶„ì„

```python
def analyze_target_correlation(df, target_col, top_n=10):
    """
    íƒ€ê²Ÿ ë³€ìˆ˜ì™€ ê°€ì¥ ìƒê´€ê´€ê³„ê°€ ë†’ì€ features ì‹ë³„
    
    Parameters:
    -----------
    df : DataFrame
    target_col : str
        íƒ€ê²Ÿ ë³€ìˆ˜ ì´ë¦„
    top_n : int
        ìƒìœ„ ëª‡ ê°œ ë³€ìˆ˜ë¥¼ í‘œì‹œí• ì§€
    
    Returns:
    --------
    top_features : DataFrame
        ìƒìœ„ ìƒê´€ê´€ê³„ ë³€ìˆ˜ ëª©ë¡
    """
    # íƒ€ê²Ÿ ë³€ìˆ˜ì™€ì˜ ìƒê´€ê³„ìˆ˜
    target_corr = df.corr()[target_col].drop(target_col).sort_values(
        key=abs, ascending=False
    )
    
    print(f"\n" + "=" * 70)
    print(f"ğŸ“Š '{target_col}'ì™€ ê°€ì¥ ìƒê´€ê´€ê³„ê°€ ë†’ì€ ë³€ìˆ˜ (Top {top_n})")
    print(f"=" * 70)
    print(f"{'Feature':<25} {'Correlation':<15} {'Strength'}")
    print(f"-" * 70)
    
    for feature, corr in target_corr.head(top_n).items():
        strength = get_correlation_strength(abs(corr))
        direction = "â†‘" if corr > 0 else "â†“"
        print(f"{feature:<25} {direction} {corr:>6.3f}          {strength}")
    
    # ì‹œê°í™”
    fig, axes = plt.subplots(1, 2, figsize=(16, 6))
    
    # ë§‰ëŒ€ ê·¸ë˜í”„
    top_features = target_corr.head(top_n)
    colors = ['green' if x > 0 else 'red' for x in top_features.values]
    axes[0].barh(range(len(top_features)), top_features.values, color=colors, alpha=0.7)
    axes[0].set_yticks(range(len(top_features)))
    axes[0].set_yticklabels(top_features.index)
    axes[0].set_xlabel('Correlation Coefficient', fontsize=11)
    axes[0].set_title(f'Top {top_n} Features Correlated with {target_col}', fontsize=12)
    axes[0].axvline(x=0, color='black', linewidth=0.8)
    axes[0].grid(True, alpha=0.3, axis='x')
    
    # ì‚°ì ë„ ë§¤íŠ¸ë¦­ìŠ¤ (ìƒìœ„ 4ê°œ)
    top_4_features = target_corr.head(4).index.tolist()
    selected_cols = top_4_features + [target_col]
    
    # Pairplot
    sns.pairplot(
        df[selected_cols],
        diag_kind='kde',
        plot_kws={'alpha': 0.5, 's': 30},
        height=2
    )
    plt.suptitle(f'Pairplot: Top 4 Features vs {target_col}', y=1.02, fontsize=12)
    plt.tight_layout()
    plt.show()
    
    return target_corr.head(top_n)

def get_correlation_strength(abs_corr):
    """ìƒê´€ê³„ìˆ˜ ê°•ë„ í•´ì„"""
    if abs_corr < 0.1:
        return "ì—†ìŒ"
    elif abs_corr < 0.3:
        return "ì•½í•¨"
    elif abs_corr < 0.5:
        return "ì¤‘ê°„"
    elif abs_corr < 0.7:
        return "ê°•í•¨"
    elif abs_corr < 0.9:
        return "ë§¤ìš° ê°•í•¨"
    else:
        return "ê±°ì˜ ì™„ë²½"

# íƒ€ê²Ÿ ë³€ìˆ˜ì™€ì˜ ìƒê´€ê´€ê³„ ë¶„ì„ (ì˜ˆì‹œ: x2_strong_posë¥¼ íƒ€ê²Ÿìœ¼ë¡œ)
top_features = analyze_target_correlation(df, target_col='x2_strong_pos', top_n=7)
```

### 3.6 ë¶€ë¶„ ìƒê´€ê´€ê³„ (Partial Correlation)

```python
def calculate_partial_correlation(df, x_col, y_col, control_cols):
    """
    ë¶€ë¶„ ìƒê´€ê´€ê³„ ê³„ì‚°: ì œ3ë³€ìˆ˜ë¥¼ í†µì œí•œ í›„ xì™€ yì˜ ê´€ê³„
    
    Parameters:
    -----------
    df : DataFrame
    x_col : str
        ê´€ì‹¬ ë³€ìˆ˜ 1
    y_col : str
        ê´€ì‹¬ ë³€ìˆ˜ 2
    control_cols : list
        í†µì œí•  ë³€ìˆ˜ ëª©ë¡
    
    Returns:
    --------
    partial_corr : float
        ë¶€ë¶„ ìƒê´€ê³„ìˆ˜
    """
    from scipy.stats import pearsonr
    
    # ì”ì°¨ ê³„ì‚° (ì œ3ë³€ìˆ˜ì˜ ì˜í–¥ ì œê±°)
    def get_residuals(df, target, predictors):
        X = df[predictors]
        y = df[target]
        X = sm.add_constant(X)
        model = sm.OLS(y, X).fit()
        return model.resid
    
    # xì™€ yì˜ ì”ì°¨
    resid_x = get_residuals(df, x_col, control_cols)
    resid_y = get_residuals(df, y_col, control_cols)
    
    # ì”ì°¨ ê°„ ìƒê´€ê³„ìˆ˜ = ë¶€ë¶„ ìƒê´€ê³„ìˆ˜
    partial_corr, p_value = pearsonr(resid_x, resid_y)
    
    # ì¼ë°˜ ìƒê´€ê³„ìˆ˜
    simple_corr, _ = pearsonr(df[x_col], df[y_col])
    
    print(f"\n" + "=" * 70)
    print(f"ğŸ“Š ë¶€ë¶„ ìƒê´€ê´€ê³„ ë¶„ì„")
    print(f"=" * 70)
    print(f"X: {x_col}")
    print(f"Y: {y_col}")
    print(f"í†µì œ ë³€ìˆ˜: {', '.join(control_cols)}")
    print(f"\nì¼ë°˜ ìƒê´€ê³„ìˆ˜ (Simple):   {simple_corr:.3f}")
    print(f"ë¶€ë¶„ ìƒê´€ê³„ìˆ˜ (Partial):  {partial_corr:.3f}")
    print(f"P-value:                   {p_value:.3e}")
    
    print(f"\nğŸ’¡ í•´ì„:")
    if abs(partial_corr) < abs(simple_corr) * 0.5:
        print(f"  â†’ ê´€ê³„ê°€ í¬ê²Œ ì•½í•´ì§: í†µì œ ë³€ìˆ˜ê°€ X-Y ê´€ê³„ë¥¼ ë§¤ê°œ")
    elif abs(partial_corr) > abs(simple_corr) * 1.5:
        print(f"  â†’ ê´€ê³„ê°€ ê°•í•´ì§: í†µì œ ë³€ìˆ˜ê°€ ê´€ê³„ë¥¼ ì–µì••(suppression)")
    else:
        print(f"  â†’ ê´€ê³„ê°€ ìœ ì§€ë¨: X-YëŠ” í†µì œ ë³€ìˆ˜ì™€ ë…ë¦½ì ")
    
    # ì‹œê°í™”
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # ì›ë˜ ê´€ê³„
    axes[0].scatter(df[x_col], df[y_col], alpha=0.5, s=30)
    axes[0].set_xlabel(x_col, fontsize=11)
    axes[0].set_ylabel(y_col, fontsize=11)
    axes[0].set_title(f'Simple Correlation\nr = {simple_corr:.3f}', fontsize=12)
    axes[0].grid(True, alpha=0.3)
    
    # ì”ì°¨ ê´€ê³„ (ë¶€ë¶„ ìƒê´€)
    axes[1].scatter(resid_x, resid_y, alpha=0.5, s=30, color='green')
    axes[1].set_xlabel(f'{x_col} (residuals)', fontsize=11)
    axes[1].set_ylabel(f'{y_col} (residuals)', fontsize=11)
    axes[1].set_title(f'Partial Correlation\nr = {partial_corr:.3f}', fontsize=12)
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    return partial_corr, p_value

# ë¶€ë¶„ ìƒê´€ê´€ê³„ ì˜ˆì‹œ
# x1ê³¼ x3ì˜ ê´€ê³„ë¥¼ x2ë¥¼ í†µì œí•œ í›„ í™•ì¸
partial_corr, p_val = calculate_partial_correlation(
    df,
    x_col='x1_base',
    y_col='x3_medium_pos',
    control_cols=['x2_strong_pos']
)
```

### 3.7 ë‹¤ì¤‘ê³µì„ ì„± ì§„ë‹¨ (VIF)

```python
def calculate_vif(df, exclude_cols=None):
    """
    VIF (Variance Inflation Factor) ê³„ì‚°
    
    VIF > 10: ì‹¬ê°í•œ ë‹¤ì¤‘ê³µì„ ì„±
    VIF > 5: ì£¼ì˜ í•„ìš”
    VIF < 5: ë¬¸ì œ ì—†ìŒ
    
    Parameters:
    -----------
    df : DataFrame
        ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë§Œ í¬í•¨
    exclude_cols : list
        ì œì™¸í•  ì»¬ëŸ¼ (ì˜ˆ: íƒ€ê²Ÿ ë³€ìˆ˜)
    
    Returns:
    --------
    vif_df : DataFrame
        VIF ê°’ í…Œì´ë¸”
    """
    # ë³€ìˆ˜ ì„ íƒ
    cols = df.columns.tolist()
    if exclude_cols:
        cols = [c for c in cols if c not in exclude_cols]
    
    X = df[cols]
    
    # VIF ê³„ì‚°
    vif_data = pd.DataFrame()
    vif_data["Feature"] = cols
    vif_data["VIF"] = [
        variance_inflation_factor(X.values, i) for i in range(len(cols))
    ]
    
    # ì •ë ¬
    vif_data = vif_data.sort_values("VIF", ascending=False).reset_index(drop=True)
    
    print(f"\n" + "=" * 70)
    print(f"ğŸ“Š VIF (Variance Inflation Factor) ë¶„ì„")
    print(f"=" * 70)
    print(f"{'Feature':<25} {'VIF':<10} {'Status'}")
    print(f"-" * 70)
    
    for idx, row in vif_data.iterrows():
        feature = row['Feature']
        vif = row['VIF']
        
        if vif > 10:
            status = "ğŸš¨ ì‹¬ê° (ì œê±° ê¶Œì¥)"
            color = '\033[91m'  # Red
        elif vif > 5:
            status = "âš ï¸  ì£¼ì˜"
            color = '\033[93m'  # Yellow
        else:
            status = "âœ… ì •ìƒ"
            color = '\033[92m'  # Green
        
        print(f"{feature:<25} {vif:<10.2f} {color}{status}\033[0m")
    
    print(f"\nğŸ’¡ ë‹¤ì¤‘ê³µì„ ì„± í•´ê²° ë°©ë²•:")
    print(f"  1. VIF > 10ì¸ ë³€ìˆ˜ ì¤‘ í•˜ë‚˜ ì œê±°")
    print(f"  2. ë³€ìˆ˜ ê²°í•© (ì˜ˆ: PCA)")
    print(f"  3. Regularization (Ridge, Lasso)")
    
    # ì‹œê°í™”
    plt.figure(figsize=(10, 6))
    colors = ['red' if v > 10 else 'orange' if v > 5 else 'green' 
              for v in vif_data['VIF']]
    plt.barh(vif_data['Feature'], vif_data['VIF'], color=colors, alpha=0.7)
    plt.xlabel('VIF', fontsize=12)
    plt.title('VIF Analysis (ë‹¤ì¤‘ê³µì„ ì„± ì§„ë‹¨)', fontsize=14)
    plt.axvline(x=5, color='orange', linestyle='--', linewidth=2, label='Threshold: 5')
    plt.axvline(x=10, color='red', linestyle='--', linewidth=2, label='Threshold: 10')
    plt.legend()
    plt.grid(True, alpha=0.3, axis='x')
    plt.tight_layout()
    plt.show()
    
    return vif_data

# VIF ê³„ì‚°
vif_results = calculate_vif(df)
```

### 3.8 ìƒê´€ê´€ê³„ í–‰ë ¬ í•„í„°ë§

```python
def filter_high_correlations(df, threshold=0.8, method='pearson'):
    """
    ë†’ì€ ìƒê´€ê´€ê³„ë¥¼ ê°€ì§„ ë³€ìˆ˜ ìŒ ì‹ë³„
    
    Parameters:
    -----------
    df : DataFrame
    threshold : float
        ìƒê´€ê³„ìˆ˜ ì„ê³„ê°’ (ì ˆëŒ“ê°’)
    method : str
        ìƒê´€ê³„ìˆ˜ ë°©ë²•
    
    Returns:
    --------
    high_corr_pairs : DataFrame
        ë†’ì€ ìƒê´€ê´€ê³„ ë³€ìˆ˜ ìŒ
    """
    # ìƒê´€ê³„ìˆ˜ í–‰ë ¬
    corr_matrix = df.corr(method=method)
    
    # ìƒì‚¼ê° í–‰ë ¬ë§Œ (ì¤‘ë³µ ì œê±°)
    upper_tri = corr_matrix.where(
        np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
    )
    
    # ë†’ì€ ìƒê´€ê´€ê³„ ìŒ ì°¾ê¸°
    high_corr_pairs = []
    for column in upper_tri.columns:
        high_corr = upper_tri[column][abs(upper_tri[column]) > threshold]
        for idx, corr in high_corr.items():
            high_corr_pairs.append({
                'Feature 1': idx,
                'Feature 2': column,
                'Correlation': corr,
                'Abs_Correlation': abs(corr)
            })
    
    high_corr_df = pd.DataFrame(high_corr_pairs).sort_values(
        'Abs_Correlation', ascending=False
    )
    
    if len(high_corr_df) == 0:
        print(f"\nâœ… |r| > {threshold}ì¸ ë³€ìˆ˜ ìŒì´ ì—†ìŠµë‹ˆë‹¤.")
        return high_corr_df
    
    print(f"\n" + "=" * 70)
    print(f"ğŸ” ë†’ì€ ìƒê´€ê´€ê³„ ë³€ìˆ˜ ìŒ (|r| > {threshold})")
    print(f"=" * 70)
    print(high_corr_df.to_string(index=False))
    
    print(f"\nğŸ’¡ ê¶Œì¥ ì¡°ì¹˜:")
    print(f"  - ë‘ ë³€ìˆ˜ ì¤‘ í•˜ë‚˜ ì œê±° ê³ ë ¤")
    print(f"  - íƒ€ê²Ÿê³¼ ë” ë†’ì€ ìƒê´€ê´€ê³„ë¥¼ ê°€ì§„ ë³€ìˆ˜ ìœ ì§€")
    print(f"  - ë˜ëŠ” PCA/feature engineeringìœ¼ë¡œ ê²°í•©")
    
    return high_corr_df

# ë†’ì€ ìƒê´€ê´€ê³„ ë³€ìˆ˜ ìŒ ì°¾ê¸°
high_corr_pairs = filter_high_correlations(df, threshold=0.8)
```

### 3.9 ì´ìƒì¹˜ì˜ ì˜í–¥ ë¶„ì„

```python
def analyze_outlier_effect_on_correlation(df, x_col, y_col, outlier_threshold=3):
    """
    ì´ìƒì¹˜ê°€ ìƒê´€ê³„ìˆ˜ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ë¶„ì„
    
    Parameters:
    -----------
    df : DataFrame
    x_col, y_col : str
        ë¶„ì„í•  ë³€ìˆ˜
    outlier_threshold : float
        Z-score ì„ê³„ê°’
    """
    x = df[x_col].copy()
    y = df[y_col].copy()
    
    # Z-score ê³„ì‚°
    z_x = np.abs(stats.zscore(x))
    z_y = np.abs(stats.zscore(y))
    
    # ì´ìƒì¹˜ ë§ˆìŠ¤í¬
    outlier_mask = (z_x > outlier_threshold) | (z_y > outlier_threshold)
    
    # ì´ìƒì¹˜ í¬í•¨/ì œì™¸ ìƒê´€ê³„ìˆ˜
    corr_with_outliers, _ = pearsonr(x, y)
    corr_without_outliers, _ = pearsonr(
        x[~outlier_mask], y[~outlier_mask]
    )
    
    # Spearman (ì´ìƒì¹˜ ê°•ê±´)
    spearman_with, _ = spearmanr(x, y)
    
    print(f"\n" + "=" * 70)
    print(f"ğŸ“Š ì´ìƒì¹˜ ì˜í–¥ ë¶„ì„: {x_col} vs {y_col}")
    print(f"=" * 70)
    print(f"ì „ì²´ ìƒ˜í”Œ: {len(x)}ê°œ")
    print(f"ì´ìƒì¹˜: {outlier_mask.sum()}ê°œ")
    print(f"\nPearson (ì´ìƒì¹˜ í¬í•¨):  {corr_with_outliers:.3f}")
    print(f"Pearson (ì´ìƒì¹˜ ì œì™¸):  {corr_without_outliers:.3f}")
    print(f"Spearman (ê°•ê±´):        {spearman_with:.3f}")
    
    # ì°¨ì´ í•´ì„
    diff = abs(corr_with_outliers - corr_without_outliers)
    if diff > 0.2:
        print(f"\nâš ï¸  ì´ìƒì¹˜ê°€ ìƒê´€ê³„ìˆ˜ë¥¼ í¬ê²Œ ì™œê³¡í•©ë‹ˆë‹¤ (Î”r = {diff:.3f})")
        print(f"  â†’ Spearman ìƒê´€ê³„ìˆ˜ ë˜ëŠ” ì´ìƒì¹˜ ì œê±° ê³ ë ¤")
    else:
        print(f"\nâœ… ì´ìƒì¹˜ì˜ ì˜í–¥ì´ í¬ì§€ ì•ŠìŠµë‹ˆë‹¤ (Î”r = {diff:.3f})")
    
    # ì‹œê°í™”
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # ì´ìƒì¹˜ í¬í•¨
    axes[0].scatter(x, y, alpha=0.5, s=30, label='Normal')
    axes[0].scatter(
        x[outlier_mask], y[outlier_mask],
        color='red', s=100, alpha=0.7, label='Outliers', edgecolors='black'
    )
    axes[0].set_xlabel(x_col, fontsize=11)
    axes[0].set_ylabel(y_col, fontsize=11)
    axes[0].set_title(f'With Outliers\nPearson r={corr_with_outliers:.3f}', fontsize=12)
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # ì´ìƒì¹˜ ì œì™¸
    axes[1].scatter(x[~outlier_mask], y[~outlier_mask], alpha=0.5, s=30, color='green')
    axes[1].set_xlabel(x_col, fontsize=11)
    axes[1].set_ylabel(y_col, fontsize=11)
    axes[1].set_title(f'Without Outliers\nPearson r={corr_without_outliers:.3f}', fontsize=12)
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()

# ì´ìƒì¹˜ ì˜í–¥ ë¶„ì„
analyze_outlier_effect_on_correlation(df, 'x1_base', 'x8_outliers')
```

### 3.10 ìƒê´€ê´€ê³„ ì¢…í•© ë¦¬í¬íŠ¸

```python
def generate_correlation_report(df, target_col=None, output_file='correlation_report.txt'):
    """
    ì¢…í•© ìƒê´€ê´€ê³„ ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±
    
    Parameters:
    -----------
    df : DataFrame
    target_col : str
        íƒ€ê²Ÿ ë³€ìˆ˜ (ìˆëŠ” ê²½ìš°)
    output_file : str
        ì €ì¥í•  íŒŒì¼ëª…
    """
    report = []
    report.append("=" * 70)
    report.append("ğŸ“Š CORRELATION ANALYSIS REPORT")
    report.append("=" * 70)
    report.append(f"\nìƒì„±ì¼ì‹œ: {pd.Timestamp.now()}")
    report.append(f"ë°ì´í„° í¬ê¸°: {df.shape[0]} rows Ã— {df.shape[1]} columns\n")
    
    # 1. ê¸°ë³¸ ìƒê´€ê³„ìˆ˜ í–‰ë ¬
    report.append("\n" + "=" * 70)
    report.append("1. ìƒê´€ê³„ìˆ˜ í–‰ë ¬ (Pearson)")
    report.append("=" * 70)
    corr_matrix = df.corr()
    report.append(corr_matrix.to_string())
    
    # 2. íƒ€ê²Ÿ ë³€ìˆ˜ì™€ì˜ ìƒê´€ê´€ê³„
    if target_col:
        report.append("\n" + "=" * 70)
        report.append(f"2. '{target_col}'ì™€ ìƒê´€ê´€ê³„ Top 10")
        report.append("=" * 70)
        target_corr = corr_matrix[target_col].drop(target_col).sort_values(
            key=abs, ascending=False
        ).head(10)
        for feat, corr in target_corr.items():
            report.append(f"{feat-<30} {corr:>8.3f}")
    
    # 3. ë†’ì€ ìƒê´€ê´€ê³„ ë³€ìˆ˜ ìŒ
    report.append("\n" + "=" * 70)
    report.append("3. ë†’ì€ ìƒê´€ê´€ê³„ ë³€ìˆ˜ ìŒ (|r| > 0.7)")
    report.append("=" * 70)
    high_corr = filter_high_correlations(df, threshold=0.7, method='pearson')
    if len(high_corr) > 0:
        report.append(high_corr.to_string(index=False))
    else:
        report.append("ì—†ìŒ")
    
    # 4. VIF ë¶„ì„
    report.append("\n" + "=" * 70)
    report.append("4. VIF ë¶„ì„ (ë‹¤ì¤‘ê³µì„ ì„±)")
    report.append("=" * 70)
    vif_df = calculate_vif(df, exclude_cols=[target_col] if target_col else None)
    report.append(vif_df.to_string(index=False))
    
    # 5. ê¶Œì¥ ì‚¬í•­
    report.append("\n" + "=" * 70)
    report.append("5. ê¶Œì¥ ì‚¬í•­")
    report.append("=" * 70)
    
    # VIF ê¸°ë°˜ ê¶Œì¥
    high_vif = vif_df[vif_df['VIF'] > 10]
    if len(high_vif) > 0:
        report.append("\nâš ï¸  ë‹¤ì¤‘ê³µì„ ì„± ë¬¸ì œ:")
        for feat in high_vif['Feature']:
            report.append(f"  - {feat} ë³€ìˆ˜ ì œê±° ë˜ëŠ” ê²°í•© ê³ ë ¤")
    else:
        report.append("\nâœ… ë‹¤ì¤‘ê³µì„ ì„± ë¬¸ì œ ì—†ìŒ")
    
    # íƒ€ê²Ÿ ìƒê´€ ê¸°ë°˜ ê¶Œì¥
    if target_col:
        weak_corr = target_corr[abs(target_corr) < 0.1]
        if len(weak_corr) > 0:
            report.append(f"\nğŸ’¡ ì•½í•œ ìƒê´€ê´€ê³„ ë³€ìˆ˜ ({len(weak_corr)}ê°œ):")
            report.append(f"  â†’ Feature selection ì‹œ ì œê±° ê³ ë ¤")
    
    # ë¦¬í¬íŠ¸ ì €ì¥
    report_text = "\n".join(report)
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(report_text)
    
    print(report_text)
    print(f"\nâœ… ë¦¬í¬íŠ¸ ì €ì¥: {output_file}")
    
    return report_text

# ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±
report = generate_correlation_report(df, target_col='x2_strong_pos')
```

---

## 4. ì˜ˆì‹œ

### 4.1 ì‹¤ì „ ì˜ˆì œ: ë¶€ë™ì‚° ê°€ê²© ì˜ˆì¸¡ Feature Selection

```python
print("=" * 70)
print("ğŸ“ˆ ë¹„ì¦ˆë‹ˆìŠ¤ ì‹œë‚˜ë¦¬ì˜¤: ë¶€ë™ì‚° ê°€ê²© ì˜ˆì¸¡ ëª¨ë¸")
print("=" * 70)
print("\nëª©í‘œ:")
print("- 50ê°œ featuresì—ì„œ ì¤‘ìš” ë³€ìˆ˜ ì„ íƒ")
print("- ë‹¤ì¤‘ê³µì„ ì„± ì œê±°")
print("- ëª¨ë¸ ì„±ëŠ¥ í–¥ìƒ")

print("\nğŸ”„ ë¶„ì„ í”„ë¡œì„¸ìŠ¤:")
print("-" * 70)
print("1ë‹¨ê³„: íƒ€ê²Ÿ(ê°€ê²©)ê³¼ ìƒê´€ê´€ê³„ ë¶„ì„")
print("   â†’ Top 20ê°œ features ì„ íƒ (|r| > 0.3)")
print("\n2ë‹¨ê³„: ì„ íƒëœ features ê°„ ìƒê´€ê´€ê³„ í™•ì¸")
print("   â†’ 'ë©´ì 'ê³¼ 'ë°©ê°œìˆ˜': r=0.92 (ë†’ì€ ìƒê´€)")
print("   â†’ 'ê±´ë¬¼ë‚˜ì´'ì™€ 'ë¦¬ëª¨ë¸ë§ì—°ë„': r=-0.88")
print("\n3ë‹¨ê³„: VIF ê³„ì‚°")
print("   â†’ 'ë©´ì ': VIF=15 (ì‹¬ê°)")
print("   â†’ 'ë°©ê°œìˆ˜': VIF=12 (ì‹¬ê°)")
print("\n4ë‹¨ê³„: ë³€ìˆ˜ ì œê±° ì „ëµ")
print("   â†’ 'ë©´ì ' ìœ ì§€ (íƒ€ê²Ÿ ìƒê´€ 0.75)")
print("   â†’ 'ë°©ê°œìˆ˜' ì œê±° (íƒ€ê²Ÿ ìƒê´€ 0.68)")
print("   â†’ 'ë¦¬ëª¨ë¸ë§ì—°ë„' ìœ ì§€ (íƒ€ê²Ÿ ìƒê´€ 0.42)")
print("   â†’ 'ê±´ë¬¼ë‚˜ì´' ì œê±° (íƒ€ê²Ÿ ìƒê´€ -0.40)")

print("\nâœ… ìµœì¢… ê²°ê³¼:")
print("-" * 70)
print("ì„ íƒëœ features: 15ê°œ")
print("ëª¨ë¸ ì„±ëŠ¥:")
print("  - Before: RÂ² = 0.78, Training time = 25s")
print("  - After:  RÂ² = 0.80, Training time = 8s")
print("  â†’ ì„±ëŠ¥ í–¥ìƒ + 3ë°° ë¹ ë¥¸ í•™ìŠµ")
```

### 4.2 ì…ì¶œë ¥ ì˜ˆì‹œ

```python
# ì…ë ¥: ì›ë³¸ ë°ì´í„°
print("\nğŸ“¥ ì…ë ¥ ë°ì´í„°:")
print(df.head())

# ì¶œë ¥ 1: ìƒê´€ê³„ìˆ˜ í–‰ë ¬
print("\nğŸ“¤ ì¶œë ¥ 1: ìƒê´€ê³„ìˆ˜ í–‰ë ¬")
print(pearson_corr.round(3))

# ì¶œë ¥ 2: íƒ€ê²Ÿ ìƒê´€ê´€ê³„
print("\nğŸ“¤ ì¶œë ¥ 2: íƒ€ê²Ÿ ë³€ìˆ˜ ìƒê´€ê´€ê³„")
print(top_features)

# ì¶œë ¥ 3: VIF í…Œì´ë¸”
print("\nğŸ“¤ ì¶œë ¥ 3: VIF ë¶„ì„ ê²°ê³¼")
print(vif_results)

# ì¶œë ¥ 4: ë†’ì€ ìƒê´€ê´€ê³„ ìŒ
print("\nğŸ“¤ ì¶œë ¥ 4: ë†’ì€ ìƒê´€ê´€ê³„ ë³€ìˆ˜ ìŒ")
print(high_corr_pairs)
```

---

## 5. ì—ì´ì „íŠ¸ ë§¤í•‘

### 5.1 ë‹´ë‹¹ ì—ì´ì „íŠ¸

| ì‘ì—… | Primary Agent | Supporting Agents |
|------|--------------|-------------------|
| ìƒê´€ê³„ìˆ˜ ê³„ì‚° ë° í•´ì„ | `data-scientist` | - |
| íˆíŠ¸ë§µ ì‹œê°í™” | `data-visualization-specialist` | `data-scientist` |
| VIF ë¶„ì„ | `data-scientist` | `feature-engineering-specialist` |
| Feature selection | `feature-engineering-specialist` | `data-scientist` |
| ë¶€ë¶„ ìƒê´€ê´€ê³„ ë¶„ì„ | `data-scientist` | - |

### 5.2 ê´€ë ¨ ìŠ¤í‚¬

**Scientific Skills**:
- `scipy` (pearsonr, spearmanr, kendalltau)
- `pandas` (corr ë©”ì„œë“œ)
- `seaborn` (heatmap, pairplot)
- `statsmodels` (VIF, OLS)
- `matplotlib` (ì‹œê°í™”)

---

## 6. í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬

### 6.1 í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬

```bash
# ë°ì´í„° ì²˜ë¦¬
pip install pandas==2.2.0
pip install numpy==1.26.3

# í†µê³„ ë¶„ì„
pip install scipy==1.12.0
pip install statsmodels==0.14.1

# ì‹œê°í™”
pip install matplotlib==3.8.2
pip install seaborn==0.13.1
```

### 6.2 ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „ í™•ì¸

```python
import pandas as pd
import numpy as np
import scipy
import statsmodels
import matplotlib
import seaborn as sns

print("ë¼ì´ë¸ŒëŸ¬ë¦¬ ë²„ì „:")
print(f"pandas: {pd.__version__}")
print(f"numpy: {np.__version__}")
print(f"scipy: {scipy.__version__}")
print(f"statsmodels: {statsmodels.__version__}")
print(f"matplotlib: {matplotlib.__version__}")
print(f"seaborn: {sns.__version__}")
```

---

## 7. ì²´í¬í¬ì¸íŠ¸

### 7.1 ë¶„ì„ ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] **ë°ì´í„° ì „ì²˜ë¦¬**
  - [ ] ê²°ì¸¡ê°’ ì²˜ë¦¬ ì™„ë£Œ
  - [ ] ì´ìƒì¹˜ í™•ì¸ ì™„ë£Œ
  - [ ] ë³€ìˆ˜ê°€ ìˆ˜ì¹˜í˜•ì¸ì§€ í™•ì¸

- [ ] **ë¶„ì„ ëª©ì  ëª…í™•í™”**
  - [ ] Feature selectionìš©ì¸ê°€?
  - [ ] ë‹¤ì¤‘ê³µì„ ì„± ì§„ë‹¨ìš©ì¸ê°€?
  - [ ] íƒìƒ‰ì  ë¶„ì„ìš©ì¸ê°€?

### 7.2 ë¶„ì„ ì¤‘ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] **ìƒê´€ê³„ìˆ˜ ì„ íƒ**
  - [ ] ì„ í˜• ê´€ê³„: Pearson
  - [ ] ë‹¨ì¡° ê´€ê³„: Spearman
  - [ ] ì´ìƒì¹˜ ë§ìŒ: Spearman ë˜ëŠ” Kendall

- [ ] **í•´ì„**
  - [ ] ìƒê´€ê³„ìˆ˜ í¬ê¸° í™•ì¸ (|r| ê°’)
  - [ ] P-value í™•ì¸ (í†µê³„ì  ìœ ì˜ì„±)
  - [ ] ì‹¤ì§ˆì  ì˜ë¯¸(practical significance) ê³ ë ¤

### 7.3 ë¶„ì„ í›„ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] **ë‹¤ì¤‘ê³µì„ ì„±**
  - [ ] VIF > 10ì¸ ë³€ìˆ˜ ì²˜ë¦¬
  - [ ] |r| > 0.8ì¸ ë³€ìˆ˜ ìŒ ì²˜ë¦¬

- [ ] **Feature Selection**
  - [ ] íƒ€ê²Ÿê³¼ ì•½í•œ ìƒê´€ ë³€ìˆ˜ ì œê±°
  - [ ] ì¤‘ë³µ ë³€ìˆ˜ ì œê±°
  - [ ] ìµœì¢… ë³€ìˆ˜ ëª©ë¡ ë¬¸ì„œí™”

---

## 8. íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 8.1 ì¼ë°˜ì  ì˜¤ë¥˜

**ë¬¸ì œ 1: `ValueError: Input contains NaN`**

```python
# ì›ì¸: ê²°ì¸¡ê°’ ì¡´ì¬
# í•´ê²°:
df_clean = df.dropna()  # ë˜ëŠ”
df_clean = df.fillna(df.mean())
```

**ë¬¸ì œ 2: ìƒê´€ê³„ìˆ˜ê°€ NaN**

```python
# ì›ì¸: ë³€ìˆ˜ì˜ í‘œì¤€í¸ì°¨ê°€ 0 (ìƒìˆ˜)
# í•´ê²°: ìƒìˆ˜ ë³€ìˆ˜ ì œê±°
df_clean = df.loc[:, df.std() > 0]
```

**ë¬¸ì œ 3: VIF ê³„ì‚° ì‹œ ë¬´í•œëŒ€(`inf`) ë°œìƒ**

```python
# ì›ì¸: ì™„ë²½í•œ ë‹¤ì¤‘ê³µì„ ì„± (r=1.0)
# í•´ê²°: ì¤‘ë³µ ë³€ìˆ˜ í•˜ë‚˜ ì œê±°
high_corr_pairs = filter_high_correlations(df, threshold=0.99)
# í•˜ë‚˜ì”© ì œê±° í›„ ì¬ê³„ì‚°
```

### 8.2 í•´ì„ ê´€ë ¨

**Q1: ìƒê´€ê´€ê³„ê°€ ìˆìœ¼ë©´ ì¸ê³¼ê´€ê³„ê°€ ìˆë‚˜ìš”?**

```
A: ì•„ë‹™ë‹ˆë‹¤.
- ìƒê´€ê´€ê³„: Xì™€ Yê°€ í•¨ê»˜ ë³€í•œë‹¤
- ì¸ê³¼ê´€ê³„: Xê°€ Yë¥¼ ì•¼ê¸°í•œë‹¤

ì¸ê³¼ê´€ê³„ ì…ì¦ ë°©ë²•:
1. ì‹œê°„ ìˆœì„œ (Xê°€ Yë³´ë‹¤ ë¨¼ì €)
2. ë©”ì»¤ë‹ˆì¦˜ ì„¤ëª… ê°€ëŠ¥
3. ì œ3ë³€ìˆ˜ í†µì œ (ì‹¤í—˜ ë˜ëŠ” í†µê³„ì  í†µì œ)
4. ë°˜ì¦ ê°€ëŠ¥ì„± ë°°ì œ
```

**Q2: P-valueëŠ” ë¬´ì—‡ì„ ì˜ë¯¸í•˜ë‚˜ìš”?**

```
A: ê·€ë¬´ê°€ì„¤(ìƒê´€ê³„ìˆ˜=0) í•˜ì—ì„œ ê´€ì¸¡ëœ ê²°ê³¼ê°€ ë‚˜ì˜¬ í™•ë¥ 

í•´ì„:
- p < 0.05: í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ ìƒê´€ê´€ê³„
- p â‰¥ 0.05: ìš°ì—°ì— ì˜í•œ ìƒê´€ê´€ê³„ì¼ ê°€ëŠ¥ì„±

ì£¼ì˜:
- p-valueëŠ” íš¨ê³¼ í¬ê¸°(|r|)ì™€ ë‹¤ë¦„
- ìƒ˜í”Œì´ í¬ë©´ ì‘ì€ rë„ ìœ ì˜ë¯¸í•  ìˆ˜ ìˆìŒ
â†’ ì‹¤ì§ˆì  ì˜ë¯¸(practical significance) í•¨ê»˜ ê³ ë ¤
```

**Q3: Pearson vs Spearman ì–¸ì œ ì‚¬ìš©í•˜ë‚˜ìš”?**

```
A: ë°ì´í„° íŠ¹ì„±ì— ë”°ë¼ ì„ íƒ

Pearson:
âœ… ì„ í˜• ê´€ê³„
âœ… ì •ê·œë¶„í¬ (ê²€ì • ì‹œ)
âœ… ì´ìƒì¹˜ ì—†ìŒ

Spearman:
âœ… ë¹„ì„ í˜• ë‹¨ì¡° ê´€ê³„
âœ… ìˆœì„œí˜• ë°ì´í„°
âœ… ì´ìƒì¹˜ ë§ìŒ
âœ… ì •ê·œì„± ê°€ì • ë¶ˆí•„ìš”

ì‹¤ë¬´ ì „ëµ:
1. ì‚°ì ë„ë¡œ ê´€ê³„ í™•ì¸
2. ì„ í˜•ì´ë©´ Pearson
3. ê³¡ì„ ì´ì§€ë§Œ ë‹¨ì¡°ë©´ Spearman
4. ë‘˜ ë‹¤ ê³„ì‚°í•˜ì—¬ ë¹„êµ
```

### 8.3 ë‹¤ì¤‘ê³µì„ ì„± í•´ê²° ì „ëµ

```python
# ì „ëµ 1: ë³€ìˆ˜ ì œê±°
# VIF > 10ì¸ ë³€ìˆ˜ ì¤‘ íƒ€ê²Ÿ ìƒê´€ ë‚®ì€ ê²ƒ ì œê±°

# ì „ëµ 2: PCAë¡œ í†µí•©
from sklearn.decomposition import PCA
pca = PCA(n_components=1)
X_combined = pca.fit_transform(df[['var1', 'var2']])

# ì „ëµ 3: Regularization
from sklearn.linear_model import Ridge, Lasso
# Ridge/LassoëŠ” ë‹¤ì¤‘ê³µì„ ì„±ì— ê°•ê±´

# ì „ëµ 4: Domain knowledge
# ë¹„ì¦ˆë‹ˆìŠ¤ ê´€ì ì—ì„œ ë” ì¤‘ìš”í•œ ë³€ìˆ˜ ìœ ì§€
```

---

## 9. ì°¸ê³  ìë£Œ

### 9.1 ê³µì‹ ë¬¸ì„œ

- **Pandas Correlation**: https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html
- **SciPy Stats**: https://docs.scipy.org/doc/scipy/reference/stats.html
- **Statsmodels VIF**: https://www.statsmodels.org/stable/generated/statsmodels.stats.outliers_influence.variance_inflation_factor.html
- **Seaborn Heatmap**: https://seaborn.pydata.org/generated/seaborn.heatmap.html

### 9.2 ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤

1. **ìƒê´€ê´€ê³„ ë¶„ì„ íŒŒì´í”„ë¼ì¸**
   ```
   1. ì‚°ì ë„ë¡œ ê´€ê³„ ì‹œê°í™”
   2. ì ì ˆí•œ ìƒê´€ê³„ìˆ˜ ì„ íƒ (Pearson/Spearman)
   3. P-valueë¡œ í†µê³„ì  ìœ ì˜ì„± í™•ì¸
   4. íš¨ê³¼ í¬ê¸°(|r|)ë¡œ ì‹¤ì§ˆì  ì˜ë¯¸ íŒë‹¨
   5. ì¸ê³¼ê´€ê³„ì™€ í˜¼ë™í•˜ì§€ ì•Šê¸°
   ```

2. **Feature Selection ì „ëµ**
   ```
   1. íƒ€ê²Ÿê³¼ ìƒê´€ê´€ê³„ ê³„ì‚°
   2. ì•½í•œ ìƒê´€ ë³€ìˆ˜ ì œê±° (|r| < 0.1)
   3. Features ê°„ ìƒê´€ê´€ê³„ í™•ì¸
   4. ë†’ì€ ìƒê´€ ìŒ ì¤‘ í•˜ë‚˜ ì œê±°
   5. VIFë¡œ ë‹¤ì¤‘ê³µì„ ì„± ìµœì¢… í™•ì¸
   ```

3. **ë‹¤ì¤‘ê³µì„ ì„± ê´€ë¦¬**
   ```
   - VIF < 5: ë¬¸ì œ ì—†ìŒ
   - 5 < VIF < 10: ì£¼ì˜, ëª¨ë‹ˆí„°ë§
   - VIF > 10: ì¦‰ì‹œ ì¡°ì¹˜ í•„ìš”
   ```

### 9.3 ì¶”ê°€ í•™ìŠµ ìë£Œ

- **ìƒê´€ê´€ê³„ ì§ê´€ì  ì´í•´**: https://rpsychologist.com/correlation/
- **Correlation vs Causation**: https://www.tylervigen.com/spurious-correlations
- **Partial Correlation**: https://en.wikipedia.org/wiki/Partial_correlation
- **VIF ì„¤ëª…**: https://statisticsbyjim.com/regression/multicollinearity-in-regression-analysis/

---

## 10. ìš”ì•½

### 10.1 í•µì‹¬ ë©”ì‹œì§€

ìƒê´€ê´€ê³„ ë¶„ì„ì€ ë³€ìˆ˜ ê°„ ì„ í˜•/ë¹„ì„ í˜• ì—°ê´€ì„±ì„ ì •ëŸ‰í™”í•˜ëŠ” í•„ìˆ˜ ê¸°ë²•ì…ë‹ˆë‹¤. Pearson, Spearman, Kendall ë“± ë‹¤ì–‘í•œ ìƒê´€ê³„ìˆ˜ë¥¼ ë°ì´í„° íŠ¹ì„±ì— ë§ê²Œ ì„ íƒí•˜ê³ , VIFë¡œ ë‹¤ì¤‘ê³µì„ ì„±ì„ ì§„ë‹¨í•˜ì—¬ íš¨ê³¼ì ì¸ feature selectionê³¼ ëª¨ë¸ë§ ì „ëµì„ ìˆ˜ë¦½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**ì£¼ì˜**: ìƒê´€ê´€ê³„ëŠ” ì¸ê³¼ê´€ê³„ê°€ ì•„ë‹™ë‹ˆë‹¤!

### 10.2 ì‹¤ë¬´ ì ìš© ìˆœì„œ

1. **íˆíŠ¸ë§µ**: ì „ì²´ ìƒê´€ê´€ê³„ íŒŒì•… (5ë¶„)
2. **íƒ€ê²Ÿ ìƒê´€**: ì¤‘ìš” features ì‹ë³„ (5ë¶„)
3. **ì‚°ì ë„**: ê´€ê³„ ì‹œê°ì  í™•ì¸ (10ë¶„)
4. **VIF**: ë‹¤ì¤‘ê³µì„ ì„± ì§„ë‹¨ (5ë¶„)
5. **Feature Selection**: ìµœì¢… ë³€ìˆ˜ ì„ íƒ (10ë¶„)

**ì´ ì†Œìš” ì‹œê°„**: ì•½ 35ë¶„

### 10.3 ë‹¤ìŒ ë‹¨ê³„

- **Feature ì¤‘ìš”ë„ ë¶„ì„**: `09-feature-importance.md` ì°¸ê³ 
- **í†µê³„ ê²€ì •**: `11-hypothesis-testing.md` ì°¸ê³ 
- **íšŒê·€ ë¶„ì„**: statsmodels OLS í™œìš©
- **ì°¨ì› ì¶•ì†Œ**: `06-multivariate-analysis.md` ì°¸ê³ 

---

**ì‘ì„±ì¼**: 2025-01-25  
**ë²„ì „**: 1.0  
**ë‚œì´ë„**: â­â­ (ì¤‘ê¸‰)  
**ì˜ˆìƒ ì†Œìš” ì‹œê°„**: 2-3ì‹œê°„ (í•™ìŠµ ë° ì‹¤ìŠµ)
