# 11. Hypothesis Testing (ê°€ì„¤ ê²€ì •)

## 1. ê°œìš”

### 1.1 ëª©ì 
ê°€ì„¤ ê²€ì •(Hypothesis Testing)ì€ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëª¨ì§‘ë‹¨ì— ëŒ€í•œ ê°€ì„¤ì´ í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œì§€ë¥¼ íŒë‹¨í•˜ëŠ” ì¶”ë¡  ê¸°ë²•ì…ë‹ˆë‹¤. A/B í…ŒìŠ¤íŠ¸, ê·¸ë£¹ ê°„ ì°¨ì´ ê²€ì¦, íš¨ê³¼ì„± ê²€ì¦ ë“±ì— í•„ìˆ˜ì ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.

### 1.2 ì ìš© ì‹œê¸°
- ë‘ ê·¸ë£¹ ê°„ ì°¨ì´ê°€ í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œì§€ í™•ì¸í•  ë•Œ
- A/B í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„ (ì‹ ê·œ ê¸°ëŠ¥ íš¨ê³¼ ê²€ì¦)
- ì •ì±…/ì „ëµ ë³€ê²½ ì „í›„ ë¹„êµ
- ì œí’ˆ/ì„œë¹„ìŠ¤ ê°œì„  íš¨ê³¼ ì¸¡ì •
- ê³¼í•™ì  ì£¼ì¥ì˜ í†µê³„ì  ê·¼ê±° ì œì‹œ

### 1.3 ì£¼ìš” ê²€ì • ë°©ë²•

**ëª¨ìˆ˜ ê²€ì • (Parametric Tests)**:
- **t-test**: í‰ê·  ë¹„êµ (2ê·¸ë£¹)
- **ANOVA**: í‰ê·  ë¹„êµ (3ê·¸ë£¹ ì´ìƒ)
- **Z-test**: ëŒ€í‘œë³¸ í‰ê·  ë¹„êµ
- **F-test**: ë¶„ì‚° ë¹„êµ

**ë¹„ëª¨ìˆ˜ ê²€ì • (Non-parametric Tests)**:
- **Mann-Whitney U**: t-test ëŒ€ì²´ (ë¹„ì •ê·œ)
- **Kruskal-Wallis H**: ANOVA ëŒ€ì²´ (ë¹„ì •ê·œ)
- **Wilcoxon Signed-Rank**: ëŒ€ì‘í‘œë³¸ t-test ëŒ€ì²´
- **Chi-square**: ë²”ì£¼í˜• ë³€ìˆ˜ ë…ë¦½ì„± ê²€ì •

---

## 2. ì´ë¡ ì  ë°°ê²½

### 2.1 ê°€ì„¤ ê²€ì •ì˜ ê¸°ë³¸ ê°œë…

#### ê·€ë¬´ê°€ì„¤ vs ëŒ€ë¦½ê°€ì„¤
```
ê·€ë¬´ê°€ì„¤ (Hâ‚€): "ì°¨ì´ê°€ ì—†ë‹¤", "íš¨ê³¼ê°€ ì—†ë‹¤"
ëŒ€ë¦½ê°€ì„¤ (Hâ‚): "ì°¨ì´ê°€ ìˆë‹¤", "íš¨ê³¼ê°€ ìˆë‹¤"

ì˜ˆì‹œ 1: ì‹ ì•½ íš¨ê³¼ ê²€ì¦
Hâ‚€: ì‹ ì•½ê³¼ ìœ„ì•½ì˜ íš¨ê³¼ê°€ ê°™ë‹¤ (Î¼â‚ = Î¼â‚‚)
Hâ‚: ì‹ ì•½ì˜ íš¨ê³¼ê°€ ìœ„ì•½ë³´ë‹¤ í¬ë‹¤ (Î¼â‚ > Î¼â‚‚)

ì˜ˆì‹œ 2: ì›¹ì‚¬ì´íŠ¸ A/B í…ŒìŠ¤íŠ¸
Hâ‚€: ì‹ ê·œ ë””ìì¸ê³¼ ê¸°ì¡´ ë””ìì¸ì˜ ì „í™˜ìœ¨ì´ ê°™ë‹¤
Hâ‚: ì‹ ê·œ ë””ìì¸ì˜ ì „í™˜ìœ¨ì´ ë” ë†’ë‹¤
```

#### P-value (ìœ ì˜í™•ë¥ )
```
ì •ì˜: ê·€ë¬´ê°€ì„¤ì´ ì°¸ì¼ ë•Œ, ê´€ì¸¡ëœ ê²°ê³¼(ë˜ëŠ” ë” ê·¹ë‹¨ì ì¸ ê²°ê³¼)ê°€ ë‚˜ì˜¬ í™•ë¥ 

í•´ì„:
- p < 0.05: ê·€ë¬´ê°€ì„¤ ê¸°ê° (í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ ì°¨ì´)
- p â‰¥ 0.05: ê·€ë¬´ê°€ì„¤ ì±„íƒ (ì°¨ì´ ì—†ìŒ)

ì£¼ì˜:
- p-valueëŠ” íš¨ê³¼ í¬ê¸°ê°€ ì•„ë‹˜
- p < 0.05ëŠ” ì„ì˜ì˜ ê¸°ì¤€ (ë„ë©”ì¸ì— ë”°ë¼ ì¡°ì • ê°€ëŠ¥)
- "ìœ ì˜í•˜ë‹¤" â‰  "ì¤‘ìš”í•˜ë‹¤" (practical significance)
```

#### ì œ1ì¢… ì˜¤ë¥˜ vs ì œ2ì¢… ì˜¤ë¥˜
```
                ì‹¤ì œ Hâ‚€ ì°¸    ì‹¤ì œ Hâ‚€ ê±°ì§“
Hâ‚€ ê¸°ê°         ì œ1ì¢… ì˜¤ë¥˜    ì˜¬ë°”ë¥¸ ê²°ì •
                (Î±)          (1-Î²: ê²€ì •ë ¥)
Hâ‚€ ì±„íƒ         ì˜¬ë°”ë¥¸ ê²°ì •   ì œ2ì¢… ì˜¤ë¥˜
                             (Î²)

ì œ1ì¢… ì˜¤ë¥˜ (Î±): "ê±°ì§“ ì–‘ì„±" (False Positive)
- ì‹¤ì œë¡œ ì°¨ì´ê°€ ì—†ëŠ”ë° ìˆë‹¤ê³  íŒë‹¨
- ë³´í†µ Î± = 0.05ë¡œ ì„¤ì •

ì œ2ì¢… ì˜¤ë¥˜ (Î²): "ê±°ì§“ ìŒì„±" (False Negative)
- ì‹¤ì œë¡œ ì°¨ì´ê°€ ìˆëŠ”ë° ì—†ë‹¤ê³  íŒë‹¨
- ê²€ì •ë ¥ = 1 - Î² (ë³´í†µ 0.8 ì´ìƒ ëª©í‘œ)
```

### 2.2 ê²€ì • ë°©ë²• ì„ íƒ ê°€ì´ë“œ

```
                    ì •ê·œì„± ë§Œì¡±?
                   /           \
                 Yes            No
                 /               \
          ëª¨ìˆ˜ ê²€ì •           ë¹„ëª¨ìˆ˜ ê²€ì •
         /    |    \          /    |    \
      t-test ANOVA F-test  Mann-W K-W  Wilcoxon

ì¶”ê°€ ê³ ë ¤ì‚¬í•­:
1. ìƒ˜í”Œ í¬ê¸°
   - n < 30: ì •ê·œì„± ì¤‘ìš” (ë¹„ëª¨ìˆ˜ ê³ ë ¤)
   - n â‰¥ 30: ì¤‘ì‹¬ê·¹í•œì •ë¦¬ë¡œ ì •ê·œì„± ì™„í™”
2. ë¶„ì‚° ë™ì§ˆì„±
   - ë§Œì¡±: Student's t-test
   - ë¶ˆë§Œì¡±: Welch's t-test
3. ëŒ€ì‘ ì—¬ë¶€
   - ë…ë¦½í‘œë³¸: Independent t-test
   - ëŒ€ì‘í‘œë³¸: Paired t-test
```

### 2.3 ì‹œë‚˜ë¦¬ì˜¤

**ì‹œë‚˜ë¦¬ì˜¤ 1: ì‹ ê·œ ê¸°ëŠ¥ A/B í…ŒìŠ¤íŠ¸**
```
ìƒí™©: ì›¹ì‚¬ì´íŠ¸ ê²°ì œ ë²„íŠ¼ ìƒ‰ìƒ ë³€ê²½
- Aê·¸ë£¹ (ê¸°ì¡´): íŒŒë€ìƒ‰ ë²„íŠ¼, 1000ëª…, ì „í™˜ìœ¨ 5.2%
- Bê·¸ë£¹ (ì‹ ê·œ): ë¹¨ê°„ìƒ‰ ë²„íŠ¼, 1000ëª…, ì „í™˜ìœ¨ 6.1%

ì§ˆë¬¸: ì°¨ì´ê°€ í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œê°€?

ë¶„ì„:
1. ê°€ì„¤ ì„¤ì •
   Hâ‚€: p_A = p_B (ì „í™˜ìœ¨ ë™ì¼)
   Hâ‚: p_A â‰  p_B (ì „í™˜ìœ¨ ë‹¤ë¦„)
2. ê²€ì • ì„ íƒ: Two-proportion z-test
3. ê²°ê³¼: p = 0.032 < 0.05
4. ê²°ë¡ : ìœ ì˜ë¯¸í•œ ì°¨ì´ â†’ ë¹¨ê°„ìƒ‰ ë²„íŠ¼ ì±„íƒ

ì•¡ì…˜: ì „ì²´ ì‚¬ìš©ìì—ê²Œ ë¹¨ê°„ìƒ‰ ë²„íŠ¼ ì ìš©
```

**ì‹œë‚˜ë¦¬ì˜¤ 2: ì•½ë¬¼ íš¨ëŠ¥ ë¹„êµ (3ê·¸ë£¹)**
```
ìƒí™©: ì‹ ì•½ A, ì‹ ì•½ B, ìœ„ì•½ ë¹„êµ
- ê° ê·¸ë£¹ 30ëª…, í˜ˆì•• ê°ì†ŒëŸ‰ ì¸¡ì •

ë¶„ì„:
1. ê°€ì„¤
   Hâ‚€: Î¼_A = Î¼_B = Î¼_ìœ„ì•½
   Hâ‚: ì ì–´ë„ í•˜ë‚˜ëŠ” ë‹¤ë¦„
2. ê²€ì •: One-way ANOVA
3. ê²°ê³¼: F = 8.5, p = 0.001
4. Post-hoc: Tukey HSD
   - A vs ìœ„ì•½: p < 0.001 (ìœ ì˜)
   - B vs ìœ„ì•½: p = 0.023 (ìœ ì˜)
   - A vs B: p = 0.412 (ë¹„ìœ ì˜)

ê²°ë¡ : ë‘ ì‹ ì•½ ëª¨ë‘ íš¨ê³¼ ìˆìŒ, ì„œë¡œ ë¹„ìŠ·
```

**ì‹œë‚˜ë¦¬ì˜¤ 3: ì •ê·œì„± ë¶ˆë§Œì¡± (ë¹„ëª¨ìˆ˜ ê²€ì •)**
```
ìƒí™©: ê³ ê° ë§Œì¡±ë„ (1-5ì  ì²™ë„)
- ê¸°ì¡´ ì„œë¹„ìŠ¤: [3, 4, 3, 5, 4, 3, ...]
- ê°œì„  ì„œë¹„ìŠ¤: [4, 5, 4, 5, 5, 4, ...]

ë¬¸ì œ: ì •ê·œì„± ë¶ˆë§Œì¡± (Shapiro-Wilk p = 0.003)

í•´ê²°:
1. t-test ëŒ€ì‹  Mann-Whitney U test ì‚¬ìš©
2. ê²°ê³¼: U = 450, p = 0.018
3. ê²°ë¡ : ê°œì„  ì„œë¹„ìŠ¤ê°€ ìœ ì˜ë¯¸í•˜ê²Œ ë†’ì€ ë§Œì¡±ë„
```

---

## 3. êµ¬í˜„

### 3.1 í™˜ê²½ ì„¤ì •

```python
# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from scipy.stats import (
    ttest_ind, ttest_rel, ttest_1samp,
    f_oneway, kruskal,
    mannwhitneyu, wilcoxon,
    chi2_contingency, fisher_exact,
    shapiro, normaltest, levene,
    pearsonr, spearmanr
)
import warnings
warnings.filterwarnings('ignore')

# í†µê³„ ëª¨ë¸ë§
import statsmodels.api as sm
from statsmodels.stats.multicomp import pairwise_tukeyhsd
from statsmodels.stats.proportion import proportions_ztest
from statsmodels.stats.power import TTestIndPower, FTestAnovaPower

# ì‹œê°í™”
plt.style.use('seaborn-v0_8-whitegrid')
sns.set_palette("Set2")
%matplotlib inline

# í•œê¸€ í°íŠ¸ (ì„ íƒ)
plt.rcParams['font.family'] = 'AppleGothic'  # Mac
plt.rcParams['axes.unicode_minus'] = False

# ì¶œë ¥ ì˜µì…˜
np.set_printoptions(precision=4, suppress=True)
pd.set_option('display.precision', 4)
```

### 3.2 ìƒ˜í”Œ ë°ì´í„° ìƒì„±

```python
def generate_ab_test_data(n_per_group=500, effect_size=0.3, seed=42):
    """
    A/B í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë°ì´í„° ìƒì„±
    
    Parameters:
    -----------
    n_per_group : int
        ê° ê·¸ë£¹ ìƒ˜í”Œ í¬ê¸°
    effect_size : float
        íš¨ê³¼ í¬ê¸° (Cohen's d)
    seed : int
    
    Returns:
    --------
    df : DataFrame
    """
    np.random.seed(seed)
    
    # ê·¸ë£¹ A (Control)
    group_a = np.random.normal(loc=100, scale=15, size=n_per_group)
    
    # ê·¸ë£¹ B (Treatment) - effect_sizeë§Œí¼ í‰ê·  ì¦ê°€
    mean_increase = effect_size * 15  # effect_size * std
    group_b = np.random.normal(loc=100 + mean_increase, scale=15, size=n_per_group)
    
    # DataFrame ìƒì„±
    df = pd.DataFrame({
        'group': ['A'] * n_per_group + ['B'] * n_per_group,
        'value': np.concatenate([group_a, group_b])
    })
    
    print(f"=" * 70)
    print(f"ğŸ“Š A/B í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±")
    print(f"=" * 70)
    print(f"ê·¸ë£¹ A (Control):  n={n_per_group}, Î¼={group_a.mean():.2f}, Ïƒ={group_a.std():.2f}")
    print(f"ê·¸ë£¹ B (Treatment): n={n_per_group}, Î¼={group_b.mean():.2f}, Ïƒ={group_b.std():.2f}")
    print(f"Effect Size (Cohen's d): {effect_size}")
    print(f"\nê¸°ìˆ  í†µê³„:")
    print(df.groupby('group')['value'].describe())
    
    return df

# ë°ì´í„° ìƒì„±
df_ab = generate_ab_test_data(n_per_group=500, effect_size=0.3)
```

### 3.3 ì •ê·œì„± ê²€ì • (Normality Test)

```python
def check_normality(data, alpha=0.05):
    """
    ì •ê·œì„± ê²€ì • (Shapiro-Wilk, Anderson-Darling, Kolmogorov-Smirnov)
    
    Parameters:
    -----------
    data : array-like
        ê²€ì •í•  ë°ì´í„°
    alpha : float
        ìœ ì˜ìˆ˜ì¤€
    
    Returns:
    --------
    is_normal : bool
    """
    print(f"\n" + "=" * 70)
    print(f"ğŸ“Š ì •ê·œì„± ê²€ì • (Normality Tests)")
    print(f"=" * 70)
    print(f"ìƒ˜í”Œ í¬ê¸°: {len(data)}")
    print(f"í‰ê· : {data.mean():.4f}, í‘œì¤€í¸ì°¨: {data.std():.4f}")
    
    # 1. Shapiro-Wilk test (n < 5000)
    if len(data) <= 5000:
        stat_sw, p_sw = shapiro(data)
        print(f"\n1. Shapiro-Wilk Test:")
        print(f"   í†µê³„ëŸ‰: {stat_sw:.4f}")
        print(f"   P-value: {p_sw:.4f}")
        if p_sw > alpha:
            print(f"   ê²°ë¡ : ì •ê·œë¶„í¬ë¥¼ ë”°ë¦„ (p > {alpha})")
        else:
            print(f"   ê²°ë¡ : ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ì§€ ì•ŠìŒ (p â‰¤ {alpha})")
    
    # 2. Anderson-Darling test
    result_ad = stats.anderson(data, dist='norm')
    print(f"\n2. Anderson-Darling Test:")
    print(f"   í†µê³„ëŸ‰: {result_ad.statistic:.4f}")
    print(f"   Critical values: {result_ad.critical_values}")
    print(f"   Significance levels: {result_ad.significance_level}%")
    
    # 3. Kolmogorov-Smirnov test
    stat_ks, p_ks = stats.kstest(data, 'norm', args=(data.mean(), data.std()))
    print(f"\n3. Kolmogorov-Smirnov Test:")
    print(f"   í†µê³„ëŸ‰: {stat_ks:.4f}")
    print(f"   P-value: {p_ks:.4f}")
    if p_ks > alpha:
        print(f"   ê²°ë¡ : ì •ê·œë¶„í¬ë¥¼ ë”°ë¦„ (p > {alpha})")
    else:
        print(f"   ê²°ë¡ : ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ì§€ ì•ŠìŒ (p â‰¤ {alpha})")
    
    # ì‹œê°í™”
    fig, axes = plt.subplots(1, 3, figsize=(16, 4))
    
    # íˆìŠ¤í† ê·¸ë¨ + ì •ê·œë¶„í¬ ê³¡ì„ 
    axes[0].hist(data, bins=30, density=True, alpha=0.7, edgecolor='black')
    mu, sigma = data.mean(), data.std()
    x = np.linspace(mu - 4*sigma, mu + 4*sigma, 100)
    axes[0].plot(x, stats.norm.pdf(x, mu, sigma), 'r-', linewidth=2, label='Normal Dist')
    axes[0].set_xlabel('Value', fontsize=11)
    axes[0].set_ylabel('Density', fontsize=11)
    axes[0].set_title('Histogram + Normal Curve', fontsize=12)
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)
    
    # Q-Q Plot
    stats.probplot(data, dist="norm", plot=axes[1])
    axes[1].set_title('Q-Q Plot', fontsize=12)
    axes[1].grid(True, alpha=0.3)
    
    # Boxplot
    axes[2].boxplot(data, vert=True)
    axes[2].set_ylabel('Value', fontsize=11)
    axes[2].set_title('Boxplot', fontsize=12)
    axes[2].grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    plt.show()
    
    # ì¢…í•© ê²°ë¡ 
    print(f"\nğŸ’¡ ì¢…í•© ê²°ë¡ :")
    if len(data) <= 5000:
        is_normal = p_sw > alpha
    else:
        is_normal = p_ks > alpha
    
    if is_normal:
        print(f"  âœ… ë°ì´í„°ê°€ ì •ê·œë¶„í¬ë¥¼ ë”°ë¦„ â†’ ëª¨ìˆ˜ ê²€ì • (t-test, ANOVA) ì‚¬ìš© ê°€ëŠ¥")
    else:
        print(f"  âš ï¸  ë°ì´í„°ê°€ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ì§€ ì•ŠìŒ â†’ ë¹„ëª¨ìˆ˜ ê²€ì • (Mann-Whitney, Kruskal-Wallis) ê¶Œì¥")
    
    return is_normal

# ì •ê·œì„± ê²€ì •
group_a_data = df_ab[df_ab['group'] == 'A']['value']
is_normal = check_normality(group_a_data, alpha=0.05)
```

### 3.4 ë…ë¦½í‘œë³¸ t-test (Independent t-test)

```python
def perform_independent_ttest(group1, group2, alpha=0.05, equal_var=True):
    """
    ë…ë¦½í‘œë³¸ t-test ìˆ˜í–‰
    
    Parameters:
    -----------
    group1, group2 : array-like
        ë¹„êµí•  ë‘ ê·¸ë£¹ ë°ì´í„°
    alpha : float
        ìœ ì˜ìˆ˜ì¤€
    equal_var : bool
        ë¶„ì‚° ë™ì§ˆì„± ê°€ì • (True: Student's, False: Welch's)
    
    Returns:
    --------
    result : dict
    """
    print(f"\n" + "=" * 70)
    print(f"ğŸ“Š ë…ë¦½í‘œë³¸ t-test (Independent Samples t-test)")
    print(f"=" * 70)
    
    # ê¸°ìˆ  í†µê³„
    print(f"\nê¸°ìˆ  í†µê³„:")
    print(f"  Group 1: n={len(group1)}, Î¼={group1.mean():.4f}, Ïƒ={group1.std():.4f}")
    print(f"  Group 2: n={len(group2)}, Î¼={group2.mean():.4f}, Ïƒ={group2.std():.4f}")
    print(f"  í‰ê·  ì°¨ì´: {group2.mean() - group1.mean():.4f}")
    
    # ë“±ë¶„ì‚° ê²€ì • (Levene's test)
    stat_levene, p_levene = levene(group1, group2)
    print(f"\në“±ë¶„ì‚° ê²€ì • (Levene's test):")
    print(f"  í†µê³„ëŸ‰: {stat_levene:.4f}, P-value: {p_levene:.4f}")
    if p_levene > alpha:
        print(f"  ê²°ë¡ : ë“±ë¶„ì‚° ê°€ì • ë§Œì¡± (p > {alpha}) â†’ Student's t-test")
        equal_var = True
    else:
        print(f"  ê²°ë¡ : ë“±ë¶„ì‚° ê°€ì • ë¶ˆë§Œì¡± (p â‰¤ {alpha}) â†’ Welch's t-test")
        equal_var = False
    
    # t-test ìˆ˜í–‰
    t_stat, p_value = ttest_ind(group1, group2, equal_var=equal_var)
    
    # ììœ ë„
    if equal_var:
        df = len(group1) + len(group2) - 2
    else:
        # Welch-Satterthwaite equation
        s1, s2 = group1.std(ddof=1), group2.std(ddof=1)
        n1, n2 = len(group1), len(group2)
        df = ((s1**2/n1 + s2**2/n2)**2) / ((s1**2/n1)**2/(n1-1) + (s2**2/n2)**2/(n2-1))
    
    # íš¨ê³¼ í¬ê¸° (Cohen's d)
    pooled_std = np.sqrt(((len(group1)-1)*group1.std()**2 + (len(group2)-1)*group2.std()**2) / (len(group1)+len(group2)-2))
    cohens_d = (group2.mean() - group1.mean()) / pooled_std
    
    # ì‹ ë¢°êµ¬ê°„
    diff_mean = group2.mean() - group1.mean()
    se = np.sqrt(group1.var()/len(group1) + group2.var()/len(group2))
    t_crit = stats.t.ppf(1 - alpha/2, df)
    ci_lower = diff_mean - t_crit * se
    ci_upper = diff_mean + t_crit * se
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"\nt-test ê²°ê³¼:")
    print(f"  Hâ‚€: Î¼â‚ = Î¼â‚‚ (ë‘ ê·¸ë£¹ì˜ í‰ê· ì´ ê°™ë‹¤)")
    print(f"  Hâ‚: Î¼â‚ â‰  Î¼â‚‚ (ë‘ ê·¸ë£¹ì˜ í‰ê· ì´ ë‹¤ë¥´ë‹¤)")
    print(f"\n  t-í†µê³„ëŸ‰: {t_stat:.4f}")
    print(f"  ììœ ë„: {df:.2f}")
    print(f"  P-value: {p_value:.4f}")
    print(f"  ìœ ì˜ìˆ˜ì¤€: {alpha}")
    
    if p_value < alpha:
        print(f"\n  âœ… ê²°ë¡ : ê·€ë¬´ê°€ì„¤ ê¸°ê° (p < {alpha})")
        print(f"     â†’ ë‘ ê·¸ë£¹ ê°„ í‰ê· ì— ìœ ì˜ë¯¸í•œ ì°¨ì´ê°€ ìˆìŒ")
    else:
        print(f"\n  âŒ ê²°ë¡ : ê·€ë¬´ê°€ì„¤ ì±„íƒ (p â‰¥ {alpha})")
        print(f"     â†’ ë‘ ê·¸ë£¹ ê°„ í‰ê· ì— ìœ ì˜ë¯¸í•œ ì°¨ì´ê°€ ì—†ìŒ")
    
    print(f"\níš¨ê³¼ í¬ê¸° (Cohen's d): {cohens_d:.4f}")
    if abs(cohens_d) < 0.2:
        effect_size_interp = "ì‘ìŒ (small)"
    elif abs(cohens_d) < 0.5:
        effect_size_interp = "ì¤‘ê°„ (medium)"
    elif abs(cohens_d) < 0.8:
        effect_size_interp = "í¼ (large)"
    else:
        effect_size_interp = "ë§¤ìš° í¼ (very large)"
    print(f"  í•´ì„: {effect_size_interp}")
    
    print(f"\ní‰ê·  ì°¨ì´ì˜ 95% ì‹ ë¢°êµ¬ê°„: [{ci_lower:.4f}, {ci_upper:.4f}]")
    
    # ì‹œê°í™”
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # ë°•ìŠ¤í”Œë¡¯
    data_plot = pd.DataFrame({
        'Group': ['Group 1']*len(group1) + ['Group 2']*len(group2),
        'Value': np.concatenate([group1, group2])
    })
    sns.boxplot(data=data_plot, x='Group', y='Value', ax=axes[0], palette='Set2')
    axes[0].set_title(f't-test: p-value = {p_value:.4f}', fontsize=12)
    axes[0].set_ylabel('Value', fontsize=11)
    axes[0].grid(True, alpha=0.3, axis='y')
    
    # ë¶„í¬ ë¹„êµ
    axes[1].hist(group1, bins=30, alpha=0.6, label='Group 1', density=True)
    axes[1].hist(group2, bins=30, alpha=0.6, label='Group 2', density=True)
    axes[1].axvline(group1.mean(), color='blue', linestyle='--', linewidth=2, label=f'Î¼â‚={group1.mean():.2f}')
    axes[1].axvline(group2.mean(), color='orange', linestyle='--', linewidth=2, label=f'Î¼â‚‚={group2.mean():.2f}')
    axes[1].set_xlabel('Value', fontsize=11)
    axes[1].set_ylabel('Density', fontsize=11)
    axes[1].set_title('Distribution Comparison', fontsize=12)
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    return {
        't_statistic': t_stat,
        'p_value': p_value,
        'df': df,
        'cohens_d': cohens_d,
        'ci': (ci_lower, ci_upper),
        'significant': p_value < alpha
    }

# ë…ë¦½í‘œë³¸ t-test ìˆ˜í–‰
group_a = df_ab[df_ab['group'] == 'A']['value'].values
group_b = df_ab[df_ab['group'] == 'B']['value'].values
ttest_result = perform_independent_ttest(group_a, group_b, alpha=0.05)
```

### 3.5 ëŒ€ì‘í‘œë³¸ t-test (Paired t-test)

```python
def perform_paired_ttest(before, after, alpha=0.05):
    """
    ëŒ€ì‘í‘œë³¸ t-test ìˆ˜í–‰ (ì „í›„ ë¹„êµ)
    
    Parameters:
    -----------
    before, after : array-like
        ì „í›„ ë°ì´í„° (ê°™ì€ ê°œì²´)
    alpha : float
    
    Returns:
    --------
    result : dict
    """
    print(f"\n" + "=" * 70)
    print(f"ğŸ“Š ëŒ€ì‘í‘œë³¸ t-test (Paired Samples t-test)")
    print(f"=" * 70)
    
    # ì°¨ì´ ê³„ì‚°
    diff = after - before
    
    # ê¸°ìˆ  í†µê³„
    print(f"\nê¸°ìˆ  í†µê³„:")
    print(f"  Before: n={len(before)}, Î¼={before.mean():.4f}, Ïƒ={before.std():.4f}")
    print(f"  After:  n={len(after)}, Î¼={after.mean():.4f}, Ïƒ={after.std():.4f}")
    print(f"  Difference: Î¼_diff={diff.mean():.4f}, Ïƒ_diff={diff.std():.4f}")
    
    # Paired t-test ìˆ˜í–‰
    t_stat, p_value = ttest_rel(before, after)
    
    # ììœ ë„
    df = len(before) - 1
    
    # íš¨ê³¼ í¬ê¸° (Cohen's d for paired)
    cohens_d = diff.mean() / diff.std()
    
    # ì‹ ë¢°êµ¬ê°„
    se = diff.std() / np.sqrt(len(diff))
    t_crit = stats.t.ppf(1 - alpha/2, df)
    ci_lower = diff.mean() - t_crit * se
    ci_upper = diff.mean() + t_crit * se
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"\nPaired t-test ê²°ê³¼:")
    print(f"  Hâ‚€: Î¼_diff = 0 (ì „í›„ ì°¨ì´ê°€ ì—†ë‹¤)")
    print(f"  Hâ‚: Î¼_diff â‰  0 (ì „í›„ ì°¨ì´ê°€ ìˆë‹¤)")
    print(f"\n  t-í†µê³„ëŸ‰: {t_stat:.4f}")
    print(f"  ììœ ë„: {df}")
    print(f"  P-value: {p_value:.4f}")
    
    if p_value < alpha:
        print(f"\n  âœ… ê²°ë¡ : ê·€ë¬´ê°€ì„¤ ê¸°ê° (p < {alpha})")
        print(f"     â†’ ì „í›„ ìœ ì˜ë¯¸í•œ ë³€í™”ê°€ ìˆìŒ")
        if diff.mean() > 0:
            print(f"     â†’ Afterê°€ Beforeë³´ë‹¤ ìœ ì˜ë¯¸í•˜ê²Œ ë†’ìŒ")
        else:
            print(f"     â†’ Afterê°€ Beforeë³´ë‹¤ ìœ ì˜ë¯¸í•˜ê²Œ ë‚®ìŒ")
    else:
        print(f"\n  âŒ ê²°ë¡ : ê·€ë¬´ê°€ì„¤ ì±„íƒ (p â‰¥ {alpha})")
        print(f"     â†’ ì „í›„ ìœ ì˜ë¯¸í•œ ë³€í™”ê°€ ì—†ìŒ")
    
    print(f"\níš¨ê³¼ í¬ê¸° (Cohen's d): {cohens_d:.4f}")
    print(f"í‰ê·  ì°¨ì´ì˜ 95% ì‹ ë¢°êµ¬ê°„: [{ci_lower:.4f}, {ci_upper:.4f}]")
    
    # ì‹œê°í™”
    fig, axes = plt.subplots(1, 3, figsize=(16, 4))
    
    # ì „í›„ ë¹„êµ (ì—°ê²°ì„ )
    for i in range(min(50, len(before))):  # ìµœëŒ€ 50ê°œë§Œ í‘œì‹œ
        axes[0].plot([0, 1], [before[i], after[i]], 'o-', alpha=0.3, color='gray')
    axes[0].plot([0, 1], [before.mean(), after.mean()], 'ro-', linewidth=3, markersize=10, label='Mean')
    axes[0].set_xticks([0, 1])
    axes[0].set_xticklabels(['Before', 'After'])
    axes[0].set_ylabel('Value', fontsize=11)
    axes[0].set_title('Before vs After', fontsize=12)
    axes[0].legend()
    axes[0].grid(True, alpha=0.3, axis='y')
    
    # ì°¨ì´ ë¶„í¬
    axes[1].hist(diff, bins=30, alpha=0.7, edgecolor='black')
    axes[1].axvline(0, color='red', linestyle='--', linewidth=2, label='Î¼_diff=0')
    axes[1].axvline(diff.mean(), color='green', linestyle='--', linewidth=2, label=f'Î¼_diff={diff.mean():.2f}')
    axes[1].set_xlabel('Difference (After - Before)', fontsize=11)
    axes[1].set_ylabel('Frequency', fontsize=11)
    axes[1].set_title('Distribution of Differences', fontsize=12)
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)
    
    # Boxplot
    data_plot = pd.DataFrame({
        'Condition': ['Before']*len(before) + ['After']*len(after),
        'Value': np.concatenate([before, after])
    })
    sns.boxplot(data=data_plot, x='Condition', y='Value', ax=axes[2], palette='Set2')
    axes[2].set_title(f'Paired t-test: p-value = {p_value:.4f}', fontsize=12)
    axes[2].grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    plt.show()
    
    return {
        't_statistic': t_stat,
        'p_value': p_value,
        'df': df,
        'cohens_d': cohens_d,
        'mean_diff': diff.mean(),
        'ci': (ci_lower, ci_upper),
        'significant': p_value < alpha
    }

# ëŒ€ì‘í‘œë³¸ ë°ì´í„° ìƒì„± ë° ê²€ì •
np.random.seed(42)
before_treatment = np.random.normal(100, 15, 200)
after_treatment = before_treatment + np.random.normal(5, 10, 200)  # í‰ê·  5 ì¦ê°€

paired_result = perform_paired_ttest(before_treatment, after_treatment, alpha=0.05)
```

### 3.6 ANOVA (Analysis of Variance) - 3ê·¸ë£¹ ì´ìƒ

```python
def perform_anova(groups, group_names, alpha=0.05):
    """
    ì¼ì› ë¶„ì‚°ë¶„ì„ (One-way ANOVA)
    
    Parameters:
    -----------
    groups : list of arrays
        ë¹„êµí•  ê·¸ë£¹ë“¤ (3ê°œ ì´ìƒ)
    group_names : list of str
        ê·¸ë£¹ ì´ë¦„
    alpha : float
    
    Returns:
    --------
    result : dict
    """
    print(f"\n" + "=" * 70)
    print(f"ğŸ“Š ì¼ì› ë¶„ì‚°ë¶„ì„ (One-way ANOVA)")
    print(f"=" * 70)
    
    # ê¸°ìˆ  í†µê³„
    print(f"\nê¸°ìˆ  í†µê³„:")
    for i, (group, name) in enumerate(zip(groups, group_names)):
        print(f"  {name}: n={len(group)}, Î¼={group.mean():.4f}, Ïƒ={group.std():.4f}")
    
    # ANOVA ìˆ˜í–‰
    f_stat, p_value = f_oneway(*groups)
    
    # ììœ ë„
    k = len(groups)  # ê·¸ë£¹ ìˆ˜
    n = sum([len(g) for g in groups])  # ì´ ìƒ˜í”Œ ìˆ˜
    df_between = k - 1
    df_within = n - k
    
    # íš¨ê³¼ í¬ê¸° (Eta-squared)
    grand_mean = np.mean(np.concatenate(groups))
    ss_between = sum([len(g) * (g.mean() - grand_mean)**2 for g in groups])
    ss_within = sum([((g - g.mean())**2).sum() for g in groups])
    ss_total = ss_between + ss_within
    eta_squared = ss_between / ss_total
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"\nANOVA ê²°ê³¼:")
    print(f"  Hâ‚€: Î¼â‚ = Î¼â‚‚ = ... = Î¼â‚– (ëª¨ë“  ê·¸ë£¹ì˜ í‰ê· ì´ ê°™ë‹¤)")
    print(f"  Hâ‚: ì ì–´ë„ í•˜ë‚˜ì˜ ê·¸ë£¹ í‰ê· ì´ ë‹¤ë¥´ë‹¤")
    print(f"\n  F-í†µê³„ëŸ‰: {f_stat:.4f}")
    print(f"  ììœ ë„ (between): {df_between}")
    print(f"  ììœ ë„ (within): {df_within}")
    print(f"  P-value: {p_value:.4f}")
    
    if p_value < alpha:
        print(f"\n  âœ… ê²°ë¡ : ê·€ë¬´ê°€ì„¤ ê¸°ê° (p < {alpha})")
        print(f"     â†’ ì ì–´ë„ í•˜ë‚˜ì˜ ê·¸ë£¹ í‰ê· ì´ ë‹¤ë¦„")
        print(f"     â†’ Post-hoc ê²€ì • í•„ìš” (Tukey HSD)")
    else:
        print(f"\n  âŒ ê²°ë¡ : ê·€ë¬´ê°€ì„¤ ì±„íƒ (p â‰¥ {alpha})")
        print(f"     â†’ ëª¨ë“  ê·¸ë£¹ì˜ í‰ê· ì´ ê°™ìŒ")
    
    print(f"\níš¨ê³¼ í¬ê¸° (Eta-squared): {eta_squared:.4f}")
    if eta_squared < 0.01:
        effect_size_interp = "ì‘ìŒ"
    elif eta_squared < 0.06:
        effect_size_interp = "ì¤‘ê°„"
    else:
        effect_size_interp = "í¼"
    print(f"  í•´ì„: {effect_size_interp}")
    
    # ì‹œê°í™”
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # ë°•ìŠ¤í”Œë¡¯
    data_plot = pd.DataFrame({
        'Group': np.concatenate([[name]*len(g) for name, g in zip(group_names, groups)]),
        'Value': np.concatenate(groups)
    })
    sns.boxplot(data=data_plot, x='Group', y='Value', ax=axes[0], palette='Set3')
    axes[0].set_title(f'ANOVA: F={f_stat:.2f}, p-value={p_value:.4f}', fontsize=12)
    axes[0].set_ylabel('Value', fontsize=11)
    axes[0].grid(True, alpha=0.3, axis='y')
    
    # í‰ê·  ë¹„êµ
    means = [g.mean() for g in groups]
    sems = [g.std() / np.sqrt(len(g)) for g in groups]
    axes[1].bar(group_names, means, yerr=sems, alpha=0.7, capsize=10, edgecolor='black')
    axes[1].set_ylabel('Mean Value', fontsize=11)
    axes[1].set_title('Group Means (Â±SEM)', fontsize=12)
    axes[1].grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    plt.show()
    
    # Post-hoc ê²€ì • (ANOVA ìœ ì˜ ì‹œ)
    if p_value < alpha:
        perform_posthoc_tukey(groups, group_names, alpha)
    
    return {
        'f_statistic': f_stat,
        'p_value': p_value,
        'df_between': df_between,
        'df_within': df_within,
        'eta_squared': eta_squared,
        'significant': p_value < alpha
    }

def perform_posthoc_tukey(groups, group_names, alpha=0.05):
    """
    Tukey HSD post-hoc test
    """
    print(f"\n" + "=" * 70)
    print(f"ğŸ“Š Post-hoc: Tukey HSD")
    print(f"=" * 70)
    
    # ë°ì´í„° ì¤€ë¹„
    data_all = np.concatenate(groups)
    labels_all = np.concatenate([[name]*len(g) for name, g in zip(group_names, groups)])
    
    # Tukey HSD
    tukey_result = pairwise_tukeyhsd(data_all, labels_all, alpha=alpha)
    
    print(tukey_result)
    
    # ì‹œê°í™”
    plt.figure(figsize=(8, 6))
    tukey_result.plot_simultaneous(ylabel='Group', xlabel='Mean Value')
    plt.title('Tukey HSD: 95% Confidence Intervals', fontsize=12)
    plt.tight_layout()
    plt.show()
    
    print(f"\nğŸ’¡ í•´ì„:")
    print(f"  - reject=True: ë‘ ê·¸ë£¹ ê°„ ìœ ì˜ë¯¸í•œ ì°¨ì´")
    print(f"  - ì‹ ë¢°êµ¬ê°„ì´ 0ì„ í¬í•¨í•˜ì§€ ì•Šìœ¼ë©´ ìœ ì˜ë¯¸í•œ ì°¨ì´")

# 3ê·¸ë£¹ ë°ì´í„° ìƒì„± ë° ANOVA
np.random.seed(42)
group1 = np.random.normal(100, 15, 150)
group2 = np.random.normal(105, 15, 150)
group3 = np.random.normal(110, 15, 150)

anova_result = perform_anova(
    [group1, group2, group3],
    ['Control', 'Treatment A', 'Treatment B'],
    alpha=0.05
)
```

### 3.7 Mann-Whitney U Test (ë¹„ëª¨ìˆ˜ t-test ëŒ€ì²´)

```python
def perform_mann_whitney(group1, group2, alpha=0.05):
    """
    Mann-Whitney U test (ë¹„ëª¨ìˆ˜ ë…ë¦½í‘œë³¸ ê²€ì •)
    
    ì •ê·œì„± ê°€ì • ë¶ˆí•„ìš”, ìˆœìœ„ ê¸°ë°˜
    """
    print(f"\n" + "=" * 70)
    print(f"ğŸ“Š Mann-Whitney U Test (ë¹„ëª¨ìˆ˜ ê²€ì •)")
    print(f"=" * 70)
    
    # ê¸°ìˆ  í†µê³„
    print(f"\nê¸°ìˆ  í†µê³„:")
    print(f"  Group 1: n={len(group1)}, median={np.median(group1):.4f}, IQR={stats.iqr(group1):.4f}")
    print(f"  Group 2: n={len(group2)}, median={np.median(group2):.4f}, IQR={stats.iqr(group2):.4f}")
    
    # Mann-Whitney U test
    u_stat, p_value = mannwhitneyu(group1, group2, alternative='two-sided')
    
    # íš¨ê³¼ í¬ê¸° (Rank-biserial correlation)
    n1, n2 = len(group1), len(group2)
    r = 1 - (2*u_stat) / (n1 * n2)  # Rank-biserial correlation
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"\nMann-Whitney U Test ê²°ê³¼:")
    print(f"  Hâ‚€: ë‘ ê·¸ë£¹ì˜ ë¶„í¬ê°€ ê°™ë‹¤")
    print(f"  Hâ‚: ë‘ ê·¸ë£¹ì˜ ë¶„í¬ê°€ ë‹¤ë¥´ë‹¤")
    print(f"\n  U-í†µê³„ëŸ‰: {u_stat:.4f}")
    print(f"  P-value: {p_value:.4f}")
    
    if p_value < alpha:
        print(f"\n  âœ… ê²°ë¡ : ê·€ë¬´ê°€ì„¤ ê¸°ê° (p < {alpha})")
        print(f"     â†’ ë‘ ê·¸ë£¹ ê°„ ìœ ì˜ë¯¸í•œ ì°¨ì´ê°€ ìˆìŒ")
    else:
        print(f"\n  âŒ ê²°ë¡ : ê·€ë¬´ê°€ì„¤ ì±„íƒ (p â‰¥ {alpha})")
        print(f"     â†’ ë‘ ê·¸ë£¹ ê°„ ìœ ì˜ë¯¸í•œ ì°¨ì´ê°€ ì—†ìŒ")
    
    print(f"\níš¨ê³¼ í¬ê¸° (Rank-biserial correlation): {r:.4f}")
    
    # ì‹œê°í™”
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # ë°•ìŠ¤í”Œë¡¯
    data_plot = pd.DataFrame({
        'Group': ['Group 1']*len(group1) + ['Group 2']*len(group2),
        'Value': np.concatenate([group1, group2])
    })
    sns.boxplot(data=data_plot, x='Group', y='Value', ax=axes[0], palette='Set2')
    axes[0].set_title(f'Mann-Whitney U: p-value = {p_value:.4f}', fontsize=12)
    axes[0].grid(True, alpha=0.3, axis='y')
    
    # ë¶„í¬ ë¹„êµ
    axes[1].hist(group1, bins=30, alpha=0.6, label='Group 1', density=True)
    axes[1].hist(group2, bins=30, alpha=0.6, label='Group 2', density=True)
    axes[1].axvline(np.median(group1), color='blue', linestyle='--', linewidth=2, label=f'Medâ‚={np.median(group1):.2f}')
    axes[1].axvline(np.median(group2), color='orange', linestyle='--', linewidth=2, label=f'Medâ‚‚={np.median(group2):.2f}')
    axes[1].set_xlabel('Value', fontsize=11)
    axes[1].set_ylabel('Density', fontsize=11)
    axes[1].set_title('Distribution Comparison', fontsize=12)
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    return {
        'u_statistic': u_stat,
        'p_value': p_value,
        'effect_size_r': r,
        'significant': p_value < alpha
    }

# ë¹„ì •ê·œ ë°ì´í„° ìƒì„± (ì§€ìˆ˜ë¶„í¬)
np.random.seed(42)
non_normal_1 = np.random.exponential(scale=2.0, size=200)
non_normal_2 = np.random.exponential(scale=2.5, size=200)

mann_whitney_result = perform_mann_whitney(non_normal_1, non_normal_2, alpha=0.05)
```

### 3.8 Chi-Square Test (ë²”ì£¼í˜• ë³€ìˆ˜ ë…ë¦½ì„± ê²€ì •)

```python
def perform_chi_square_test(contingency_table, alpha=0.05):
    """
    Chi-square test of independence (ë²”ì£¼í˜• ë³€ìˆ˜ ë…ë¦½ì„± ê²€ì •)
    
    Parameters:
    -----------
    contingency_table : DataFrame or array
        êµì°¨í‘œ (contingency table)
    alpha : float
    
    Returns:
    --------
    result : dict
    """
    print(f"\n" + "=" * 70)
    print(f"ğŸ“Š Chi-square Test of Independence")
    print(f"=" * 70)
    
    # êµì°¨í‘œ ì¶œë ¥
    print(f"\nêµì°¨í‘œ (Contingency Table):")
    print(contingency_table)
    
    # Chi-square ê²€ì •
    chi2, p_value, dof, expected = chi2_contingency(contingency_table)
    
    # íš¨ê³¼ í¬ê¸° (CramÃ©r's V)
    n = contingency_table.sum().sum() if isinstance(contingency_table, pd.DataFrame) else contingency_table.sum()
    min_dim = min(contingency_table.shape[0], contingency_table.shape[1]) - 1
    cramers_v = np.sqrt(chi2 / (n * min_dim))
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"\nChi-square Test ê²°ê³¼:")
    print(f"  Hâ‚€: ë‘ ë³€ìˆ˜ê°€ ë…ë¦½ì ì´ë‹¤ (ê´€ë ¨ ì—†ìŒ)")
    print(f"  Hâ‚: ë‘ ë³€ìˆ˜ê°€ ë…ë¦½ì ì´ì§€ ì•Šë‹¤ (ê´€ë ¨ ìˆìŒ)")
    print(f"\n  Ï‡Â² í†µê³„ëŸ‰: {chi2:.4f}")
    print(f"  ììœ ë„: {dof}")
    print(f"  P-value: {p_value:.4f}")
    
    if p_value < alpha:
        print(f"\n  âœ… ê²°ë¡ : ê·€ë¬´ê°€ì„¤ ê¸°ê° (p < {alpha})")
        print(f"     â†’ ë‘ ë³€ìˆ˜ ê°„ ìœ ì˜ë¯¸í•œ ê´€ë ¨ì„±ì´ ìˆìŒ")
    else:
        print(f"\n  âŒ ê²°ë¡ : ê·€ë¬´ê°€ì„¤ ì±„íƒ (p â‰¥ {alpha})")
        print(f"     â†’ ë‘ ë³€ìˆ˜ ê°„ ê´€ë ¨ì„±ì´ ì—†ìŒ (ë…ë¦½ì )")
    
    print(f"\níš¨ê³¼ í¬ê¸° (CramÃ©r's V): {cramers_v:.4f}")
    if cramers_v < 0.1:
        effect_interp = "ì‘ìŒ"
    elif cramers_v < 0.3:
        effect_interp = "ì¤‘ê°„"
    else:
        effect_interp = "í¼"
    print(f"  í•´ì„: {effect_interp}")
    
    print(f"\nê¸°ëŒ€ë¹ˆë„ (Expected Frequencies):")
    print(pd.DataFrame(expected, 
                       index=contingency_table.index if isinstance(contingency_table, pd.DataFrame) else range(contingency_table.shape[0]),
                       columns=contingency_table.columns if isinstance(contingency_table, pd.DataFrame) else range(contingency_table.shape[1])))
    
    # ê¸°ëŒ€ë¹ˆë„ < 5 ê²½ê³ 
    if (expected < 5).any():
        print(f"\nâš ï¸  ê²½ê³ : ê¸°ëŒ€ë¹ˆë„ê°€ 5 ë¯¸ë§Œì¸ ì…€ì´ ìˆìŠµë‹ˆë‹¤.")
        print(f"   â†’ Fisher's exact test ê³ ë ¤ (2x2 í‘œì˜ ê²½ìš°)")
    
    # ì‹œê°í™”
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))
    
    # íˆíŠ¸ë§µ (ê´€ì¸¡ë¹ˆë„)
    sns.heatmap(contingency_table, annot=True, fmt='d', cmap='YlOrRd', ax=axes[0], cbar_kws={'label': 'Count'})
    axes[0].set_title(f'Observed Frequencies\nÏ‡Â²={chi2:.2f}, p={p_value:.4f}', fontsize=12)
    
    # íˆíŠ¸ë§µ (ê¸°ëŒ€ë¹ˆë„)
    expected_df = pd.DataFrame(expected,
                               index=contingency_table.index if isinstance(contingency_table, pd.DataFrame) else range(contingency_table.shape[0]),
                               columns=contingency_table.columns if isinstance(contingency_table, pd.DataFrame) else range(contingency_table.shape[1]))
    sns.heatmap(expected_df, annot=True, fmt='.1f', cmap='Blues', ax=axes[1], cbar_kws={'label': 'Expected Count'})
    axes[1].set_title('Expected Frequencies', fontsize=12)
    
    plt.tight_layout()
    plt.show()
    
    return {
        'chi2_statistic': chi2,
        'p_value': p_value,
        'dof': dof,
        'cramers_v': cramers_v,
        'expected': expected,
        'significant': p_value < alpha
    }

# ë²”ì£¼í˜• ë°ì´í„° ì˜ˆì‹œ
contingency_table = pd.DataFrame(
    [[50, 30, 20],
     [45, 40, 15],
     [35, 35, 30]],
    index=['Group A', 'Group B', 'Group C'],
    columns=['Category 1', 'Category 2', 'Category 3']
)

chi_square_result = perform_chi_square_test(contingency_table, alpha=0.05)
```

### 3.9 ê²€ì •ë ¥ ë¶„ì„ (Power Analysis)

```python
def perform_power_analysis(effect_size, alpha=0.05, power=0.8, test_type='t-test'):
    """
    ê²€ì •ë ¥ ë¶„ì„: í•„ìš”í•œ ìƒ˜í”Œ í¬ê¸° ê³„ì‚°
    
    Parameters:
    -----------
    effect_size : float
        íš¨ê³¼ í¬ê¸° (Cohen's d)
    alpha : float
        ì œ1ì¢… ì˜¤ë¥˜ í™•ë¥ 
    power : float
        ê²€ì •ë ¥ (1 - Î²)
    test_type : str
        't-test' or 'anova'
    
    Returns:
    --------
    sample_size : int
    """
    print(f"\n" + "=" * 70)
    print(f"ğŸ“Š ê²€ì •ë ¥ ë¶„ì„ (Power Analysis)")
    print(f"=" * 70)
    
    if test_type == 't-test':
        analysis = TTestIndPower()
        sample_size = analysis.solve_power(
            effect_size=effect_size,
            alpha=alpha,
            power=power,
            ratio=1.0,  # ê·¸ë£¹ í¬ê¸° ë¹„ìœ¨
            alternative='two-sided'
        )
        
        print(f"\në…ë¦½í‘œë³¸ t-test:")
        print(f"  íš¨ê³¼ í¬ê¸° (Cohen's d): {effect_size}")
        print(f"  ìœ ì˜ìˆ˜ì¤€ (Î±): {alpha}")
        print(f"  ê²€ì •ë ¥ (1-Î²): {power}")
        print(f"\n  âœ… í•„ìš”í•œ ìƒ˜í”Œ í¬ê¸° (ê° ê·¸ë£¹): {int(np.ceil(sample_size))}ëª…")
        print(f"     â†’ ì´ ìƒ˜í”Œ: {int(np.ceil(sample_size)) * 2}ëª…")
    
    # íš¨ê³¼ í¬ê¸°ë³„ ìƒ˜í”Œ í¬ê¸° ì‹œê°í™”
    effect_sizes = np.linspace(0.1, 1.0, 50)
    sample_sizes = [analysis.solve_power(es, alpha, power, 1.0, 'two-sided') for es in effect_sizes]
    
    plt.figure(figsize=(10, 6))
    plt.plot(effect_sizes, sample_sizes, linewidth=2)
    plt.axhline(y=sample_size, color='r', linestyle='--', label=f'Current: n={int(np.ceil(sample_size))}')
    plt.axvline(x=effect_size, color='r', linestyle='--')
    plt.xlabel('Effect Size (Cohen\'s d)', fontsize=12)
    plt.ylabel('Required Sample Size (per group)', fontsize=12)
    plt.title(f'Power Analysis\n(Î±={alpha}, power={power})', fontsize=14)
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()
    
    print(f"\nğŸ’¡ í•´ì„:")
    print(f"  - íš¨ê³¼ í¬ê¸°ê°€ í´ìˆ˜ë¡ í•„ìš”í•œ ìƒ˜í”Œ í¬ê¸° ê°ì†Œ")
    print(f"  - ê²€ì •ë ¥ì„ ë†’ì´ë ¤ë©´ ìƒ˜í”Œ í¬ê¸° ì¦ê°€ í•„ìš”")
    print(f"  - ì‘ì€ íš¨ê³¼(d=0.2)ëŠ” í° ìƒ˜í”Œ(nâ‰ˆ400/group) í•„ìš”")
    
    return int(np.ceil(sample_size))

# ê²€ì •ë ¥ ë¶„ì„
required_n = perform_power_analysis(effect_size=0.3, alpha=0.05, power=0.8, test_type='t-test')
```

### 3.10 ì¢…í•© ê²€ì • ìš”ì•½ í•¨ìˆ˜

```python
def comprehensive_hypothesis_test(group1, group2, alpha=0.05, paired=False):
    """
    ì¢…í•© ê°€ì„¤ ê²€ì •: ì •ê·œì„± í™•ì¸ í›„ ì ì ˆí•œ ê²€ì • ì„ íƒ
    
    Parameters:
    -----------
    group1, group2 : array-like
    alpha : float
    paired : bool
        ëŒ€ì‘í‘œë³¸ ì—¬ë¶€
    """
    print(f"\n" + "=" * 80)
    print(f"ğŸ”¬ ì¢…í•© ê°€ì„¤ ê²€ì • (Comprehensive Hypothesis Test)")
    print(f"=" * 80)
    
    # 1. ì •ê·œì„± ê²€ì •
    _, p_norm1 = shapiro(group1) if len(group1) <= 5000 else stats.kstest(group1, 'norm')
    _, p_norm2 = shapiro(group2) if len(group2) <= 5000 else stats.kstest(group2, 'norm')
    
    is_normal = (p_norm1 > 0.05) and (p_norm2 > 0.05)
    
    print(f"\n1ë‹¨ê³„: ì •ê·œì„± ê²€ì •")
    print(f"  Group 1: p={p_norm1:.4f} {'(ì •ê·œ)' if p_norm1 > 0.05 else '(ë¹„ì •ê·œ)'}")
    print(f"  Group 2: p={p_norm2:.4f} {'(ì •ê·œ)' if p_norm2 > 0.05 else '(ë¹„ì •ê·œ)'}")
    
    # 2. ê²€ì • ì„ íƒ ë° ìˆ˜í–‰
    print(f"\n2ë‹¨ê³„: ê²€ì • ìˆ˜í–‰")
    
    if is_normal:
        if paired:
            print(f"  ì„ íƒ: Paired t-test (ëŒ€ì‘í‘œë³¸, ì •ê·œ)")
            result = perform_paired_ttest(group1, group2, alpha)
        else:
            print(f"  ì„ íƒ: Independent t-test (ë…ë¦½í‘œë³¸, ì •ê·œ)")
            result = perform_independent_ttest(group1, group2, alpha)
    else:
        if paired:
            print(f"  ì„ íƒ: Wilcoxon Signed-Rank test (ëŒ€ì‘í‘œë³¸, ë¹„ì •ê·œ)")
            stat, p_val = wilcoxon(group1, group2)
            print(f"  í†µê³„ëŸ‰: {stat:.4f}, P-value: {p_val:.4f}")
            result = {'p_value': p_val, 'significant': p_val < alpha}
        else:
            print(f"  ì„ íƒ: Mann-Whitney U test (ë…ë¦½í‘œë³¸, ë¹„ì •ê·œ)")
            result = perform_mann_whitney(group1, group2, alpha)
    
    # 3. ìµœì¢… ê²°ë¡ 
    print(f"\n3ë‹¨ê³„: ìµœì¢… ê²°ë¡ ")
    if result['significant']:
        print(f"  âœ… ë‘ ê·¸ë£¹ ê°„ í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ ì°¨ì´ê°€ ìˆìŠµë‹ˆë‹¤ (p < {alpha})")
    else:
        print(f"  âŒ ë‘ ê·¸ë£¹ ê°„ í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œ ì°¨ì´ê°€ ì—†ìŠµë‹ˆë‹¤ (p â‰¥ {alpha})")
    
    return result

# ì¢…í•© ê²€ì • ì‹¤í–‰
comprehensive_result = comprehensive_hypothesis_test(group_a, group_b, alpha=0.05, paired=False)
```

---

## 4. ì˜ˆì‹œ

(Due to length constraints, I'll continue in a separate message with sections 4-10)

### 4.1 ì‹¤ì „ ì˜ˆì œ: A/B í…ŒìŠ¤íŠ¸ (ì „í™˜ìœ¨ ë¹„êµ)

```python
print("=" * 70)
print("ğŸ“ˆ ë¹„ì¦ˆë‹ˆìŠ¤ ì‹œë‚˜ë¦¬ì˜¤: ì›¹ì‚¬ì´íŠ¸ A/B í…ŒìŠ¤íŠ¸")
print("=" * 70)

print("\nëª©í‘œ:")
print("- ì‹ ê·œ ê²°ì œ í˜ì´ì§€ vs ê¸°ì¡´ ê²°ì œ í˜ì´ì§€")
print("- ì „í™˜ìœ¨ ë¹„êµ ë° ìœ ì˜ì„± ê²€ì •")

print("\në°ì´í„°:")
print("- Aê·¸ë£¹ (ê¸°ì¡´): 10,000ëª…, ì „í™˜ 520ëª… (5.2%)")
print("- Bê·¸ë£¹ (ì‹ ê·œ): 10,000ëª…, ì „í™˜ 610ëª… (6.1%)")

print("\në¶„ì„:")
print("1. ê°€ì„¤ ì„¤ì •")
print("   Hâ‚€: p_A = p_B")
print("   Hâ‚: p_B > p_A (one-tailed)")
print("\n2. Two-proportion z-test ìˆ˜í–‰")
print("   z = 3.21, p = 0.0007")
print("\n3. íš¨ê³¼ í¬ê¸°")
print("   ì ˆëŒ€ ì°¨ì´: 0.9%p")
print("   ìƒëŒ€ ì°¨ì´: 17.3% ì¦ê°€")

print("\nê²°ë¡ :")
print("  âœ… ì‹ ê·œ í˜ì´ì§€ê°€ í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•˜ê²Œ ë†’ì€ ì „í™˜ìœ¨")
print("  âœ… ì‹¤ë¬´ì ìœ¼ë¡œë„ ì˜ë¯¸ ìˆëŠ” ê°œì„  (17% ì¦ê°€)")
print("\nì•¡ì…˜:")
print("  â†’ ì‹ ê·œ í˜ì´ì§€ ì „ë©´ ì ìš©")
print("  â†’ ì—°ê°„ ì¶”ê°€ ìˆ˜ìµ: ì•½ $50,000 ì˜ˆìƒ")
```

---

## 5. ì—ì´ì „íŠ¸ ë§¤í•‘

### 5.1 ë‹´ë‹¹ ì—ì´ì „íŠ¸

| ì‘ì—… | Primary Agent | Supporting Agents |
|------|--------------|-------------------|
| ê°€ì„¤ ì„¤ì • | `data-scientist` | - |
| ì •ê·œì„± ê²€ì • | `data-scientist` | - |
| t-test, ANOVA | `data-scientist` | - |
| ë¹„ëª¨ìˆ˜ ê²€ì • | `data-scientist` | - |
| ê²€ì •ë ¥ ë¶„ì„ | `data-scientist` | - |
| ê²°ê³¼ í•´ì„ ë° ë³´ê³  | `data-scientist` | `technical-documentation-writer` |

### 5.2 ê´€ë ¨ ìŠ¤í‚¬

**Scientific Skills**:
- `scipy.stats` (ëª¨ë“  ê²€ì • í•¨ìˆ˜)
- `statsmodels` (post-hoc, power analysis)
- `pandas`, `numpy` (ë°ì´í„° ì²˜ë¦¬)
- `matplotlib`, `seaborn` (ì‹œê°í™”)

---

## 6. í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬

### 6.1 í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬

```bash
# í†µê³„ ë¶„ì„
pip install scipy==1.12.0
pip install statsmodels==0.14.1

# ë°ì´í„° ì²˜ë¦¬
pip install pandas==2.2.0
pip install numpy==1.26.3

# ì‹œê°í™”
pip install matplotlib==3.8.2
pip install seaborn==0.13.1
```

---

## 7. ì²´í¬í¬ì¸íŠ¸

### 7.1 ë¶„ì„ ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] **ê°€ì„¤ ëª…í™•í™”**
  - [ ] ê·€ë¬´ê°€ì„¤ (Hâ‚€) ì •ì˜
  - [ ] ëŒ€ë¦½ê°€ì„¤ (Hâ‚) ì •ì˜
  - [ ] ë‹¨ì¸¡ or ì–‘ì¸¡ ê²€ì •?

- [ ] **ë°ì´í„° ì¤€ë¹„**
  - [ ] ìƒ˜í”Œ í¬ê¸° ì¶©ë¶„í•œê°€?
  - [ ] ê²°ì¸¡ê°’ ì²˜ë¦¬ ì™„ë£Œ
  - [ ] ì´ìƒì¹˜ í™•ì¸

### 7.2 ë¶„ì„ ì¤‘ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] **ê°€ì • í™•ì¸**
  - [ ] ì •ê·œì„± ê²€ì • (ëª¨ìˆ˜ ê²€ì • ì‹œ)
  - [ ] ë“±ë¶„ì‚° ê²€ì • (t-test ì‹œ)
  - [ ] ë…ë¦½ì„± í™•ì¸

- [ ] **ì ì ˆí•œ ê²€ì • ì„ íƒ**
  - [ ] ì •ê·œì„± ë§Œì¡± â†’ ëª¨ìˆ˜ ê²€ì •
  - [ ] ì •ê·œì„± ë¶ˆë§Œì¡± â†’ ë¹„ëª¨ìˆ˜ ê²€ì •

### 7.3 ë¶„ì„ í›„ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] **ê²°ê³¼ í•´ì„**
  - [ ] P-value í™•ì¸ (Î±ì™€ ë¹„êµ)
  - [ ] íš¨ê³¼ í¬ê¸° ê³„ì‚°
  - [ ] ì‹¤ë¬´ì  ì˜ë¯¸ ê³ ë ¤

- [ ] **ë³´ê³ **
  - [ ] ê²°ë¡  ëª…í™•íˆ ì‘ì„±
  - [ ] ì‹œê°í™” í¬í•¨
  - [ ] ì œí•œì  ì–¸ê¸‰

---

## 8. íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 8.1 ì¼ë°˜ì  ì˜¤ë¥˜

**ë¬¸ì œ 1: P-valueëŠ” ìœ ì˜í•˜ì§€ë§Œ íš¨ê³¼ í¬ê¸°ê°€ ì‘ìŒ**

```python
# ì›ì¸: ìƒ˜í”Œì´ ë„ˆë¬´ í¼ â†’ ì‘ì€ ì°¨ì´ë„ ìœ ì˜
# í•´ê²°: íš¨ê³¼ í¬ê¸°(Cohen's d, eta-squared) í•¨ê»˜ ë³´ê³ 

if p_value < 0.05 and abs(cohens_d) < 0.2:
    print("í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•˜ì§€ë§Œ ì‹¤ë¬´ì  ì˜ë¯¸ëŠ” ì‘ìŒ")
```

**ë¬¸ì œ 2: ì •ê·œì„± ê²€ì •ì—ì„œ ëª¨ë‘ ê±°ë¶€ë¨ (ëŒ€í‘œë³¸)**

```python
# ì›ì¸: nì´ í¬ë©´ ì‘ì€ í¸ì°¨ë„ ìœ ì˜
# í•´ê²°: Q-Q plotìœ¼ë¡œ ì‹œê°ì  í™•ì¸, ì¤‘ì‹¬ê·¹í•œì •ë¦¬ ì ìš©

if n > 100:
    print("ëŒ€í‘œë³¸: ì •ê·œì„± ì™„í™”, t-test ì‚¬ìš© ê°€ëŠ¥")
else:
    print("ì†Œí‘œë³¸: ë¹„ëª¨ìˆ˜ ê²€ì • ê¶Œì¥")
```

**ë¬¸ì œ 3: ë‹¤ì¤‘ ë¹„êµ ë¬¸ì œ (Multiple Testing)**

```python
# ì›ì¸: ì—¬ëŸ¬ ë²ˆ ê²€ì • â†’ ì œ1ì¢… ì˜¤ë¥˜ ì¦ê°€
# í•´ê²°: Bonferroni ë³´ì •, FDR ë³´ì •

alpha_corrected = 0.05 / n_comparisons  # Bonferroni
```

### 8.2 í•´ì„ ê´€ë ¨

**Q1: P-value 0.051ì€ ìœ ì˜í•˜ì§€ ì•Šì€ê°€ìš”?**

```
A: Î±=0.05 ê¸°ì¤€ìœ¼ë¡œëŠ” ìœ ì˜í•˜ì§€ ì•Šì§€ë§Œ...
- 0.05ëŠ” ì„ì˜ì˜ ê¸°ì¤€ (ì ˆëŒ€ì  ì•„ë‹˜)
- ë„ë©”ì¸ì— ë”°ë¼ 0.1 ë˜ëŠ” 0.01 ì‚¬ìš© ê°€ëŠ¥
- P-valueë³´ë‹¤ íš¨ê³¼ í¬ê¸°ì™€ ì‹¤ë¬´ì  ì˜ë¯¸ ì¤‘ìš”

ê¶Œì¥:
- P-value ì •í™•íˆ ë³´ê³  (0.051)
- íš¨ê³¼ í¬ê¸° í•¨ê»˜ ì œì‹œ
- ì‹¤ë¬´ íŒë‹¨ì€ ì¢…í•©ì ìœ¼ë¡œ
```

**Q2: í†µê³„ì  ìœ ì˜ì„± vs ì‹¤ë¬´ì  ìœ ì˜ì„±?**

```
A:
í†µê³„ì  ìœ ì˜ì„±: p < 0.05
- ì°¨ì´ê°€ ìš°ì—°ì´ ì•„ë‹ˆë‹¤

ì‹¤ë¬´ì  ìœ ì˜ì„±: íš¨ê³¼ í¬ê¸°
- ì°¨ì´ê°€ ì˜ë¯¸ ìˆëŠ”ê°€?

ì˜ˆì‹œ:
ì›¹ì‚¬ì´íŠ¸ ì „í™˜ìœ¨: 5.0% â†’ 5.1%
- í†µê³„ì : p=0.001 (ìœ ì˜)
- ì‹¤ë¬´ì : 0.1%p ì¦ê°€ëŠ” ë¯¸ë¯¸

â†’ ë‘˜ ë‹¤ ê³ ë ¤í•˜ì—¬ íŒë‹¨
```

**Q3: ê²€ì •ë ¥(power)ì´ ë‚®ìœ¼ë©´?**

```
A: ì œ2ì¢… ì˜¤ë¥˜(Î²) ìœ„í—˜ ì¦ê°€
- ì‹¤ì œ ì°¨ì´ê°€ ìˆëŠ”ë° ëª» ì°¾ì„ í™•ë¥  â†‘
- ê²€ì •ë ¥ < 0.8: ìƒ˜í”Œ ë¶€ì¡± ê°€ëŠ¥ì„±

í•´ê²°:
1. ìƒ˜í”Œ í¬ê¸° ì¦ê°€
2. íš¨ê³¼ í¬ê¸°ê°€ ì‘ìœ¼ë©´ ë§ì€ ìƒ˜í”Œ í•„ìš”
3. ì‚¬ì „ ê²€ì •ë ¥ ë¶„ì„ìœ¼ë¡œ í•„ìš” ìƒ˜í”Œ ê³„ì‚°
```

### 8.3 ê²€ì • ì„ íƒ í”Œë¡œìš°

```
                   ì •ê·œì„± ë§Œì¡±?
                   /         \
                 Yes          No
                 /             \
            ëŒ€ì‘í‘œë³¸?        ëŒ€ì‘í‘œë³¸?
            /    \           /    \
          Yes    No        Yes    No
          /       \         /      \
    Paired-t  Independent-t  Wilcoxon  Mann-Whitney
              /      \
          ë“±ë¶„ì‚°?    
          /    \
        Yes    No
        /       \
   Student's  Welch's
```

---

## 9. ì°¸ê³  ìë£Œ

### 9.1 ê³µì‹ ë¬¸ì„œ

- **SciPy Stats**: https://docs.scipy.org/doc/scipy/reference/stats.html
- **Statsmodels**: https://www.statsmodels.org/stable/index.html
- **Statistical Power**: https://www.stat.ubc.ca/~rollin/stats/ssize/

### 9.2 ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤

1. **ê°€ì„¤ ê²€ì • ì²´í¬ë¦¬ìŠ¤íŠ¸**
   ```
   1. ì—°êµ¬ ì§ˆë¬¸ ëª…í™•í™”
   2. ê°€ì„¤ ì„¤ì • (Hâ‚€, Hâ‚)
   3. ìœ ì˜ìˆ˜ì¤€ ê²°ì • (Î±)
   4. ê²€ì • ì„ íƒ (ì •ê·œì„±, ë…ë¦½ì„±)
   5. ìƒ˜í”Œ í¬ê¸° í™•ì¸ (ê²€ì •ë ¥)
   6. ê²€ì • ìˆ˜í–‰
   7. P-value í•´ì„
   8. íš¨ê³¼ í¬ê¸° ê³„ì‚°
   9. ê²°ë¡  ë„ì¶œ (í†µê³„+ì‹¤ë¬´)
   ```

2. **ë³´ê³  í˜•ì‹**
   ```
   "ë…ë¦½í‘œë³¸ t-test ê²°ê³¼, ê·¸ë£¹ B(M=105.3, SD=15.2)ê°€ 
   ê·¸ë£¹ A(M=100.1, SD=14.8)ë³´ë‹¤ í†µê³„ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•˜ê²Œ 
   ë†’ì•˜ë‹¤, t(998)=3.21, p=.001, d=0.34. 
   ì´ëŠ” ì¤‘ê°„ ì •ë„ì˜ íš¨ê³¼ í¬ê¸°ì´ë‹¤."
   ```

### 9.3 ì¶”ê°€ í•™ìŠµ ìë£Œ

- **í†µê³„ ê²€ì • ì„ íƒ ê°€ì´ë“œ**: https://www.graphpad.com/guides/prism/latest/statistics/stat_choosing_a_test.htm
- **P-value ì˜¤í•´ì™€ ì§„ì‹¤**: https://www.nature.com/articles/d41586-019-00857-9
- **Effect Size ê°€ì´ë“œ**: https://www.statisticshowto.com/effect-size/

---

## 10. ìš”ì•½

### 10.1 í•µì‹¬ ë©”ì‹œì§€

ê°€ì„¤ ê²€ì •ì€ ë°ì´í„° ê¸°ë°˜ ì˜ì‚¬ê²°ì •ì˜ ê³¼í•™ì  ê·¼ê±°ë¥¼ ì œê³µí•©ë‹ˆë‹¤. P-valueë§Œì´ ì•„ë‹Œ íš¨ê³¼ í¬ê¸°, ì‹ ë¢°êµ¬ê°„, ì‹¤ë¬´ì  ì˜ë¯¸ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•˜ì—¬ íŒë‹¨í•´ì•¼ í•©ë‹ˆë‹¤. ì •ê·œì„± ë“± ê°€ì •ì„ í™•ì¸í•˜ê³  ì ì ˆí•œ ê²€ì •ì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.

### 10.2 ê²€ì • ë°©ë²• ì„ íƒ ê°€ì´ë“œ

| ìƒí™© | ì¶”ì²œ ê²€ì • |
|------|----------|
| 2ê·¸ë£¹ í‰ê·  ë¹„êµ (ì •ê·œ) | Independent t-test |
| 2ê·¸ë£¹ í‰ê·  ë¹„êµ (ë¹„ì •ê·œ) | Mann-Whitney U |
| ì „í›„ ë¹„êµ (ì •ê·œ) | Paired t-test |
| ì „í›„ ë¹„êµ (ë¹„ì •ê·œ) | Wilcoxon Signed-Rank |
| 3ê·¸ë£¹+ í‰ê·  ë¹„êµ (ì •ê·œ) | ANOVA + Tukey |
| 3ê·¸ë£¹+ í‰ê·  ë¹„êµ (ë¹„ì •ê·œ) | Kruskal-Wallis |
| ë²”ì£¼í˜• ë…ë¦½ì„± | Chi-square |
| ë¹„ìœ¨ ë¹„êµ | Two-proportion z-test |

### 10.3 ë‹¤ìŒ ë‹¨ê³„

- **ì‹ ë¢°êµ¬ê°„**: `12-statistical-inference.md` ì°¸ê³ 
- **íšŒê·€ ë¶„ì„**: ë³€ìˆ˜ ê°„ ê´€ê³„ ëª¨ë¸ë§
- **ë² ì´ì§€ì•ˆ í†µê³„**: ì‚¬ì „ ì •ë³´ í™œìš©
- **ì‹¤í—˜ ì„¤ê³„**: A/B í…ŒìŠ¤íŠ¸ ìµœì í™”

---

**ì‘ì„±ì¼**: 2025-01-25  
**ë²„ì „**: 1.0  
**ë‚œì´ë„**: â­â­â­ (ê³ ê¸‰)  
**ì˜ˆìƒ ì†Œìš” ì‹œê°„**: 3-4ì‹œê°„ (í•™ìŠµ ë° ì‹¤ìŠµ)
